{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 DARIAH contribution tool This is the documentation for the DARIAH contribution tool, an instrument to register and assess community contributions to the DARIAH . The documentation contains parts that range from functional , conceptual , technical to mundane . About \u00b6 Code base To get an impression of the kind of work behind this app, we reveal how many lines of code have been written in which languages. See also how we managed to keep the code in all those languages tidy. More ... Lessons learned It has taken a lot of time to develop this app. Lots more than I expected from the start. More ... News Every now and then I resume what has happened during development. It is not regular and not comprehensive! More ... Functionality \u00b6 Business logic The actual handling of contributions, assessments and reviews is the business logic of this app. More ... Workflow At the highest level of abstraction a workflow engine implements the business logic. More ... Tables Several tables work together with the workflow engine. More ... Legacy data \u00b6 Content This app inherits 800 contributions that have been entered in 2015-2017 into a FileMaker database. We have migrated those to a MongoDB model. More ... Concepts \u00b6 Model The whole app is centered around data: contributions, assessments, reviews and more. We have to organize and specify that data. More ... Server \u00b6 Server The part of the app that guards the data sits at the server. From there it sends it to the web browsers (clients) of the users. More ... Authentication Users are authenticated at the server, and every bit of data that they subsequently receive, has passed a customs control. More ... Technology \u00b6 ES6 We have implemented the client application in ES6, i.e. modern Javascript. That is our glue language at the cliennt side. More ... Tech index We have listed most of the technology that we have made use of. More ... Integration \u00b6 API The data of the tool is accessible through an API. In fact, this app itself uses that API, whenever the client needs data from the server. More ... Maintenance \u00b6 Deploy Here are the bits and pieces you have to do in order to get a working system out of this. More ... Author \u00b6 Dirk Roorda DANS dirk.roorda@dans.knaw.nl 2019-11-08 (Redesign from a clean slate) 2019-08-06 2019-07-29 2019-03-04 2017-12-14","title":"Home"},{"location":"#home","text":"DARIAH contribution tool This is the documentation for the DARIAH contribution tool, an instrument to register and assess community contributions to the DARIAH . The documentation contains parts that range from functional , conceptual , technical to mundane .","title":"Home"},{"location":"#about","text":"Code base To get an impression of the kind of work behind this app, we reveal how many lines of code have been written in which languages. See also how we managed to keep the code in all those languages tidy. More ... Lessons learned It has taken a lot of time to develop this app. Lots more than I expected from the start. More ... News Every now and then I resume what has happened during development. It is not regular and not comprehensive! More ...","title":"About"},{"location":"#functionality","text":"Business logic The actual handling of contributions, assessments and reviews is the business logic of this app. More ... Workflow At the highest level of abstraction a workflow engine implements the business logic. More ... Tables Several tables work together with the workflow engine. More ...","title":"Functionality"},{"location":"#legacy-data","text":"Content This app inherits 800 contributions that have been entered in 2015-2017 into a FileMaker database. We have migrated those to a MongoDB model. More ...","title":"Legacy data"},{"location":"#concepts","text":"Model The whole app is centered around data: contributions, assessments, reviews and more. We have to organize and specify that data. More ...","title":"Concepts"},{"location":"#server","text":"Server The part of the app that guards the data sits at the server. From there it sends it to the web browsers (clients) of the users. More ... Authentication Users are authenticated at the server, and every bit of data that they subsequently receive, has passed a customs control. More ...","title":"Server"},{"location":"#technology","text":"ES6 We have implemented the client application in ES6, i.e. modern Javascript. That is our glue language at the cliennt side. More ... Tech index We have listed most of the technology that we have made use of. More ...","title":"Technology"},{"location":"#integration","text":"API The data of the tool is accessible through an API. In fact, this app itself uses that API, whenever the client needs data from the server. More ...","title":"Integration"},{"location":"#maintenance","text":"Deploy Here are the bits and pieces you have to do in order to get a working system out of this. More ...","title":"Maintenance"},{"location":"#author","text":"Dirk Roorda DANS dirk.roorda@dans.knaw.nl 2019-11-08 (Redesign from a clean slate) 2019-08-06 2019-07-29 2019-03-04 2017-12-14","title":"Author"},{"location":"About/Codebase/","text":"Codebase \u00b6 Statistics \u00b6 Legend The numbers stand for lines of code. 1000 lines is ~ 20 typed A4 pages of text. The statistics have been gathered by the cloc tool . Statistics Formalisms \u00b6 YAML \u00b6 See YAML . A simple plain-text way to convey structured data. What Markdown is to text, YAML is to XML-JSON. In this app we use YAML for configuration details. Usage in this app the data model lists all the tables and fields, including how they hang together and how we want to represent them on screen. It also defines access control. If you, as developer, need to add new tables and fields, you can do so by modifying the table definition files general configuration: see the config files . Markdown \u00b6 See Markdown . A simple plain-text way to write formatted text. See it as a shortcut to writing HTML. It is handy for writing documentation without being distracted by too many formatting options and issues, as you experience when writing in Word or plain HTML. General usage Markdown is usually converted to HTML, but even when it is not converted, it is still very readable. If you use GitHub, one of the first things is to write a README file for your project. This must be a markdown file. If you use other documentation options on GitHub, such as Wiki or Pages, you will also write markdown. Markdown has a sister: YAML , which is used for structured data. Usage in this app all documentation here is written in markdown all big editable text fields in this app support markdown. JavaScript \u00b6 See JavaScript . The principal scripting language for web applications. It has evolved into a performant language with a beautiful syntax, capable of running on the server and in websites. Usage in this app This app uses JavaScript in the client only. A previous incarnation of this app has been written as a Single Page App which lead to an explosion of Javascript and techniques within Javascript, such as React and Redux. That became too complex for our purposes, and we are back to simple, but modern Javascript and JQuery . Python \u00b6 See Python . A general purpose scripting language with excellent data processing facilities. Usage in this app This app uses python (version 3.6.1+) for the web server. The web server itself is Flask , a light-weight framework for handling http(s) requests. We have added a set of controllers . The actual code there is structured into generic functions for data access and specific tweaks to handle the business workflow of the contribution tool. CSS \u00b6 See CSS . Styling the app has nightmarish overtones, because the concerns of style often cut right across the concerns of the components. There are several ways to control the resulting mess, and one of the best is to use the modern features of CSS. General usage Cascading style sheets are the ultimate way to paint the final look and feel of the website. By using flex boxes instead of tables we can make the app respond gracefully to changes in the size of the display without resorting to the bureaucracy of overdefining style properties. Note that our app does not use the HTML <table> element any more for aligning pieces of content. Usage in this app We use a lot of the CSS-3 features, including variables , and calc() . This lessens our need for a style sheet preprocessor such as SASS to 0%. Note especially how colour management has become easy: all colour definitions are in variables all colour definitions are in HSLA , which allows a very consistent definition of families of colours. Quote from Mozilla : One advantage of HSLA over RGB is that it is more intuitive: you can guess 1 2 3 at the color you want, and tweak it from there. It is also easier to create a set of matching colors (e.g., by keeping the hue the same, while varying the lightness/darkness and saturation). This is exactly what we do. See vars.css . Shell \u00b6 See Shell . The shell is the interpreter of system level commands. Usage in this app Our app does not use it, but we use it to develop the app. All the development tasks, such pushing code to GitHub, transporting databases to the production server are done by specialized frameworks. These frameworks must be steered by intricate commands with many options which are easily forgotten. That's why we have a build script. You have to pass it just the name of a task, and the script executes that task with all the sophistication needed. HTML \u00b6 See HTML . The core language of the web. Usage in this app Surprisingly, our code contains very little HTML. Our server only sends snippets of HTML to the browser i response to focused request, such as the first short page , that serves to load a the style sheets and the JavaScript into the browser. This JavaScript code triggers the fetching of new HTML for portions that the user is interacting with. JSON \u00b6 See JSON . A format to serialize JavaScript objects. General usage In web applications, the program logic happens at two separate places (at least): the server and the client. It is important that data can flow seamlessly from one programming context to the other. JSON achieves that. Usage in this app to send data from client to server Keeping the code tidy \u00b6 There are three progressive levels of caring for your code. Level 1: code style Adopt a style guide and meticulously keep to it. It is hard, especially if you work in two syntactically and culturally diverse languages such as JavaScript and Python. Add CSS, Markdown and YAML to the mix, and you can feel the need for a next step. Yet this is the fundamental step, it cannot be skipped. Level 2: linters Linters are little programs that observe your code and check it for correctness and style, as far as that is possible without actually running the code. Usually, your editing environment runs them sneakily while you type or when you save, and give you unobtrusive but conspicuous feedback. It saves you a lot of round trips of compiling/building/running/staring at errors. Moreover, it gives you the feedback right where you are typing, so you do not have to lookup files and line numbers. Sometimes linters give you so much feedback that your heart sinks at the thought of going through all those cases and fix them all, even if you have a splendid IDE. That is where the next step comes in. Level 3: formatters Formatters have a lot in common with linters, but they fix the problems. Sometimes they parse your program with the parser of the language and then format the abstract syntax three they've got. That is the end of all style inconsistencies. Tools Here is an overview of tools used in developing this app. Formatters Formatters are not perfect, sometimes they produce code at which the linter balks, especially yapf is doing that. Luckily, you can selectively suppress certain kinds of transformations. language linter formatter JavaScript eslint prettier Python flake8 black Editor or IDE? For projects like these, you need a good editing environment. IDEs The good old ones like Eclipse are not really suited to the JavaScript and Python environments. There are interesting modern ones such as GitHub's Atom modernized ones such as Microsoft's Visual Studio Code and commercial ones such as Webstorm . Editors You can also choose to work with a text editor, such as the free Vim or the commercial Sublime Text . Vim My choice has been Vim, since I use it from its start in 1991. These are the key reasons for which Vim stands out: it has a compositional command set, like Unix itself. By this you get all your (massive) editing chores done without much remembering and thinking. it has a rich ecosystem of plugins. By this you can turn Vim into an IDE. It is rock solid and performant. You can edit many small files and then some big ones, at the same time. You do not loose data. My Vim setup Just for the record, here is a piece of my .vimrc file (the configuration file, which draws in plugins, and customises the interface). You can find out more about the plugins by clicking on them, they are all GitHub repos: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 call plug#begin () \"Python Plug 'vimjas/vim-python-pep8-indent' \" javascript Plug 'jelera/vim-javascript-syntax' Plug 'pangloss/vim-javascript' Plug 'othree/yajs.vim' Plug 'othree/javascript-libraries-syntax.vim' Plug 'mxw/vim-jsx' \" css Plug 'hail2u/vim-css3-syntax' \"utility Plug 'nathanaelkane/vim-indent-guides' Plug 'scrooloose/nerdtree' Plug 'w0rp/ale' \"color Plug 'morhetz/gruvbox' call plug# end () An honourable mention for the ALE plugin. This is an asynchronous plugin that invokes linters for your files while you edit. The beauty is, that if you have installed the linters first outside Vim, ALE is smart enough to detect them and run them for you, asynchronously, and with zero configuration.","title":"Codebase"},{"location":"About/Codebase/#codebase","text":"","title":"Codebase"},{"location":"About/Codebase/#statistics","text":"Legend The numbers stand for lines of code. 1000 lines is ~ 20 typed A4 pages of text. The statistics have been gathered by the cloc tool . Statistics","title":"Statistics"},{"location":"About/Codebase/#formalisms","text":"","title":"Formalisms"},{"location":"About/Codebase/#yaml","text":"See YAML . A simple plain-text way to convey structured data. What Markdown is to text, YAML is to XML-JSON. In this app we use YAML for configuration details. Usage in this app the data model lists all the tables and fields, including how they hang together and how we want to represent them on screen. It also defines access control. If you, as developer, need to add new tables and fields, you can do so by modifying the table definition files general configuration: see the config files .","title":"YAML"},{"location":"About/Codebase/#markdown","text":"See Markdown . A simple plain-text way to write formatted text. See it as a shortcut to writing HTML. It is handy for writing documentation without being distracted by too many formatting options and issues, as you experience when writing in Word or plain HTML. General usage Markdown is usually converted to HTML, but even when it is not converted, it is still very readable. If you use GitHub, one of the first things is to write a README file for your project. This must be a markdown file. If you use other documentation options on GitHub, such as Wiki or Pages, you will also write markdown. Markdown has a sister: YAML , which is used for structured data. Usage in this app all documentation here is written in markdown all big editable text fields in this app support markdown.","title":"Markdown"},{"location":"About/Codebase/#javascript","text":"See JavaScript . The principal scripting language for web applications. It has evolved into a performant language with a beautiful syntax, capable of running on the server and in websites. Usage in this app This app uses JavaScript in the client only. A previous incarnation of this app has been written as a Single Page App which lead to an explosion of Javascript and techniques within Javascript, such as React and Redux. That became too complex for our purposes, and we are back to simple, but modern Javascript and JQuery .","title":"JavaScript"},{"location":"About/Codebase/#python","text":"See Python . A general purpose scripting language with excellent data processing facilities. Usage in this app This app uses python (version 3.6.1+) for the web server. The web server itself is Flask , a light-weight framework for handling http(s) requests. We have added a set of controllers . The actual code there is structured into generic functions for data access and specific tweaks to handle the business workflow of the contribution tool.","title":"Python"},{"location":"About/Codebase/#css","text":"See CSS . Styling the app has nightmarish overtones, because the concerns of style often cut right across the concerns of the components. There are several ways to control the resulting mess, and one of the best is to use the modern features of CSS. General usage Cascading style sheets are the ultimate way to paint the final look and feel of the website. By using flex boxes instead of tables we can make the app respond gracefully to changes in the size of the display without resorting to the bureaucracy of overdefining style properties. Note that our app does not use the HTML <table> element any more for aligning pieces of content. Usage in this app We use a lot of the CSS-3 features, including variables , and calc() . This lessens our need for a style sheet preprocessor such as SASS to 0%. Note especially how colour management has become easy: all colour definitions are in variables all colour definitions are in HSLA , which allows a very consistent definition of families of colours. Quote from Mozilla : One advantage of HSLA over RGB is that it is more intuitive: you can guess 1 2 3 at the color you want, and tweak it from there. It is also easier to create a set of matching colors (e.g., by keeping the hue the same, while varying the lightness/darkness and saturation). This is exactly what we do. See vars.css .","title":"CSS"},{"location":"About/Codebase/#shell","text":"See Shell . The shell is the interpreter of system level commands. Usage in this app Our app does not use it, but we use it to develop the app. All the development tasks, such pushing code to GitHub, transporting databases to the production server are done by specialized frameworks. These frameworks must be steered by intricate commands with many options which are easily forgotten. That's why we have a build script. You have to pass it just the name of a task, and the script executes that task with all the sophistication needed.","title":"Shell"},{"location":"About/Codebase/#html","text":"See HTML . The core language of the web. Usage in this app Surprisingly, our code contains very little HTML. Our server only sends snippets of HTML to the browser i response to focused request, such as the first short page , that serves to load a the style sheets and the JavaScript into the browser. This JavaScript code triggers the fetching of new HTML for portions that the user is interacting with.","title":"HTML"},{"location":"About/Codebase/#json","text":"See JSON . A format to serialize JavaScript objects. General usage In web applications, the program logic happens at two separate places (at least): the server and the client. It is important that data can flow seamlessly from one programming context to the other. JSON achieves that. Usage in this app to send data from client to server","title":"JSON"},{"location":"About/Codebase/#keeping-the-code-tidy","text":"There are three progressive levels of caring for your code. Level 1: code style Adopt a style guide and meticulously keep to it. It is hard, especially if you work in two syntactically and culturally diverse languages such as JavaScript and Python. Add CSS, Markdown and YAML to the mix, and you can feel the need for a next step. Yet this is the fundamental step, it cannot be skipped. Level 2: linters Linters are little programs that observe your code and check it for correctness and style, as far as that is possible without actually running the code. Usually, your editing environment runs them sneakily while you type or when you save, and give you unobtrusive but conspicuous feedback. It saves you a lot of round trips of compiling/building/running/staring at errors. Moreover, it gives you the feedback right where you are typing, so you do not have to lookup files and line numbers. Sometimes linters give you so much feedback that your heart sinks at the thought of going through all those cases and fix them all, even if you have a splendid IDE. That is where the next step comes in. Level 3: formatters Formatters have a lot in common with linters, but they fix the problems. Sometimes they parse your program with the parser of the language and then format the abstract syntax three they've got. That is the end of all style inconsistencies. Tools Here is an overview of tools used in developing this app. Formatters Formatters are not perfect, sometimes they produce code at which the linter balks, especially yapf is doing that. Luckily, you can selectively suppress certain kinds of transformations. language linter formatter JavaScript eslint prettier Python flake8 black Editor or IDE? For projects like these, you need a good editing environment. IDEs The good old ones like Eclipse are not really suited to the JavaScript and Python environments. There are interesting modern ones such as GitHub's Atom modernized ones such as Microsoft's Visual Studio Code and commercial ones such as Webstorm . Editors You can also choose to work with a text editor, such as the free Vim or the commercial Sublime Text . Vim My choice has been Vim, since I use it from its start in 1991. These are the key reasons for which Vim stands out: it has a compositional command set, like Unix itself. By this you get all your (massive) editing chores done without much remembering and thinking. it has a rich ecosystem of plugins. By this you can turn Vim into an IDE. It is rock solid and performant. You can edit many small files and then some big ones, at the same time. You do not loose data. My Vim setup Just for the record, here is a piece of my .vimrc file (the configuration file, which draws in plugins, and customises the interface). You can find out more about the plugins by clicking on them, they are all GitHub repos: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 call plug#begin () \"Python Plug 'vimjas/vim-python-pep8-indent' \" javascript Plug 'jelera/vim-javascript-syntax' Plug 'pangloss/vim-javascript' Plug 'othree/yajs.vim' Plug 'othree/javascript-libraries-syntax.vim' Plug 'mxw/vim-jsx' \" css Plug 'hail2u/vim-css3-syntax' \"utility Plug 'nathanaelkane/vim-indent-guides' Plug 'scrooloose/nerdtree' Plug 'w0rp/ale' \"color Plug 'morhetz/gruvbox' call plug# end () An honourable mention for the ALE plugin. This is an asynchronous plugin that invokes linters for your files while you edit. The beauty is, that if you have installed the linters first outside Vim, ALE is smart enough to detect them and run them for you, asynchronously, and with zero configuration.","title":"Keeping the code tidy"},{"location":"About/Lessons/","text":"Lessons \u00b6 Because it took so long to develop this tool, and because it has grown so big, and because I have taken time to reduce that complexity again, I started reflecting on the choices I've made, and the things I've learned. What follows can be read as a self-assessment of the development process. \"Best\" practices \u00b6 The previous incarnation of the tool has been built using modern, top-notch, popular frameworks and tools, such as React, MongoDb, Python, modern Javascript (ES6), and its documentation is in Markdown on Github. But it is a complex beast, and it will be hard for other developers to dive in. Developers that want to upgrade this tool, should be seasoned React developers, or they should be willing and have time to enter a steep learning curve. In a clean-slate approach after that I have retraced my steps. The app is no longer a Single Page App. Most of the work happens at the server. This approach - alternative approaches \u00b6 A quick glance at the statistics of the code base makes clear the amount of thought that has gone into the tool. I have asked myself the question: why do we need so much programming for such a mundane task? Is it really necessary to build something this big for it? The answer: no. Before redesigning I considered these options: More classical framework: Django We could have used Django, but then we would have missed the opportunity to engage in real modern web application programming. The Javascript world is brewing with dynamics and innovation, and we would have skipped all that. Besides, also a Django application would contain a considerable amount of custom programming. More classical framework: Django We could have used Django, but then we would have missed the opportunity to engage in real modern web application programming. The Javascript world is brewing with dynamics and innovation, and we would have skipped all that. Besides, also a Django application would contain a considerable amount of custom programming. Generic app/framework We could have used an app like Trello or Basecamp, or even GitHub itself, or a content management system that has not been designed to support a specific workflow like this. We would have had several disadvantages: an extra dependency on a Silicon-Valley service the struggle to customize the service the need to instruct the users to use the system according to the intended workflow. Clean-slate approach: from the ground up Remove the focus on the client: all business logic to the server. The gains of an SPA are not crucial for this app, which will never have a mass audience. It is also not needed to give the users an experience that is close to a native app. The overhead of front-end development is gigantic in terms of frameworks needed: React, Redux, Webpack, Lodash, all together some 20,000 files in the local node-modules directory. Plus ,5000 lines of custom written Javascript code and another 5,000 lines in JSX (React Javascript). That has all been ditched now, while at the server there is very little increase in code. One of the reasons for that is that no matter how intelligent the app at the client side is, the server still needs to check the complete business logic. With a thich client, the logistics of data-synchronization between server and client becomes very intricate. What we have now, is something that has been built from the ground up again . We have total control over all aspects of the app, its data, and the servers at which it runs. We can connect it to other apps, define new microservices around it quite easily. So, the price has been high, much higher than I expected (and promised), but I think we've got something to build on. The learning curve (for what it is worth) \u00b6 When I started writing, I had the experience of developing SHEBANQ . At first, the tools I used for SHEBANQ were a model for developing this contrib tool. From the start it was clear that the contrib tool needed more profound underpinnings. I started out to write those underpinnings myself, programmed in pure, modern Javascript. That worked to a certain extent, but I doubted whether it was strong enough to carry the weight of the full app. After a while I started a big search, trying Google's Angular, Facebook's React, and various solutions that combined these frameworks in so-called full-stack setups, before I went back to server side coding in Python. Here are some lessons I learned during what followed. CRUD layer Some operations are so ubiquitous, that you have to program them once and for all: create/update/delete/read of database items (CRUD), all subject to user permissions. All things that are particular to specific tables and fields must be specified as configuration, all actions must read the configs and carry out generic code. Well, that has proved sub-optimal. The CRUD engine become far too abstract and complex to be tamed, and it become virtually impossible to add new functionality with any level of confidence. Now I have a CRUD machine for the normal, straightforward things. Anything that goes beyond that it hard-coded in derived classes that can inherit from the generic CRUD classes. Custom Workflow layer You cannot do all the business logic this way, without overloading your nice generic system, and even the specific coding. There is still too much structure in the specifics, and that can be captured by a workflow level.","title":"Lessons"},{"location":"About/Lessons/#lessons","text":"Because it took so long to develop this tool, and because it has grown so big, and because I have taken time to reduce that complexity again, I started reflecting on the choices I've made, and the things I've learned. What follows can be read as a self-assessment of the development process.","title":"Lessons"},{"location":"About/Lessons/#best-practices","text":"The previous incarnation of the tool has been built using modern, top-notch, popular frameworks and tools, such as React, MongoDb, Python, modern Javascript (ES6), and its documentation is in Markdown on Github. But it is a complex beast, and it will be hard for other developers to dive in. Developers that want to upgrade this tool, should be seasoned React developers, or they should be willing and have time to enter a steep learning curve. In a clean-slate approach after that I have retraced my steps. The app is no longer a Single Page App. Most of the work happens at the server.","title":"\"Best\" practices"},{"location":"About/Lessons/#this-approach-alternative-approaches","text":"A quick glance at the statistics of the code base makes clear the amount of thought that has gone into the tool. I have asked myself the question: why do we need so much programming for such a mundane task? Is it really necessary to build something this big for it? The answer: no. Before redesigning I considered these options: More classical framework: Django We could have used Django, but then we would have missed the opportunity to engage in real modern web application programming. The Javascript world is brewing with dynamics and innovation, and we would have skipped all that. Besides, also a Django application would contain a considerable amount of custom programming. More classical framework: Django We could have used Django, but then we would have missed the opportunity to engage in real modern web application programming. The Javascript world is brewing with dynamics and innovation, and we would have skipped all that. Besides, also a Django application would contain a considerable amount of custom programming. Generic app/framework We could have used an app like Trello or Basecamp, or even GitHub itself, or a content management system that has not been designed to support a specific workflow like this. We would have had several disadvantages: an extra dependency on a Silicon-Valley service the struggle to customize the service the need to instruct the users to use the system according to the intended workflow. Clean-slate approach: from the ground up Remove the focus on the client: all business logic to the server. The gains of an SPA are not crucial for this app, which will never have a mass audience. It is also not needed to give the users an experience that is close to a native app. The overhead of front-end development is gigantic in terms of frameworks needed: React, Redux, Webpack, Lodash, all together some 20,000 files in the local node-modules directory. Plus ,5000 lines of custom written Javascript code and another 5,000 lines in JSX (React Javascript). That has all been ditched now, while at the server there is very little increase in code. One of the reasons for that is that no matter how intelligent the app at the client side is, the server still needs to check the complete business logic. With a thich client, the logistics of data-synchronization between server and client becomes very intricate. What we have now, is something that has been built from the ground up again . We have total control over all aspects of the app, its data, and the servers at which it runs. We can connect it to other apps, define new microservices around it quite easily. So, the price has been high, much higher than I expected (and promised), but I think we've got something to build on.","title":"This approach - alternative approaches"},{"location":"About/Lessons/#the-learning-curve-for-what-it-is-worth","text":"When I started writing, I had the experience of developing SHEBANQ . At first, the tools I used for SHEBANQ were a model for developing this contrib tool. From the start it was clear that the contrib tool needed more profound underpinnings. I started out to write those underpinnings myself, programmed in pure, modern Javascript. That worked to a certain extent, but I doubted whether it was strong enough to carry the weight of the full app. After a while I started a big search, trying Google's Angular, Facebook's React, and various solutions that combined these frameworks in so-called full-stack setups, before I went back to server side coding in Python. Here are some lessons I learned during what followed. CRUD layer Some operations are so ubiquitous, that you have to program them once and for all: create/update/delete/read of database items (CRUD), all subject to user permissions. All things that are particular to specific tables and fields must be specified as configuration, all actions must read the configs and carry out generic code. Well, that has proved sub-optimal. The CRUD engine become far too abstract and complex to be tamed, and it become virtually impossible to add new functionality with any level of confidence. Now I have a CRUD machine for the normal, straightforward things. Anything that goes beyond that it hard-coded in derived classes that can inherit from the generic CRUD classes. Custom Workflow layer You cannot do all the business logic this way, without overloading your nice generic system, and even the specific coding. There is still too much structure in the specifics, and that can be captured by a workflow level.","title":"The learning curve (for what it is worth)"},{"location":"About/News/","text":"News \u00b6 2019-11-09 \u00b6 A complete redesign is nearing completion. We started a brand new repo, but the old work and its history is still at the old location","title":"News"},{"location":"About/News/#news","text":"","title":"News"},{"location":"About/News/#2019-11-09","text":"A complete redesign is nearing completion. We started a brand new repo, but the old work and its history is still at the old location","title":"2019-11-09"},{"location":"About/Stats/","text":"cloc github.com/AlDanial/cloc v 1.82 T=0.44 s (215.5 files/s, 36005.0 lines/s) Language files blank comment code Python 37 1505 624 5918 YAML 23 135 7 2975 Markdown 18 606 0 1728 CSS 7 66 9 1182 JavaScript 2 41 6 607 Bourne Shell 6 44 56 131 HTML 1 2 0 61 -------- -------- -------- -------- -------- SUM: 94 2399 702 12602","title":"Stats"},{"location":"Concepts/Model/","text":"Model \u00b6 This application contains a generic engine to display MongoDB data according to any specified data model, respecting access privileges. MongoDB \u00b6 We store the data in a MongoDB . Data as documents A MongoDB does not work with a fixed schema. A MongoDB collection consists of documents , which are essentially JSON-like structures, arbitrarily large and arbitrarily nested. That makes it easy to add new kinds of data to documents and collections when the need arises to do so. This will not break the existing code. MongoDB is optimized to read quickly, at the cost of more expensive data manipulation operations. Its documentation favours storing related data inside the main document. This increases the redundancy of the data and may lead to consistency problems, unless the application tries to enforce consistency somehow. In this app, with a limited amount of data, we use MongoDB primarily for its flexibility. We still adhere largely to SQL-like practices when we deal with related tables. So instead of storing the information of related documents directly inside the main document, we only store references to related documents inside the main documents. Terminology Because our treatment of data is still very relational, we prefer wording derived from SQL databases, at least in the present documentation: MongoDB SQL collection table document record Data model \u00b6 The data model has a generic part and a table specific part. The generic part is specified in tables.yaml . classification of tables user tables The main tables that receive user content: contrib , assessment , review . user entry tables Also tables that receive user content, namely the entries users make in assessments and reviews: criteriaEntry and reviewEntry . valueTables Tables that define the values for fields in other tables, such as discipline , keyword , tadirahObject . The user table is also a value table. systemTables A subset of the value tables: decision and permissionGroup . Essential for the integrity of the business logic. details Specification of detail records: which tables act as details for which masters? master-detail A record may have detail records associated with it. We call such a record a master record with respect to its details. Details are records, usually in another table, having a field that points to their master record by means of an _id value. Convention Whenever possible, the field in a detail table that points to a master is named after the master table. cascade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 When a master record is deleted, its details have a dangling reference to a non-existing master record. In some cases it is desirable to delete the detail records as well. ??? example \"criteriaEntry\" *criteriaEntry* records are deleted with their master: an assessment record. ??? example \"criteria\" *criteria* records are *not* deleted with their master record. ???+ hint \"Deletion prohibited\" In all cases where a record has dependencies, deletion of such a record is prohibited, unless all of its dependencies are marked for cascade-deletion. In order to remove a contribution with assessments and reviews, you first have to delete all its assessments and reviews. provenance Fields for recording the edit history of a record. provenanceSpecs Field specifications for the provenance fields. We have these fields: editors List of ids of non-owner users that may edit this record, even if their group permissions do not allow it. creator Id of the user that created the record. dateCreated Datetime when the record was created. modified Trail of modification events, each event has the name of the user who caused the change and the datetime when it happened. The trail will be weeded out over time. The field \"editors\" may be changed by the owner of a record, and by people with higher powers such as the backoffice, not by the editors themselves (unless they also have higher power). All other fields cannot be modified by users, not even by users with higher powers. Only the system itself will write and update these fields. Backdoors A person with access to the underlying Mongo DB can do with the data what (s)he wants. This requires a direct interaction with the machine on which the database resides. Webaccess is not sufficient. The table specific models are in the yaml files in tables . The table models consist of the specifications of the fields in that table. For each field there is a key under which some specs are written. fieldSpecs label A user-friendly display name of the field. type The data type of the field. It can be a plain data type, or the name of a value table that contains the possible values of this field. Possible plain types are: text A string of characters, usually just a one-liner. url A syntactically valid URL: i.e. a string of text that can be interpreted as a URL. A validation routine will check this. email A syntactically valid email address. A validation routine will check this. markdown A string of characters, which may extend to several pages, formaated as Markdown text. bool2 true or false . bool3 true , null , or false . int An integer number. decimal An decimal number. money An decimal number with an implicit monetary unit: \u20ac. datetime A date time, mostly represented in its ISO 8601 format. Related values When a field refers to other records, there is much more to specify. In this case tType is the name of a value table. multiple Whether there is only one value allowed for this field, or a list of values. perm Who has read and edit access to this field? Conventions The specification is greatly simplified by conventions. Only what deviates from the following conventions needs to be specified: label : same as table name, first letter capitalized type : text multiple : false perm.read : public perm.edit : edit , i.e. the creator and the editors of the record. See below for more about permissions. Permission model \u00b6 The authorization system is built on the basis of groups and permission levels. Users are assigned to groups, and things require permission levels. When a user wants to act upon a thing, his/her group will be compared to the permission level of the thing, and based on the outcome of the comparison, the action will be allowed or forbidden. The configuration of the permissions system as a whole is in perm . The the table-specific permissions are under the perm keys of the table config files mentioned above. Groups Under the key roles the groups and pseudo groups are given. Here is a short description. group is pseudo description nobody no deliberately empty: no user is member of this group public no user, not logged in auth no authenticated user edit yes authenticated user and editor of records in question own yes authenticated user and creator of records in question coord no national coordinator office no back office user system no system administrator root no full control Explanation Groups are attributes of users, as an indication of the power they have. Informally, we need to distinguish between: Nobody nobody is a group without users, and if there were users, they could not do anything. Useful in cases where you want to state that something is not permitted to anybody. The public public is a group for unidentified an unauthenticated users. They can only list/read public information and have no right to edit anything and can do no actions that change anything in the database. Authenticated users auth is the group of DARIAH users authenticated by the DARIAH Identity provider. This is the default group for logged-in users. They can see DARIAH internal information (within limits) an can add items and then modify/delete them, within limits. National coordinators coord is the group of National Coordinators. They are DARIAH users that coordinate the DARIAH outputs for an entire member country. They can (de)select contributions and see their cost fields but only for contributions in the countries they coordinate. Backoffice employees office is the group of users that work for the DARIAH ERIC office. They can modify records created by others (within limits), but cannot perform technical actions that affect the system. System managers sysadmin is the group of users that control the system, not only through the interface of the app, but also with low-level access to the database and the machine that serves the app. Can modify system-technical records, within limits. root root is the one user that can bootstrap the system. Complete rights. Still there are untouchable things, that would compromise the integrity of the system. Even root cannot modify those from within the system. Root is the owner of the system, and can assign people to the roles of system managers and backoffice employees. From there on, these latter groups can do everything that is needed for the day-to-day operation of the functions that the system is designed to fulfill. Pseudo groups In some cases, the identity of the user is relevant, namely when users have a special relationship to the records they want to modify, such as ownership , editorship , etc. When those relationships apply, users are put in a pseudo group such own or edit . Conventions For deletion of records we have a convention without exceptions: records in userEntry tables cannot be deleted directly, only as a result of deleting their master record. Users that have created a record or are mentioned in its editors field may also delete that record if no workflow conditions forbid it. Super users may delete records in user tables. Nobody may delete records in value tables. Assigning users to groups Once users are in a group, their permissions are known. But there are also permissions to regulate who may assign groups to users. Not yet implemented These rules are not yet in force in the redesigned system These permissions derive from the groups as well, with a few additional rules: nobody can assign anybody to the group nobody ; a person can only add people to groups that have at most his/her own power; a person can only assign groups to people that have less power than him/herself. Notes Example If you are office , you cannot promote yourself or anyone else to system or root . If you are office , you cannot demote another member of office to the group auth . You cannot demote/promote your peers, or the ones above you. You can demote yourself, but not promote yourself. You can demote people below you. You can promote people below you, but only up to your own level. nobody Note that users in group nobody have no rights. There should be no users in that group, but if by misconfiguration there is a user in that group, (s)he can do nothing. root A consequence of the promotion/demotion rules is that if there is no user in the group root , nobody can be made root from within the system. Likewise, if a user is root, nobody can take away his/her root status, except him/herself. When importing data into the system by means of load.sh you can specify to make a specific user root . Which user that is, is specified in config.yaml , see rootUser . The command is 1 ./load.sh -r to be executed in the home directory on the server. Alternatively, issuing 1 ./load.sh -R will also convert all other root users on the system to office users. Once the root user is in place, (s)he can assign system admins and back office people. Once those are in place, the daily governance of the system can take place. Name handling \u00b6 The problem There are a lot of names in these yaml files. The most obvious way to use them in our programs (Python on the server, JavaScript on the client) is by just mentioning them as strings, e.g.: 1 title = DM [ 'tables' ][ 'permissionGroup' ][ 'title' ] and 1 title = DM . tables . permissionGroup . title or 1 const { tables : { permissionGroup : { title } } } = DM But then the question arises: how can we use these names in our programs in such a way that we are protected agains typos? Partial solution We tackle this problem in the server code, but not in the client code. Python Well, we convert the .yaml model files to Python modules that expose the same model, but now as Python data structure. This is done by means of the config.py script, just before starting the server. That enables us to collect the names and generate some code. Every part of the .yaml files that may act as a name, is collected. We then define a class Names that contains a member name = ' name ' for each name . So whenever we want to refer to a name in one of the models, we have a Python variable in our name space that is equal to that name prepended with N. . By consequently using N. name instead of a plain string, we guard ourselves against typos, because the Python parser will complain about undefined variables.","title":"Model"},{"location":"Concepts/Model/#model","text":"This application contains a generic engine to display MongoDB data according to any specified data model, respecting access privileges.","title":"Model"},{"location":"Concepts/Model/#mongodb","text":"We store the data in a MongoDB . Data as documents A MongoDB does not work with a fixed schema. A MongoDB collection consists of documents , which are essentially JSON-like structures, arbitrarily large and arbitrarily nested. That makes it easy to add new kinds of data to documents and collections when the need arises to do so. This will not break the existing code. MongoDB is optimized to read quickly, at the cost of more expensive data manipulation operations. Its documentation favours storing related data inside the main document. This increases the redundancy of the data and may lead to consistency problems, unless the application tries to enforce consistency somehow. In this app, with a limited amount of data, we use MongoDB primarily for its flexibility. We still adhere largely to SQL-like practices when we deal with related tables. So instead of storing the information of related documents directly inside the main document, we only store references to related documents inside the main documents. Terminology Because our treatment of data is still very relational, we prefer wording derived from SQL databases, at least in the present documentation: MongoDB SQL collection table document record","title":"MongoDB"},{"location":"Concepts/Model/#data-model","text":"The data model has a generic part and a table specific part. The generic part is specified in tables.yaml . classification of tables user tables The main tables that receive user content: contrib , assessment , review . user entry tables Also tables that receive user content, namely the entries users make in assessments and reviews: criteriaEntry and reviewEntry . valueTables Tables that define the values for fields in other tables, such as discipline , keyword , tadirahObject . The user table is also a value table. systemTables A subset of the value tables: decision and permissionGroup . Essential for the integrity of the business logic. details Specification of detail records: which tables act as details for which masters? master-detail A record may have detail records associated with it. We call such a record a master record with respect to its details. Details are records, usually in another table, having a field that points to their master record by means of an _id value. Convention Whenever possible, the field in a detail table that points to a master is named after the master table. cascade 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 When a master record is deleted, its details have a dangling reference to a non-existing master record. In some cases it is desirable to delete the detail records as well. ??? example \"criteriaEntry\" *criteriaEntry* records are deleted with their master: an assessment record. ??? example \"criteria\" *criteria* records are *not* deleted with their master record. ???+ hint \"Deletion prohibited\" In all cases where a record has dependencies, deletion of such a record is prohibited, unless all of its dependencies are marked for cascade-deletion. In order to remove a contribution with assessments and reviews, you first have to delete all its assessments and reviews. provenance Fields for recording the edit history of a record. provenanceSpecs Field specifications for the provenance fields. We have these fields: editors List of ids of non-owner users that may edit this record, even if their group permissions do not allow it. creator Id of the user that created the record. dateCreated Datetime when the record was created. modified Trail of modification events, each event has the name of the user who caused the change and the datetime when it happened. The trail will be weeded out over time. The field \"editors\" may be changed by the owner of a record, and by people with higher powers such as the backoffice, not by the editors themselves (unless they also have higher power). All other fields cannot be modified by users, not even by users with higher powers. Only the system itself will write and update these fields. Backdoors A person with access to the underlying Mongo DB can do with the data what (s)he wants. This requires a direct interaction with the machine on which the database resides. Webaccess is not sufficient. The table specific models are in the yaml files in tables . The table models consist of the specifications of the fields in that table. For each field there is a key under which some specs are written. fieldSpecs label A user-friendly display name of the field. type The data type of the field. It can be a plain data type, or the name of a value table that contains the possible values of this field. Possible plain types are: text A string of characters, usually just a one-liner. url A syntactically valid URL: i.e. a string of text that can be interpreted as a URL. A validation routine will check this. email A syntactically valid email address. A validation routine will check this. markdown A string of characters, which may extend to several pages, formaated as Markdown text. bool2 true or false . bool3 true , null , or false . int An integer number. decimal An decimal number. money An decimal number with an implicit monetary unit: \u20ac. datetime A date time, mostly represented in its ISO 8601 format. Related values When a field refers to other records, there is much more to specify. In this case tType is the name of a value table. multiple Whether there is only one value allowed for this field, or a list of values. perm Who has read and edit access to this field? Conventions The specification is greatly simplified by conventions. Only what deviates from the following conventions needs to be specified: label : same as table name, first letter capitalized type : text multiple : false perm.read : public perm.edit : edit , i.e. the creator and the editors of the record. See below for more about permissions.","title":"Data model"},{"location":"Concepts/Model/#permission-model","text":"The authorization system is built on the basis of groups and permission levels. Users are assigned to groups, and things require permission levels. When a user wants to act upon a thing, his/her group will be compared to the permission level of the thing, and based on the outcome of the comparison, the action will be allowed or forbidden. The configuration of the permissions system as a whole is in perm . The the table-specific permissions are under the perm keys of the table config files mentioned above. Groups Under the key roles the groups and pseudo groups are given. Here is a short description. group is pseudo description nobody no deliberately empty: no user is member of this group public no user, not logged in auth no authenticated user edit yes authenticated user and editor of records in question own yes authenticated user and creator of records in question coord no national coordinator office no back office user system no system administrator root no full control Explanation Groups are attributes of users, as an indication of the power they have. Informally, we need to distinguish between: Nobody nobody is a group without users, and if there were users, they could not do anything. Useful in cases where you want to state that something is not permitted to anybody. The public public is a group for unidentified an unauthenticated users. They can only list/read public information and have no right to edit anything and can do no actions that change anything in the database. Authenticated users auth is the group of DARIAH users authenticated by the DARIAH Identity provider. This is the default group for logged-in users. They can see DARIAH internal information (within limits) an can add items and then modify/delete them, within limits. National coordinators coord is the group of National Coordinators. They are DARIAH users that coordinate the DARIAH outputs for an entire member country. They can (de)select contributions and see their cost fields but only for contributions in the countries they coordinate. Backoffice employees office is the group of users that work for the DARIAH ERIC office. They can modify records created by others (within limits), but cannot perform technical actions that affect the system. System managers sysadmin is the group of users that control the system, not only through the interface of the app, but also with low-level access to the database and the machine that serves the app. Can modify system-technical records, within limits. root root is the one user that can bootstrap the system. Complete rights. Still there are untouchable things, that would compromise the integrity of the system. Even root cannot modify those from within the system. Root is the owner of the system, and can assign people to the roles of system managers and backoffice employees. From there on, these latter groups can do everything that is needed for the day-to-day operation of the functions that the system is designed to fulfill. Pseudo groups In some cases, the identity of the user is relevant, namely when users have a special relationship to the records they want to modify, such as ownership , editorship , etc. When those relationships apply, users are put in a pseudo group such own or edit . Conventions For deletion of records we have a convention without exceptions: records in userEntry tables cannot be deleted directly, only as a result of deleting their master record. Users that have created a record or are mentioned in its editors field may also delete that record if no workflow conditions forbid it. Super users may delete records in user tables. Nobody may delete records in value tables. Assigning users to groups Once users are in a group, their permissions are known. But there are also permissions to regulate who may assign groups to users. Not yet implemented These rules are not yet in force in the redesigned system These permissions derive from the groups as well, with a few additional rules: nobody can assign anybody to the group nobody ; a person can only add people to groups that have at most his/her own power; a person can only assign groups to people that have less power than him/herself. Notes Example If you are office , you cannot promote yourself or anyone else to system or root . If you are office , you cannot demote another member of office to the group auth . You cannot demote/promote your peers, or the ones above you. You can demote yourself, but not promote yourself. You can demote people below you. You can promote people below you, but only up to your own level. nobody Note that users in group nobody have no rights. There should be no users in that group, but if by misconfiguration there is a user in that group, (s)he can do nothing. root A consequence of the promotion/demotion rules is that if there is no user in the group root , nobody can be made root from within the system. Likewise, if a user is root, nobody can take away his/her root status, except him/herself. When importing data into the system by means of load.sh you can specify to make a specific user root . Which user that is, is specified in config.yaml , see rootUser . The command is 1 ./load.sh -r to be executed in the home directory on the server. Alternatively, issuing 1 ./load.sh -R will also convert all other root users on the system to office users. Once the root user is in place, (s)he can assign system admins and back office people. Once those are in place, the daily governance of the system can take place.","title":"Permission model"},{"location":"Concepts/Model/#name-handling","text":"The problem There are a lot of names in these yaml files. The most obvious way to use them in our programs (Python on the server, JavaScript on the client) is by just mentioning them as strings, e.g.: 1 title = DM [ 'tables' ][ 'permissionGroup' ][ 'title' ] and 1 title = DM . tables . permissionGroup . title or 1 const { tables : { permissionGroup : { title } } } = DM But then the question arises: how can we use these names in our programs in such a way that we are protected agains typos? Partial solution We tackle this problem in the server code, but not in the client code. Python Well, we convert the .yaml model files to Python modules that expose the same model, but now as Python data structure. This is done by means of the config.py script, just before starting the server. That enables us to collect the names and generate some code. Every part of the .yaml files that may act as a name, is collected. We then define a class Names that contains a member name = ' name ' for each name . So whenever we want to refer to a name in one of the models, we have a Python variable in our name space that is equal to that name prepended with N. . By consequently using N. name instead of a plain string, we guard ourselves against typos, because the Python parser will complain about undefined variables.","title":"Name handling"},{"location":"Functionality/Business/","text":"Business Logic \u00b6 Here we document the functionality of the app from the perspective of the users and stakeholders. We focus on the scenarios that are supported. Status of implementation The DARIAH contribution tool is a big experiment in accounting for research releated output of institutions that cooperate in a European Research Infrastructure with limited funding. The development of this tool so far has been a significant amount of work, in a landscape that has been changing in several respects: the underlying goals and expectations the business logic that is needed the technology on which all is based It is likely that further developments will lead to simpler goals, easier business logic, and a simpler implementation. That is why we do not implement all of the initial specs. Some of the not-implemented items we mark with: (\u2717) a single \u2717: but expected to be implemented at some point in the future. (\u2717\u2717) a double \u2717: unsure if it will ever be be impemented. Business content \u00b6 All information regarding the assessment and review of contributions, is in so-called back-office tables: packages , criteria , types . Source of business rules The business tables have been compiled under guidance of the HaS project by Lisa de Leeuw. Dirk Roorda has entered them into a big back office configuration file which will be read by an import script and transported into the MongoDB database. Contributions \u00b6 A contribution is a piece of work in Digital Humanities, delivered by a person or institute, and potentially relevant to the European DARIAH research infrastructure. Selection by National Coordinators The National Coordinators of DARIAH may add such a contribution to their agreed budget of in-kind contributions to DARIAH as a whole. This makes it necessary to assess contributions against a set of well-defined criteria. Assessment scenario \u00b6 Contributions may represent diverse efforts such as consultancy, workshops, software development, and hosting services. Diversification and time dependency This asks for a diversification of contribution types and associated criteria. The assessor of a contribution (from now on called applicant ) needs to state how that contribution scores for each relevant criterion, and for each score, evidence must be given. Moreover, types and criteria may change over time, but during an assessment and review cycle they should be fixed. Packages, types, criteria Contribution types and their associated assessment criteria are represented by a package record. What is a package? A package is a fixed constellation of types and criteria; it defines a set of contribution types, and a set of criteria, and a mapping between criteria and types. Every criterion is linked to a number of contribution types, meaning that the criterion is relevant to contributions of those types and no others. Every criterion is associated with exactly one package, hence the package ultimately determines the mapping between types and criteria. Active packages At any point in time there are one or more active packages, usually just one. Validity interval A package has a validity interval, i.e. a start date and an end date. A package is active at a point in time, if that point in time is inside the validity interval. The types of an active package are the active types, and its criteria are the active criteria. Technically, more than one package can be valid at the same time. In that case, the sets of active types and criteria are the union of the sets of types and criteria for each active package. But the intention is that there is always exactly one active package. Workflow looks at active packages Other components may call workflow functions in order to determine what the active packages, types and criteria are, so they can render inactive and active ones in different ways. Moreover, workflow will prevent certain actions for inactive items. Inactive contribution type Contributions with an inactive type cannot be assessed. If there are already assessments of such a contribution in the system, they will remain in the system, but workflow will mark them as stalled , and they can no longer be edited. In order to assess such a contribution, you have to change its type to an active contribution type. Rationale Time dependent packages of types and criteria allow evolution of insights. If the current classification of contributions into types appears to have shortcomings, it is possible to remedy the types. Also, criteria can be tweaked and rewritten. Evolution of packages If the current package has trivial mistakes, e.g. in wording or spelling, you can modify its criteria and type records. However, the best way to change a package for significant changes is by creating a new package, and associate new types and criteria to it, leaving the current package unchanged. Then set the validity interval to a suitable value. You can let the old and new package overlap for testing purposes. During that interval, the old and new types and criteria are valid. After that, you can terminate the old package by adjusting its validity interval. Assessments \u00b6 Applicants with write-access to a contribution can add a self-assessment to a contribution. A self assessment is a record in the assessment table, and consists of a few metadata fields. Criteria and criteria entry records When an assessment record is created, additional detail records will be created as well. These are criteriaEntry records. For each assessment, there is a fixed set of criteriaEntry records. This set is determined by the currently active set of criteria: one criteriaEntry record will be created per active criterion. A criteriaEntry record has a field for choosing a score and a text field for entering the evidence. Scores are defined in yet another type of record. Assessment scoring \u00b6 Score records The scores for a criterion are entered in with the help of score records, which are detail records of criteria. Scores have a number, typically 0 , 2 , 4 , and a short description, typically None , Partial , Full , but the number and nature of scores may vary freely between criteria. The score of an assessment as a whole is the sum of the individual scores expressed as percentage of the total amount of points that can be assigned. A temporary overall score is obtained by treating unfilled scores as having value 0 . Non applicable scores Some criteria may allow scores with a value -1 (non-applicable). If an assessment assigns that score to a criterion, 0 points are added, but points missed from this criterion will be subtracted from the total score, so that this criterion will not be counted in the average. Example Suppose there are four criteria, A, B, C, D. A, B, and C have scores 0 , 2 , and 4 . D has scores -1 , 0 , 2 , 4 . Now there are two contributions U and V, with scores as follows: Criterion contrib U contrib V A 4 4 B 4 4 C 4 4 D -1 0 sum 12 12 total 12 16 score 100% 75% See how U does better than V although they have an equal number of points. But for U criterion D does not count, while for V it counts, but the score is 0. Note Not all criteria will allow -1 values! Review scenario \u00b6 After a contributor has filled out an assessment, (s)he can submit it for review. The office will select two reviewers, and they will get access to the self-assessment. Upon asking for review, the assessment and the contribution will be locked. Reviewer roles The two reviewers have distinct roles: reviewer 1 (expert) inspects the assessment closely and advises a decision; reviewer 2(final say) makes the decision. (\u2717\u2717) Both reviewers can enter comments in a comment stream, which are detail records of the assessment. The advice/decision that can be made by the reviewers is approve End of review process with positive outcome. The assessment will remain locked. The assessment score will be made public. reject End of review process with negative outcome. The assessment will remain locked. No assessment score will be made public. (\u2717\u2717) The applicant may enter an objection. In that case the back office will ask a second opinion and take appropriate action, which might lead to a change of decision, e.g. towards revise , or to a new review by other reviewers. revise The assessment and contribution will be unlocked, and the applicant can modify both of them in response to comments by the reviewers. When (s)he is finished, the applicant can resubmit the modified version. Selection scenario \u00b6 The National Coordinator of a country can select contributions from his/her country as in-kind contribution of his country to DARIAH for a specific year. Selection may overrule Ideally, only contributions that have been well-reviewed will be selected. But the app also supports the selection of contributions in whatever stage of the assessment/review process. Selection states The national coordinator can select or deselect a contribution. Deselect means: explicitly reject . (S)he can also refrain from making a decision. As a consequence, there are three possible selection states for a contribution: selected deselected undecided Selection interface The contribution record has a button for selecting it. Only NCs and backoffice people can see/use it. There is also an overview page for contributions which show the selected state of them. NCs can use this overview to (de)select the contributions of their country. Revoking selection decisions Once a NC makes a selection decision, (s)he cannot revoke it. As a last resort, a backoffice member can undo a decision, after which the NC gets a new chance to decide. Selection workflow There are no preconditions for selecting a contribution other than that a contribution is not already selected or deselected. After (de)selection, a contribution gets the workflow attribute frozen , which prevents all modifications of that contribution, except changing its selected field (only by backoffice personnel). Also, all its assessments and reviews, including their criteria entry records and review entry records get frozen . Moreover, the contribution will be consolidated, displayable on the interface, (\u2717) and a pdf report can be generated from the consolidated record on demand. Management information \u00b6 The app compiles management information of a statistical nature, both to the public and authenticated users. Access rights The quantity of information given is dependent on user rights. The public can see contributions, but not assessments and reviews, except the ones that are finalized with outcome \"accept\". In those cases, the assessment score is also visible. National coordinators NCs can (se)select contributions from this overview, but only the ones that belong to the country for which they are national coordinator. Left-overs \u00b6 (\u2717\u2717) Email notification It might be handy to send emails to users involved in assessing and reviewing to notify them that a key event has occurred, such as the submission of an assessment, the appointment of reviewers, the decisions by reviewers. Currently, the app does not send mail. password mail Users are able to request a password reset, and will get a mail with a password link. These emails are not sent by the app, but by the DARIAH Authentication Infrastructure. (\u2717\u2717) Concurrent access When multiple users work on the same item, or one user works on the same item in multiple browsers/browser windows/browser tabs, save conflicts may occur. These save conflicts are not handled graciously. The last saver wins. This problem is hard to solve, but it can be mitigated. One way of mitigation is already in the app: whenever a user leaves a field (s)he has been editing, it will be saved to the database. However, in those cases the whole record will be saved, which may lead to more data loss than is strictly necessary. A data loss scenario A user opens a browser tab to edit a contribution record, with the aim to add a keyword. The contribution has no description yet. Before assigning the label, the same user opens the same contribution in another tab and starts writing a lengthy description, and saves it. Then (s)he returns to the first tab and assigns a keyword. Upon saving, the whole record will be saved, including the description, which is still empty. This will overwrite the description saved in the second tab, a moment before. Push notifications An other problem is that important actions such as submission or selection maybe triggered from one tab, without other tabs being aware of that. In such cases, it would be desirable to send push notifications to all browsers that have that record open so that the user can refresh the page. I know it can be done ( socket , python-socket ) but it requires a bit of research to find the best way to do it.","title":"Business"},{"location":"Functionality/Business/#business-logic","text":"Here we document the functionality of the app from the perspective of the users and stakeholders. We focus on the scenarios that are supported. Status of implementation The DARIAH contribution tool is a big experiment in accounting for research releated output of institutions that cooperate in a European Research Infrastructure with limited funding. The development of this tool so far has been a significant amount of work, in a landscape that has been changing in several respects: the underlying goals and expectations the business logic that is needed the technology on which all is based It is likely that further developments will lead to simpler goals, easier business logic, and a simpler implementation. That is why we do not implement all of the initial specs. Some of the not-implemented items we mark with: (\u2717) a single \u2717: but expected to be implemented at some point in the future. (\u2717\u2717) a double \u2717: unsure if it will ever be be impemented.","title":"Business Logic"},{"location":"Functionality/Business/#business-content","text":"All information regarding the assessment and review of contributions, is in so-called back-office tables: packages , criteria , types . Source of business rules The business tables have been compiled under guidance of the HaS project by Lisa de Leeuw. Dirk Roorda has entered them into a big back office configuration file which will be read by an import script and transported into the MongoDB database.","title":"Business content"},{"location":"Functionality/Business/#contributions","text":"A contribution is a piece of work in Digital Humanities, delivered by a person or institute, and potentially relevant to the European DARIAH research infrastructure. Selection by National Coordinators The National Coordinators of DARIAH may add such a contribution to their agreed budget of in-kind contributions to DARIAH as a whole. This makes it necessary to assess contributions against a set of well-defined criteria.","title":"Contributions"},{"location":"Functionality/Business/#assessment-scenario","text":"Contributions may represent diverse efforts such as consultancy, workshops, software development, and hosting services. Diversification and time dependency This asks for a diversification of contribution types and associated criteria. The assessor of a contribution (from now on called applicant ) needs to state how that contribution scores for each relevant criterion, and for each score, evidence must be given. Moreover, types and criteria may change over time, but during an assessment and review cycle they should be fixed. Packages, types, criteria Contribution types and their associated assessment criteria are represented by a package record. What is a package? A package is a fixed constellation of types and criteria; it defines a set of contribution types, and a set of criteria, and a mapping between criteria and types. Every criterion is linked to a number of contribution types, meaning that the criterion is relevant to contributions of those types and no others. Every criterion is associated with exactly one package, hence the package ultimately determines the mapping between types and criteria. Active packages At any point in time there are one or more active packages, usually just one. Validity interval A package has a validity interval, i.e. a start date and an end date. A package is active at a point in time, if that point in time is inside the validity interval. The types of an active package are the active types, and its criteria are the active criteria. Technically, more than one package can be valid at the same time. In that case, the sets of active types and criteria are the union of the sets of types and criteria for each active package. But the intention is that there is always exactly one active package. Workflow looks at active packages Other components may call workflow functions in order to determine what the active packages, types and criteria are, so they can render inactive and active ones in different ways. Moreover, workflow will prevent certain actions for inactive items. Inactive contribution type Contributions with an inactive type cannot be assessed. If there are already assessments of such a contribution in the system, they will remain in the system, but workflow will mark them as stalled , and they can no longer be edited. In order to assess such a contribution, you have to change its type to an active contribution type. Rationale Time dependent packages of types and criteria allow evolution of insights. If the current classification of contributions into types appears to have shortcomings, it is possible to remedy the types. Also, criteria can be tweaked and rewritten. Evolution of packages If the current package has trivial mistakes, e.g. in wording or spelling, you can modify its criteria and type records. However, the best way to change a package for significant changes is by creating a new package, and associate new types and criteria to it, leaving the current package unchanged. Then set the validity interval to a suitable value. You can let the old and new package overlap for testing purposes. During that interval, the old and new types and criteria are valid. After that, you can terminate the old package by adjusting its validity interval.","title":"Assessment scenario"},{"location":"Functionality/Business/#assessments","text":"Applicants with write-access to a contribution can add a self-assessment to a contribution. A self assessment is a record in the assessment table, and consists of a few metadata fields. Criteria and criteria entry records When an assessment record is created, additional detail records will be created as well. These are criteriaEntry records. For each assessment, there is a fixed set of criteriaEntry records. This set is determined by the currently active set of criteria: one criteriaEntry record will be created per active criterion. A criteriaEntry record has a field for choosing a score and a text field for entering the evidence. Scores are defined in yet another type of record.","title":"Assessments"},{"location":"Functionality/Business/#assessment-scoring","text":"Score records The scores for a criterion are entered in with the help of score records, which are detail records of criteria. Scores have a number, typically 0 , 2 , 4 , and a short description, typically None , Partial , Full , but the number and nature of scores may vary freely between criteria. The score of an assessment as a whole is the sum of the individual scores expressed as percentage of the total amount of points that can be assigned. A temporary overall score is obtained by treating unfilled scores as having value 0 . Non applicable scores Some criteria may allow scores with a value -1 (non-applicable). If an assessment assigns that score to a criterion, 0 points are added, but points missed from this criterion will be subtracted from the total score, so that this criterion will not be counted in the average. Example Suppose there are four criteria, A, B, C, D. A, B, and C have scores 0 , 2 , and 4 . D has scores -1 , 0 , 2 , 4 . Now there are two contributions U and V, with scores as follows: Criterion contrib U contrib V A 4 4 B 4 4 C 4 4 D -1 0 sum 12 12 total 12 16 score 100% 75% See how U does better than V although they have an equal number of points. But for U criterion D does not count, while for V it counts, but the score is 0. Note Not all criteria will allow -1 values!","title":"Assessment scoring"},{"location":"Functionality/Business/#review-scenario","text":"After a contributor has filled out an assessment, (s)he can submit it for review. The office will select two reviewers, and they will get access to the self-assessment. Upon asking for review, the assessment and the contribution will be locked. Reviewer roles The two reviewers have distinct roles: reviewer 1 (expert) inspects the assessment closely and advises a decision; reviewer 2(final say) makes the decision. (\u2717\u2717) Both reviewers can enter comments in a comment stream, which are detail records of the assessment. The advice/decision that can be made by the reviewers is approve End of review process with positive outcome. The assessment will remain locked. The assessment score will be made public. reject End of review process with negative outcome. The assessment will remain locked. No assessment score will be made public. (\u2717\u2717) The applicant may enter an objection. In that case the back office will ask a second opinion and take appropriate action, which might lead to a change of decision, e.g. towards revise , or to a new review by other reviewers. revise The assessment and contribution will be unlocked, and the applicant can modify both of them in response to comments by the reviewers. When (s)he is finished, the applicant can resubmit the modified version.","title":"Review scenario"},{"location":"Functionality/Business/#selection-scenario","text":"The National Coordinator of a country can select contributions from his/her country as in-kind contribution of his country to DARIAH for a specific year. Selection may overrule Ideally, only contributions that have been well-reviewed will be selected. But the app also supports the selection of contributions in whatever stage of the assessment/review process. Selection states The national coordinator can select or deselect a contribution. Deselect means: explicitly reject . (S)he can also refrain from making a decision. As a consequence, there are three possible selection states for a contribution: selected deselected undecided Selection interface The contribution record has a button for selecting it. Only NCs and backoffice people can see/use it. There is also an overview page for contributions which show the selected state of them. NCs can use this overview to (de)select the contributions of their country. Revoking selection decisions Once a NC makes a selection decision, (s)he cannot revoke it. As a last resort, a backoffice member can undo a decision, after which the NC gets a new chance to decide. Selection workflow There are no preconditions for selecting a contribution other than that a contribution is not already selected or deselected. After (de)selection, a contribution gets the workflow attribute frozen , which prevents all modifications of that contribution, except changing its selected field (only by backoffice personnel). Also, all its assessments and reviews, including their criteria entry records and review entry records get frozen . Moreover, the contribution will be consolidated, displayable on the interface, (\u2717) and a pdf report can be generated from the consolidated record on demand.","title":"Selection scenario"},{"location":"Functionality/Business/#management-information","text":"The app compiles management information of a statistical nature, both to the public and authenticated users. Access rights The quantity of information given is dependent on user rights. The public can see contributions, but not assessments and reviews, except the ones that are finalized with outcome \"accept\". In those cases, the assessment score is also visible. National coordinators NCs can (se)select contributions from this overview, but only the ones that belong to the country for which they are national coordinator.","title":"Management information"},{"location":"Functionality/Business/#left-overs","text":"(\u2717\u2717) Email notification It might be handy to send emails to users involved in assessing and reviewing to notify them that a key event has occurred, such as the submission of an assessment, the appointment of reviewers, the decisions by reviewers. Currently, the app does not send mail. password mail Users are able to request a password reset, and will get a mail with a password link. These emails are not sent by the app, but by the DARIAH Authentication Infrastructure. (\u2717\u2717) Concurrent access When multiple users work on the same item, or one user works on the same item in multiple browsers/browser windows/browser tabs, save conflicts may occur. These save conflicts are not handled graciously. The last saver wins. This problem is hard to solve, but it can be mitigated. One way of mitigation is already in the app: whenever a user leaves a field (s)he has been editing, it will be saved to the database. However, in those cases the whole record will be saved, which may lead to more data loss than is strictly necessary. A data loss scenario A user opens a browser tab to edit a contribution record, with the aim to add a keyword. The contribution has no description yet. Before assigning the label, the same user opens the same contribution in another tab and starts writing a lengthy description, and saves it. Then (s)he returns to the first tab and assigns a keyword. Upon saving, the whole record will be saved, including the description, which is still empty. This will overwrite the description saved in the second tab, a moment before. Push notifications An other problem is that important actions such as submission or selection maybe triggered from one tab, without other tabs being aware of that. In such cases, it would be desirable to send push notifications to all browsers that have that record open so that the user can refresh the page. I know it can be done ( socket , python-socket ) but it requires a bit of research to find the best way to do it.","title":"Left-overs"},{"location":"Functionality/Tables/","text":"Tables with custom logic \u00b6 Here are the particulars of our tables. contrib \u00b6 assessment \u00b6 In particular the current score of the assessment is presented here. The score is computed workflow function computeScore and presented by presentScore . Not only the score is presented, but also its derivation. Submission \u00b6 It is presented whether the assessment currently counts as submitted for review, and if yes, also the date-time of the last submission. In this case there is also a button to withdraw the assessment from review. If the assessment does not count as submitted, a submit button is presented. Permissions This is not the whole truth, the presence of these action buttons is dependent on additional constraints, such as whether the current user has rights to submit, and whether the assessment is complete. It can also be the case that the assessment has been reviewed with outcome revise . In that case, the submit button changes into an Enter revisions button, and later to Submit for review (again) . Stalled If the contribution has received an other type since the creation of this assessment, this assessment will count as stalled , and cannot be used for review. In this case, the criteria of the assessment are not the criteria by which the contribution should be assessed. So the system stalls this assessment. It is doomed, it can never be submitted. Unless you decide to change back the type of the contribution. If that is not an option, the best thing to do is to copy the worthwhile material from this assessment into a fresh assessment. criteriaEntry \u00b6 These records are meant to be shown as detail records of an assessment. As such, they are part of a big form. Each record is a row in that form in which the user can enter a score and state evidence for that score. The display of the rows is such that completed entries are clearly differentiated from incomplete ones. review \u00b6 The biggest task for review templates is to show the reviews of both reviewers side by side, and to make the review editable for the corresponding reviewer. In doing so, the app needs to know the exact stage the review process is in, to be able to temporarily lock reviews when they are considered by the final reviewer. This is responsible to present the reviewers with controls to make their decisions, and present to other users the effects of those decisions. reviewEntry \u00b6 These records are meant to be shown as detail records of a review. As such, they are part of a big form. Each record is a row in that form in which the user can enter review comments.","title":"Tables"},{"location":"Functionality/Tables/#tables-with-custom-logic","text":"Here are the particulars of our tables.","title":"Tables with custom logic"},{"location":"Functionality/Tables/#contrib","text":"","title":"contrib"},{"location":"Functionality/Tables/#assessment","text":"In particular the current score of the assessment is presented here. The score is computed workflow function computeScore and presented by presentScore . Not only the score is presented, but also its derivation.","title":"assessment"},{"location":"Functionality/Tables/#submission","text":"It is presented whether the assessment currently counts as submitted for review, and if yes, also the date-time of the last submission. In this case there is also a button to withdraw the assessment from review. If the assessment does not count as submitted, a submit button is presented. Permissions This is not the whole truth, the presence of these action buttons is dependent on additional constraints, such as whether the current user has rights to submit, and whether the assessment is complete. It can also be the case that the assessment has been reviewed with outcome revise . In that case, the submit button changes into an Enter revisions button, and later to Submit for review (again) . Stalled If the contribution has received an other type since the creation of this assessment, this assessment will count as stalled , and cannot be used for review. In this case, the criteria of the assessment are not the criteria by which the contribution should be assessed. So the system stalls this assessment. It is doomed, it can never be submitted. Unless you decide to change back the type of the contribution. If that is not an option, the best thing to do is to copy the worthwhile material from this assessment into a fresh assessment.","title":"Submission"},{"location":"Functionality/Tables/#criteriaentry","text":"These records are meant to be shown as detail records of an assessment. As such, they are part of a big form. Each record is a row in that form in which the user can enter a score and state evidence for that score. The display of the rows is such that completed entries are clearly differentiated from incomplete ones.","title":"criteriaEntry"},{"location":"Functionality/Tables/#review","text":"The biggest task for review templates is to show the reviews of both reviewers side by side, and to make the review editable for the corresponding reviewer. In doing so, the app needs to know the exact stage the review process is in, to be able to temporarily lock reviews when they are considered by the final reviewer. This is responsible to present the reviewers with controls to make their decisions, and present to other users the effects of those decisions.","title":"review"},{"location":"Functionality/Tables/#reviewentry","text":"These records are meant to be shown as detail records of a review. As such, they are part of a big form. Each record is a row in that form in which the user can enter review comments.","title":"reviewEntry"},{"location":"Functionality/Workflow/","text":"Workflow Engine \u00b6 Description \u00b6 The workflow engine of this app is a system to handle business logic. Whereas the database consists of neutral things (fields, records, lists), the workflow engine weaves additional attributes around it, that indicate additional constraints. These additional workflow attributes are computed by the server on the fly, and then stored in a separate table in the database: workflow . From then on the following happens with the workflow attributes: the server uses the workflow info to enforce the business logic; the server updates the workflow attributes after any insert/update/delete action. Realization \u00b6 Workflow is realized at the server. Its rules are specified in the code that is specific for the concepts of contribution, assessment, and review. The engine itself is in workflow . compute.py is responsible for computiong the workflow attributes for each contribution, and updating them after changes. apply.py is used to apply and enforce the workflow rules.","title":"Workflow"},{"location":"Functionality/Workflow/#workflow-engine","text":"","title":"Workflow Engine"},{"location":"Functionality/Workflow/#description","text":"The workflow engine of this app is a system to handle business logic. Whereas the database consists of neutral things (fields, records, lists), the workflow engine weaves additional attributes around it, that indicate additional constraints. These additional workflow attributes are computed by the server on the fly, and then stored in a separate table in the database: workflow . From then on the following happens with the workflow attributes: the server uses the workflow info to enforce the business logic; the server updates the workflow attributes after any insert/update/delete action.","title":"Description"},{"location":"Functionality/Workflow/#realization","text":"Workflow is realized at the server. Its rules are specified in the code that is specific for the concepts of contribution, assessment, and review. The engine itself is in workflow . compute.py is responsible for computiong the workflow attributes for each contribution, and updating them after changes. apply.py is used to apply and enforce the workflow rules.","title":"Realization"},{"location":"Integration/API/","text":"API \u00b6 Outdated These API specs are currently outdated. All API calls are structured like this: https://dariah-beta.dans.knaw.nl /api/db/ verb ? parameters Below there is a partial specification of the verbs and their parameters. Permissions Data access is controlled. You only get the data you have rights to access. If you fetch records, it depends on your access level which records and which fields are being returned. The contribution tool itself uses this API to feed itself with data. Source code In those cases where this documentation fails to give the information you need you might want to look into the source code: index.py list \u00b6 list?table= table name &complete= false or true task Get the records of the table with name table name . Details If complete=false , fetch only the titles of each record. Otherwise, fetch all fields that you are entitled to read. The result is a json object, containing sub objects for the specification of the data model of this table. The actual records are under entities , keyed by their MongoDB _id . Per entity, the fields can be found under the key values . view a table https://dariah-beta.dans.knaw.nl/api/db/list?table=contrib&complete=true view \u00b6 view?table= table name &id= mongoId task Get an individual item from the table with name table name , and identifier mongoId , having all fields you are entitled to read. view an item https://dariah-beta.dans.knaw.nl/api/db/view?table=contrib&id=5bab57edb5dbf5258908b315","title":"API"},{"location":"Integration/API/#api","text":"Outdated These API specs are currently outdated. All API calls are structured like this: https://dariah-beta.dans.knaw.nl /api/db/ verb ? parameters Below there is a partial specification of the verbs and their parameters. Permissions Data access is controlled. You only get the data you have rights to access. If you fetch records, it depends on your access level which records and which fields are being returned. The contribution tool itself uses this API to feed itself with data. Source code In those cases where this documentation fails to give the information you need you might want to look into the source code: index.py","title":"API"},{"location":"Integration/API/#list","text":"list?table= table name &complete= false or true task Get the records of the table with name table name . Details If complete=false , fetch only the titles of each record. Otherwise, fetch all fields that you are entitled to read. The result is a json object, containing sub objects for the specification of the data model of this table. The actual records are under entities , keyed by their MongoDB _id . Per entity, the fields can be found under the key values . view a table https://dariah-beta.dans.knaw.nl/api/db/list?table=contrib&complete=true","title":"list"},{"location":"Integration/API/#view","text":"view?table= table name &id= mongoId task Get an individual item from the table with name table name , and identifier mongoId , having all fields you are entitled to read. view an item https://dariah-beta.dans.knaw.nl/api/db/view?table=contrib&id=5bab57edb5dbf5258908b315","title":"view"},{"location":"Legacy/Content/","text":"Initial Content \u00b6 There are already 800 contributions in the system. They have been collected in a FileMaker database in the past. Import legacy content We convert this content and use it for an initial filling of the contribution tool. The legacy import is automated and repeatable, even into a database that has been used in production for a while. Legacy contributions \u00b6 The legacy content for this application consists of a FileMaker database. What is in the database? In it there is a web of tables and value lists. The essential content is a contribution table containing 800 contributions. There is also a bit of assessment information. How do we get that data out? From FileMaker to XML We have exported tables and value lists as XML. This is a manual and clumsy process. XML to Mongo DB The machinery for this step is programmed in a Python script, and the configuration details are spelled out in a config . It reads the XML, extracts the field definitions from it, and reads the data from the fields in the rows. We then do the following: adapt the table and field organization; adjust the field types and the values, especially for datetime and currency; generate value tables and cross-tables; add extra information for countries, so that they can be visualized on a map; link values to existing tables; import a moderately denormalized version of the data into MongoDB. Importing and reimporting The source data model is complex, the target data model is complex, and the app as a whole must support a complex workflow. It is impossible to design everything up-front, so we need to be able to retrace our steps and redo the import. As long as the system is not in production, we can just regenerate the database whenever needed, thereby loosing all manual modifications. But there comes a time, and it has arrived now, that people want to experiment with the data. But the app is not finished yet, and maybe there are more design jumps to make. So we need an import script that can reimport the initial data without disturbing the new data. We have written mongoFromFm.py that does exactly this. From transfer to import We started out running the import script in the development situation, populating a MongoDB instance there, dumping its data, and bulk-importing that into the production instance. The problem with that is that the production system will have a different set of users than the development system. Now contributions get tied to users, so if we move over contributions without users, their creator fields will dangle. It turns out to be much better to use the import script also in the production situation. So we ship the FileMaker input for the script to the production server, and run the import there, with slightly different settings. An additional advantage is, that we replace a coarse bulk import by a much more intelligent and sensitive approach: we add the records programmatically, and we have a chance to look before we act. Requirements The task for the import script boils down to these requirements: records that have been manually modified in the target system MAY NOT be overwritten; existing relationships between records MUST be preserved. See later, under Discussion how this is achieved. Usage Production 1 python3 mongoFromFm.py production Development 1 python3 mongoFromFm.py development Extras in development mode In development mode, the following things happen: excel spreadsheets with the original FileMaker data and the resulting MongoDB data are generated; a bunch of test users is added; the ownership of some contributions is changed to the developer, for ease of testing. Discussion \u00b6 The main idea is that all records that come out of the conversion progress, are marked as pristine . Later, when a record is changed under the influence of the tool, this mark is removed. Preventing data loss All records generated by this program will have a field isPristine , set to true . The DARIAH contribution tool will remove this field from a record after it has modified it. This import tool does not delete databases, nor collections, only individual documents. Before import, an inspection is made: the ids of the existing records are retrieved. The ids will be classified: pristine , non-pristine , troublesome . Troublesome means: not pristine, and occurring in the records to be imported. The following will happen: Existing pristine records will be deleted. The import records will be filtered: the ones with troublesome ids will be left out. The filtered import records will be inserted. This guarantees that there is no data loss: no records that have been touched by the application are deleted nor overwritten. Maintaining existing relationships The mongoId creation happens deterministically, with fixed identifiers, generated on the basis of the table name and the record number ONLY. The records are generated in a deterministic order. If the import script has not changed, the results will be identical. If identical records are imported, the results will be identical. If identical records are imported repeatedly, there will be no change after the first time. If the script changes, but the number and order of records that are generated remains the same the generated ids are still the same. Relationships may still break This does not guarantee that no relationships will break. But the only case where things might go wrong are the non-pristine records. If they refer to a value table, and the value table has been reorganized, data may become corrupt. If this happens, ad-hoc remedies are needed. The script will output a clear overview with the number of non-pristine records per table. The user table All production users in the system are not pristine. So they will be untouched. No initial data refers to production users. So the legacy users are disjoint from the production users. The same holds for the test users: they live only on the test system. Nothing in the production system has any link to a test user. The import script creates some group assignments for production users. These links between group and user happen per eppn , not per id. If the receiving database has different assignments in place, they will be non-pristine, and hence will not be overwritten. The import script has stabilized over time, in the sense that it does not change the existing organization of tables, but only adds new data.","title":"Content"},{"location":"Legacy/Content/#initial-content","text":"There are already 800 contributions in the system. They have been collected in a FileMaker database in the past. Import legacy content We convert this content and use it for an initial filling of the contribution tool. The legacy import is automated and repeatable, even into a database that has been used in production for a while.","title":"Initial Content"},{"location":"Legacy/Content/#legacy-contributions","text":"The legacy content for this application consists of a FileMaker database. What is in the database? In it there is a web of tables and value lists. The essential content is a contribution table containing 800 contributions. There is also a bit of assessment information. How do we get that data out? From FileMaker to XML We have exported tables and value lists as XML. This is a manual and clumsy process. XML to Mongo DB The machinery for this step is programmed in a Python script, and the configuration details are spelled out in a config . It reads the XML, extracts the field definitions from it, and reads the data from the fields in the rows. We then do the following: adapt the table and field organization; adjust the field types and the values, especially for datetime and currency; generate value tables and cross-tables; add extra information for countries, so that they can be visualized on a map; link values to existing tables; import a moderately denormalized version of the data into MongoDB. Importing and reimporting The source data model is complex, the target data model is complex, and the app as a whole must support a complex workflow. It is impossible to design everything up-front, so we need to be able to retrace our steps and redo the import. As long as the system is not in production, we can just regenerate the database whenever needed, thereby loosing all manual modifications. But there comes a time, and it has arrived now, that people want to experiment with the data. But the app is not finished yet, and maybe there are more design jumps to make. So we need an import script that can reimport the initial data without disturbing the new data. We have written mongoFromFm.py that does exactly this. From transfer to import We started out running the import script in the development situation, populating a MongoDB instance there, dumping its data, and bulk-importing that into the production instance. The problem with that is that the production system will have a different set of users than the development system. Now contributions get tied to users, so if we move over contributions without users, their creator fields will dangle. It turns out to be much better to use the import script also in the production situation. So we ship the FileMaker input for the script to the production server, and run the import there, with slightly different settings. An additional advantage is, that we replace a coarse bulk import by a much more intelligent and sensitive approach: we add the records programmatically, and we have a chance to look before we act. Requirements The task for the import script boils down to these requirements: records that have been manually modified in the target system MAY NOT be overwritten; existing relationships between records MUST be preserved. See later, under Discussion how this is achieved. Usage Production 1 python3 mongoFromFm.py production Development 1 python3 mongoFromFm.py development Extras in development mode In development mode, the following things happen: excel spreadsheets with the original FileMaker data and the resulting MongoDB data are generated; a bunch of test users is added; the ownership of some contributions is changed to the developer, for ease of testing.","title":"Legacy contributions"},{"location":"Legacy/Content/#discussion","text":"The main idea is that all records that come out of the conversion progress, are marked as pristine . Later, when a record is changed under the influence of the tool, this mark is removed. Preventing data loss All records generated by this program will have a field isPristine , set to true . The DARIAH contribution tool will remove this field from a record after it has modified it. This import tool does not delete databases, nor collections, only individual documents. Before import, an inspection is made: the ids of the existing records are retrieved. The ids will be classified: pristine , non-pristine , troublesome . Troublesome means: not pristine, and occurring in the records to be imported. The following will happen: Existing pristine records will be deleted. The import records will be filtered: the ones with troublesome ids will be left out. The filtered import records will be inserted. This guarantees that there is no data loss: no records that have been touched by the application are deleted nor overwritten. Maintaining existing relationships The mongoId creation happens deterministically, with fixed identifiers, generated on the basis of the table name and the record number ONLY. The records are generated in a deterministic order. If the import script has not changed, the results will be identical. If identical records are imported, the results will be identical. If identical records are imported repeatedly, there will be no change after the first time. If the script changes, but the number and order of records that are generated remains the same the generated ids are still the same. Relationships may still break This does not guarantee that no relationships will break. But the only case where things might go wrong are the non-pristine records. If they refer to a value table, and the value table has been reorganized, data may become corrupt. If this happens, ad-hoc remedies are needed. The script will output a clear overview with the number of non-pristine records per table. The user table All production users in the system are not pristine. So they will be untouched. No initial data refers to production users. So the legacy users are disjoint from the production users. The same holds for the test users: they live only on the test system. Nothing in the production system has any link to a test user. The import script creates some group assignments for production users. These links between group and user happen per eppn , not per id. If the receiving database has different assignments in place, they will be non-pristine, and hence will not be overwritten. The import script has stabilized over time, in the sense that it does not change the existing organization of tables, but only adds new data.","title":"Discussion"},{"location":"Maintenance/Deploy/","text":"Deployment \u00b6 Basic information \u00b6 what where source code GitHub repository https://github.com/Dans-labs/dariah-contrib tech doc GitHub Pages https://dans-labs.github.io/dariah-contrib/ tech doc source https://github.com/Dans-labs/dariah-contrib/blob/master/docs app live https://dariah-beta.dans.knaw.nl Python \u00b6 This app needs Python , version at least 3.6.3. ??? details \"development\" Install it from https://www.python.org/downloads . 1 2 3 4 5 6 7 8 9 The list of Python dependencies to be `pip`-installed is in [requirements.txt](https://github.com/Dans-labs/dariah-contrib/blob/master/requirements.txt) . Install them like so: ```sh pip3 install pymongo flask ``` ??? details \"production\" Python can be installed by means of the package manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ```sh yum install rh-python36 rh-python36-python-pymongo rh-python36-mod_wsgi scl enable rh-python36 bash cp /opt/rh/httpd24/root/usr/lib64/httpd/modules/mod_rh-python36-wsgi.so modules cd /etc/httpd cp /opt/rh/httpd24/root/etc/httpd/conf.modules.d/10-rh-python36-wsgi.conf conf.modules.d/ pip install flask ``` More info about running Python3 in the web server [mod_wsgi guide](https://modwsgi.readthedocs.io/en/develop/user-guides/quick-installation-guide.html) . The website runs with SELinux enforced, and also the updating process works in that mode. Mongo DB \u00b6 This app works with database Mongo DB version 4.0.3 or higher. ??? abstract \"On the mac\" ??? details \"Installing\" sh brew install MongoDB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 ??? details \"Upgrading\" ```sh brew update brew upgrade MongoDB brew link --overwrite MongoDB brew services stop MongoDB brew services start MongoDB ``` ??? details \"Using\" ??? details \"Daemon\" ```sh mongod -f /usr/local/etc/mongod.conf ``` Stop it with ++ctrl+\"c\"++ ??? details \"Console\" ```sh mongo ``` If the DARIAH data has been loaded, say on the mongo prompt: ```sh use dariah ``` and continue with query statements inside the daria collection. ??? details \"In programs\" Via pymongo (no connection information needed). ```sh pip3 install pymongo ``` ```python from pymongo import MongoClient clientm = MongoClient() MONGO = clientm.dariah contributions = list(MONGO['contrib'].find() ``` Web framework \u00b6 For the server application code we use Flask , a Python3 micro framework to route URLs to functions that perform requests and return responses. It contains a development web server. ??? details \"What the server code does\" The code for the server is at its heart a mapping between routes (URL patterns) and functions (request => response transformers). The app source code for the server resides in serve.py and other .py files in controllers imported by it. 1 2 3 4 The module [index.py](/server/server/index.py) defines routes and associates functions to be executed for those routes. These functions take a request, and turn it into a response. ??? details \"Sessions and a secret key\" The server needs a secret key, we store it in a fixed place. Here is the command to generate and store the key. 1 2 3 4 5 6 7 8 9 ```sh tab=\"server\" cd /opt/web-apps date +%s | sha256sum | base64 | head -c 32 > dariah_jwt.secret ``` ``` sh tab=\"mac\" cd /opt/web-apps date +%s | shasum -a 256 | base64 | head -c 32 > dariah_jwt.secret ``` Web server \u00b6 ??? explanation \"production\" The production web server is httpd (Apache) . Flask connects to it through mod_wsgi (take care to use a version that speaks Python3). This connection is defined in the default config file. See default_example.conf . 1 2 3 4 5 6 * `/etc/httpd/config.d/` * `default.conf` (config for this site) * `shib.conf` (config for shibboleth authentication) * ... The app starts/stops when Apache starts/stops. ??? explanation \"development\" In development, flask runs its own little web server. You can run the development server by saying, in the top level directory of the repo clone: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ```sh ./build.sh serve ``` which starts a small web server that listens to localhost on port 8001. In this case a production build of the app is served locally. For real development, it is better to work with a development server for the client as well that can hot-load modified css and javascript. Whenever you save a modified python source file, the server reloads itself. In order to follow that scenario, you should start the server as ```sh ./build.sh servehot ``` ??? caution This does not yet start the client site development server! User authentication \u00b6 We make use of the DARIAH infrastructure for user authentication AAI (see in particular Integrating Shibboleth Authentication into your Application . Documentation \u00b6 The app itself gives access to documentation: what where live https://dans-labs.github.io/dariah-contrib/ source https://github.com/Dans-labs/dariah-contrib/blob/master/docs not only for end users, but also for developers and app-designers. The docs are generated as static GitHub pages by mkdocs with a DANS theme which has been customized from mkdocs-material . To get the DANS theme, follow the instructions in mkdocs-dans . File structure \u00b6 By GitHub clone we mean a clone of Dans-labs/dariah-contrib . The absolute location is not important. ??? abstract \"Production server\" For the production server we assume everything resides in /opt , on the development machine the location does not matter. 1 2 3 4 5 6 7 8 9 10 11 On production we need in that location: * `shibboleth` Config for the DARIAH identity provider. * `webapps` * `dariah-contrib` Root of the GitHub clone. * `dariah_jwt.secret` Secret used for encrypting sessions, can be generated with [gen_jwt_secret.sh](https://github.com/Dans-labs/dariah-contrib/blob/master/server/gen_jwt_secret.sh) ??? abstract \"Development machine\" On the development machine we need just the GitHub clone and 1 2 3 * `dariah-contrib` Root of the GitHub clone. * `/opt/web-apps/dariah_jwt.secret`","title":"Deploy"},{"location":"Maintenance/Deploy/#deployment","text":"","title":"Deployment"},{"location":"Maintenance/Deploy/#basic-information","text":"what where source code GitHub repository https://github.com/Dans-labs/dariah-contrib tech doc GitHub Pages https://dans-labs.github.io/dariah-contrib/ tech doc source https://github.com/Dans-labs/dariah-contrib/blob/master/docs app live https://dariah-beta.dans.knaw.nl","title":"Basic information"},{"location":"Maintenance/Deploy/#python","text":"This app needs Python , version at least 3.6.3. ??? details \"development\" Install it from https://www.python.org/downloads . 1 2 3 4 5 6 7 8 9 The list of Python dependencies to be `pip`-installed is in [requirements.txt](https://github.com/Dans-labs/dariah-contrib/blob/master/requirements.txt) . Install them like so: ```sh pip3 install pymongo flask ``` ??? details \"production\" Python can be installed by means of the package manager. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ```sh yum install rh-python36 rh-python36-python-pymongo rh-python36-mod_wsgi scl enable rh-python36 bash cp /opt/rh/httpd24/root/usr/lib64/httpd/modules/mod_rh-python36-wsgi.so modules cd /etc/httpd cp /opt/rh/httpd24/root/etc/httpd/conf.modules.d/10-rh-python36-wsgi.conf conf.modules.d/ pip install flask ``` More info about running Python3 in the web server [mod_wsgi guide](https://modwsgi.readthedocs.io/en/develop/user-guides/quick-installation-guide.html) . The website runs with SELinux enforced, and also the updating process works in that mode.","title":"Python"},{"location":"Maintenance/Deploy/#mongo-db","text":"This app works with database Mongo DB version 4.0.3 or higher. ??? abstract \"On the mac\" ??? details \"Installing\" sh brew install MongoDB 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 ??? details \"Upgrading\" ```sh brew update brew upgrade MongoDB brew link --overwrite MongoDB brew services stop MongoDB brew services start MongoDB ``` ??? details \"Using\" ??? details \"Daemon\" ```sh mongod -f /usr/local/etc/mongod.conf ``` Stop it with ++ctrl+\"c\"++ ??? details \"Console\" ```sh mongo ``` If the DARIAH data has been loaded, say on the mongo prompt: ```sh use dariah ``` and continue with query statements inside the daria collection. ??? details \"In programs\" Via pymongo (no connection information needed). ```sh pip3 install pymongo ``` ```python from pymongo import MongoClient clientm = MongoClient() MONGO = clientm.dariah contributions = list(MONGO['contrib'].find() ```","title":"Mongo DB"},{"location":"Maintenance/Deploy/#web-framework","text":"For the server application code we use Flask , a Python3 micro framework to route URLs to functions that perform requests and return responses. It contains a development web server. ??? details \"What the server code does\" The code for the server is at its heart a mapping between routes (URL patterns) and functions (request => response transformers). The app source code for the server resides in serve.py and other .py files in controllers imported by it. 1 2 3 4 The module [index.py](/server/server/index.py) defines routes and associates functions to be executed for those routes. These functions take a request, and turn it into a response. ??? details \"Sessions and a secret key\" The server needs a secret key, we store it in a fixed place. Here is the command to generate and store the key. 1 2 3 4 5 6 7 8 9 ```sh tab=\"server\" cd /opt/web-apps date +%s | sha256sum | base64 | head -c 32 > dariah_jwt.secret ``` ``` sh tab=\"mac\" cd /opt/web-apps date +%s | shasum -a 256 | base64 | head -c 32 > dariah_jwt.secret ```","title":"Web framework"},{"location":"Maintenance/Deploy/#web-server","text":"??? explanation \"production\" The production web server is httpd (Apache) . Flask connects to it through mod_wsgi (take care to use a version that speaks Python3). This connection is defined in the default config file. See default_example.conf . 1 2 3 4 5 6 * `/etc/httpd/config.d/` * `default.conf` (config for this site) * `shib.conf` (config for shibboleth authentication) * ... The app starts/stops when Apache starts/stops. ??? explanation \"development\" In development, flask runs its own little web server. You can run the development server by saying, in the top level directory of the repo clone: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ```sh ./build.sh serve ``` which starts a small web server that listens to localhost on port 8001. In this case a production build of the app is served locally. For real development, it is better to work with a development server for the client as well that can hot-load modified css and javascript. Whenever you save a modified python source file, the server reloads itself. In order to follow that scenario, you should start the server as ```sh ./build.sh servehot ``` ??? caution This does not yet start the client site development server!","title":"Web server"},{"location":"Maintenance/Deploy/#user-authentication","text":"We make use of the DARIAH infrastructure for user authentication AAI (see in particular Integrating Shibboleth Authentication into your Application .","title":"User authentication"},{"location":"Maintenance/Deploy/#documentation","text":"The app itself gives access to documentation: what where live https://dans-labs.github.io/dariah-contrib/ source https://github.com/Dans-labs/dariah-contrib/blob/master/docs not only for end users, but also for developers and app-designers. The docs are generated as static GitHub pages by mkdocs with a DANS theme which has been customized from mkdocs-material . To get the DANS theme, follow the instructions in mkdocs-dans .","title":"Documentation"},{"location":"Maintenance/Deploy/#file-structure","text":"By GitHub clone we mean a clone of Dans-labs/dariah-contrib . The absolute location is not important. ??? abstract \"Production server\" For the production server we assume everything resides in /opt , on the development machine the location does not matter. 1 2 3 4 5 6 7 8 9 10 11 On production we need in that location: * `shibboleth` Config for the DARIAH identity provider. * `webapps` * `dariah-contrib` Root of the GitHub clone. * `dariah_jwt.secret` Secret used for encrypting sessions, can be generated with [gen_jwt_secret.sh](https://github.com/Dans-labs/dariah-contrib/blob/master/server/gen_jwt_secret.sh) ??? abstract \"Development machine\" On the development machine we need just the GitHub clone and 1 2 3 * `dariah-contrib` Root of the GitHub clone. * `/opt/web-apps/dariah_jwt.secret`","title":"File structure"},{"location":"Server/Authentication/","text":"Authentication \u00b6 Authentication on the production system is deferred to the DARIAH Identity Provider. The server performs shibboleth authentication, with credentials coming from the DARIAH Identity provider . /login /logout /slogout The login/logout actions take place at the server after visiting /login , /logout or /slogout . Logout in two stages Currently, /logout performs a logout from this app, but not from the DARIAH Identity Provider. To do the latter, one has to go to /slogout and close the browser. User records \u00b6 When users log in, the details from DARIAH identity provider will be stored in the user table. Auto update User details are stored upon every login. When the DARIAH identity provider has changed attributes of a user, these new attributes will be picked up and stored, replacing older values. So the user table updates itself automatically. These updates reach our user table only for those users that actually log in, at the moment that they do log in. Future users The system may contain records for users that have never logged in. This happens when future users of the system are assigned to field values by their email address. Whenever such a user logs in, the attributes obtained during the authentication will flow into the incomplete user record if it exists, otherwise a new user record will be made. The new user will find him/herself in all places where his/her email address had been entered. Attributes \u00b6 The systems maintains user-associated attributes from two sources: Attributes from the DARIAH identity provider user field in this app attribute provided by DARIAH comments eppn eppn a string by which the user is identified in the DARIAH context email mail the email address according to the DARIAH identity provider firstName givenName lastName sn name cn common name, probably just firstName lastName org o organization to which the user is affiliated membership isMemberOf a semicolon separated string of groups within the DARIAH organization to which the user belongs, e.g. lr_DARIAH-User , humanities-at-scale-contributors , dariah-eu-contributors rel affiliation the type of relation the user has with DARIAH, such as member@dariah.eu Ignored attributes We do not use unscoped-affiliation , which is the affiliation without the @dariah.eu part. Attributes generated by this app user field comments mayLogin whether the user is allowed to login. Default true , but the back office can use this field to prevent a user from logging in. When a user leaves, we advise to set mayLogin to true . It is not an option to delete a user, because (s)he can be the creator/modifier of records that are still in the system. authority the basis on which the identity of the user has been established. See the values below. group the permission level of this user. See the values below. dateLastLogin when the user has logged in most recently statusLastLogin whether the last login attempt was successful And, like almost all records in the system, some standard fields are added. You will not find these fields on the interface in most cases, but it is good to know that they will be recorded in the database. field comments creator the user that created this user record. The legacy user have HaSProject as creator, which is itself a user that cannot login. Other user records do not have a creator. So authenticated users cannot change their user records. dateCreated when the record was created modified a list of modification events, having the date of modification and the user who did it for each event. Display of user attributes When a user is presented on the interface, we choose between the following representations, in order of highest preference first. name , coming from the DARIAH attribute cn (common name) firstName lastName email eppn-autority We append (org) if available. Groups \u00b6 When giving users permissions, the groups they are in play an important role. The attributes authority and group contain the necessary information. authority values authority comments absent the user has never been authenticated. Used for people that occur in the system, but have not yet logged in. DARIAH the user has been logged in by the DARIAH identity provider legacy the user has been imported from the FileMaker legacy data. This kind of user cannot log in. local the user has logged in on the development system. This kind of user should not be present in the production system! group values See the permission model . statusLastLogin values statusLastLogin comments Approved successful login attempt Rejected unsuccessful login attempt","title":"Authentication"},{"location":"Server/Authentication/#authentication","text":"Authentication on the production system is deferred to the DARIAH Identity Provider. The server performs shibboleth authentication, with credentials coming from the DARIAH Identity provider . /login /logout /slogout The login/logout actions take place at the server after visiting /login , /logout or /slogout . Logout in two stages Currently, /logout performs a logout from this app, but not from the DARIAH Identity Provider. To do the latter, one has to go to /slogout and close the browser.","title":"Authentication"},{"location":"Server/Authentication/#user-records","text":"When users log in, the details from DARIAH identity provider will be stored in the user table. Auto update User details are stored upon every login. When the DARIAH identity provider has changed attributes of a user, these new attributes will be picked up and stored, replacing older values. So the user table updates itself automatically. These updates reach our user table only for those users that actually log in, at the moment that they do log in. Future users The system may contain records for users that have never logged in. This happens when future users of the system are assigned to field values by their email address. Whenever such a user logs in, the attributes obtained during the authentication will flow into the incomplete user record if it exists, otherwise a new user record will be made. The new user will find him/herself in all places where his/her email address had been entered.","title":"User records"},{"location":"Server/Authentication/#attributes","text":"The systems maintains user-associated attributes from two sources: Attributes from the DARIAH identity provider user field in this app attribute provided by DARIAH comments eppn eppn a string by which the user is identified in the DARIAH context email mail the email address according to the DARIAH identity provider firstName givenName lastName sn name cn common name, probably just firstName lastName org o organization to which the user is affiliated membership isMemberOf a semicolon separated string of groups within the DARIAH organization to which the user belongs, e.g. lr_DARIAH-User , humanities-at-scale-contributors , dariah-eu-contributors rel affiliation the type of relation the user has with DARIAH, such as member@dariah.eu Ignored attributes We do not use unscoped-affiliation , which is the affiliation without the @dariah.eu part. Attributes generated by this app user field comments mayLogin whether the user is allowed to login. Default true , but the back office can use this field to prevent a user from logging in. When a user leaves, we advise to set mayLogin to true . It is not an option to delete a user, because (s)he can be the creator/modifier of records that are still in the system. authority the basis on which the identity of the user has been established. See the values below. group the permission level of this user. See the values below. dateLastLogin when the user has logged in most recently statusLastLogin whether the last login attempt was successful And, like almost all records in the system, some standard fields are added. You will not find these fields on the interface in most cases, but it is good to know that they will be recorded in the database. field comments creator the user that created this user record. The legacy user have HaSProject as creator, which is itself a user that cannot login. Other user records do not have a creator. So authenticated users cannot change their user records. dateCreated when the record was created modified a list of modification events, having the date of modification and the user who did it for each event. Display of user attributes When a user is presented on the interface, we choose between the following representations, in order of highest preference first. name , coming from the DARIAH attribute cn (common name) firstName lastName email eppn-autority We append (org) if available.","title":"Attributes"},{"location":"Server/Authentication/#groups","text":"When giving users permissions, the groups they are in play an important role. The attributes authority and group contain the necessary information. authority values authority comments absent the user has never been authenticated. Used for people that occur in the system, but have not yet logged in. DARIAH the user has been logged in by the DARIAH identity provider legacy the user has been imported from the FileMaker legacy data. This kind of user cannot log in. local the user has logged in on the development system. This kind of user should not be present in the production system! group values See the permission model . statusLastLogin values statusLastLogin comments Approved successful login attempt Rejected unsuccessful login attempt","title":"Groups"},{"location":"Server/Server/","text":"Server \u00b6 Although this app is a single page application with most of the business logic coded at the client side, there are a bunch of things that are handled at the server side. Data access Almost all data access is handled by server side controllers that implement a data api. These controllers are informed by the data model . When the web server starts, the data model files are read, and converted to python modules with the same base name that encapsulate the information in the YAML files. These modules are then imported by all controllers, so that almost all data access happens in conformance with the data model and its permissions. Other way of data access The module info bypasses the regular data access methods, and peeks straight into the MongoDB data. perm \u00b6 See perm . Contains the methods to compute permissions for controllers, tables and fields. Here are the main methods. db \u00b6 See db . This is the data access module. It uses the data model to serve any data to any user in such a way that no data is sent from server to client that the current user is not entitled to see. auth \u00b6 See auth . Contains the methods to authenticate users. Here all the logic about user sessions and session cookies is written down. It builds on the Flask web framework. authenticate() 1 authenticate ( login = False ) task Tries to authenticate the current user by looking up a session created by the DARIAH identity provider, and retrieving the attributes of that session. If it finds unsatisfactory attributes in the session, the session will be deleted, and the user is not authenticated. login This is only relevant on the development system. If True , the server asks for a login name on the command line, and if a valid test user is typed in, it logs that user in. On the production system, the login process takes place outside this app. Only after login this app is able to detect whether a user has logged in and if so, which user that is. deauthenticate() 1 deauthenticate () task Clears the info of the current user and if that user has been identified by the DARIAH identity provider, the corresponding session will be deleted. workflow \u00b6 See workflow . Implements the workflow engine which takes care of various aspects of the business logic, just above the level of data fetching and permissions. overview \u00b6 See overview . This module is reponsible for an overview of the statistics of the contributions. National coordinators and backoffice personnel can (de)select contributions from this page. utils \u00b6 See utils . Low level stuff.","title":"Server"},{"location":"Server/Server/#server","text":"Although this app is a single page application with most of the business logic coded at the client side, there are a bunch of things that are handled at the server side. Data access Almost all data access is handled by server side controllers that implement a data api. These controllers are informed by the data model . When the web server starts, the data model files are read, and converted to python modules with the same base name that encapsulate the information in the YAML files. These modules are then imported by all controllers, so that almost all data access happens in conformance with the data model and its permissions. Other way of data access The module info bypasses the regular data access methods, and peeks straight into the MongoDB data.","title":"Server"},{"location":"Server/Server/#perm","text":"See perm . Contains the methods to compute permissions for controllers, tables and fields. Here are the main methods.","title":"perm"},{"location":"Server/Server/#db","text":"See db . This is the data access module. It uses the data model to serve any data to any user in such a way that no data is sent from server to client that the current user is not entitled to see.","title":"db"},{"location":"Server/Server/#auth","text":"See auth . Contains the methods to authenticate users. Here all the logic about user sessions and session cookies is written down. It builds on the Flask web framework. authenticate() 1 authenticate ( login = False ) task Tries to authenticate the current user by looking up a session created by the DARIAH identity provider, and retrieving the attributes of that session. If it finds unsatisfactory attributes in the session, the session will be deleted, and the user is not authenticated. login This is only relevant on the development system. If True , the server asks for a login name on the command line, and if a valid test user is typed in, it logs that user in. On the production system, the login process takes place outside this app. Only after login this app is able to detect whether a user has logged in and if so, which user that is. deauthenticate() 1 deauthenticate () task Clears the info of the current user and if that user has been identified by the DARIAH identity provider, the corresponding session will be deleted.","title":"auth"},{"location":"Server/Server/#workflow","text":"See workflow . Implements the workflow engine which takes care of various aspects of the business logic, just above the level of data fetching and permissions.","title":"workflow"},{"location":"Server/Server/#overview","text":"See overview . This module is reponsible for an overview of the statistics of the contributions. National coordinators and backoffice personnel can (de)select contributions from this page.","title":"overview"},{"location":"Server/Server/#utils","text":"See utils . Low level stuff.","title":"utils"},{"location":"Technology/ES6/","text":"ES6 \u00b6 We use ECMAScript 6, also known as ES6, also known as ES2015, also known as JavaScript for the client side of the app. The evolution of JavaScript to ES6 and beyond has transformed JavaScript from a \"horrible language\" into a performant language with a beautiful syntax on one of the most widely supported platforms: the browser. Instead of pushing JavaScript out of sight, we fully embrace it as our principal programming formalism at the client side. Code style Our source code conforms to a number of style guides, which are checked by eslint . There are many options and choices, ours are here . Not all of these are relevant, because we also enforce style by means of prettier . We highlight a few, not all, concepts in ES6 that we make use of. Arrow functions \u00b6 arrow notation There is now a very handy notation to write functions: arrow notation. Instead of 1 2 3 function myFunc ( x , y ) { return x + y } you can write: 1 const myFunc = ( x , y ) => x + y even shorter If there is only 1 argument, it is even shorter: 1 const myFunc = x => x + 1 functions returning functions If you have functions that return functions, everything goes smoother now: 1 const handleEvent = id => event => dispatch ({ id , value : event . target . value })","title":"ES6"},{"location":"Technology/ES6/#es6","text":"We use ECMAScript 6, also known as ES6, also known as ES2015, also known as JavaScript for the client side of the app. The evolution of JavaScript to ES6 and beyond has transformed JavaScript from a \"horrible language\" into a performant language with a beautiful syntax on one of the most widely supported platforms: the browser. Instead of pushing JavaScript out of sight, we fully embrace it as our principal programming formalism at the client side. Code style Our source code conforms to a number of style guides, which are checked by eslint . There are many options and choices, ours are here . Not all of these are relevant, because we also enforce style by means of prettier . We highlight a few, not all, concepts in ES6 that we make use of.","title":"ES6"},{"location":"Technology/ES6/#arrow-functions","text":"arrow notation There is now a very handy notation to write functions: arrow notation. Instead of 1 2 3 function myFunc ( x , y ) { return x + y } you can write: 1 const myFunc = ( x , y ) => x + y even shorter If there is only 1 argument, it is even shorter: 1 const myFunc = x => x + 1 functions returning functions If you have functions that return functions, everything goes smoother now: 1 const handleEvent = id => event => dispatch ({ id , value : event . target . value })","title":"Arrow functions"},{"location":"Technology/Tech/","text":"Technical references \u00b6 This is an alphabetical list of tech references. Sometimes we refer to a technology without making use of it in the app, we have marked those entries with an \u2717. References \u00b6 Generic bash shell scripting cloc counting lines of code iso8601 date and time format Web design webApi interacting with the loaded document in a browser Styling css cascading stylesheets: flexbox laying out boxes in flexible ways hsl color space html markup language for the web Shorthands markdown rich text from plain text yaml configuration language, as simple as markdown. Editing \u2717 IDE Integrated Developer's Environment \u2717 Atom IDE by GitHub \u2717 SublimeText commercial text editor vim old-hands text editor, still competes with IDEs ALE runs linters and formatters within vim \u2717 Visual Studio Code IDE by Microsoft \u2717 Webstorm commercial IDE ???+ abstract \"Linters and formatters * flake8 code linter for Python * black code formatter for Python Client side language javascript scripting language for the web eslint checks ES6 code against style requirements prettier code formatter for javascript Server side python data-oriented scripting language flask micro web framework wsgi bridge between the python language and webservers \u2717 socket push messages from server to all connected clients \u2717 python-socket python wrapper for socket.io Database mongodb NO-SQL database, JSON/Javascript based Other \u2717 shebanq web interface for Hebrew text and linguistic annotations","title":"Tech"},{"location":"Technology/Tech/#technical-references","text":"This is an alphabetical list of tech references. Sometimes we refer to a technology without making use of it in the app, we have marked those entries with an \u2717.","title":"Technical references"},{"location":"Technology/Tech/#references","text":"Generic bash shell scripting cloc counting lines of code iso8601 date and time format Web design webApi interacting with the loaded document in a browser Styling css cascading stylesheets: flexbox laying out boxes in flexible ways hsl color space html markup language for the web Shorthands markdown rich text from plain text yaml configuration language, as simple as markdown. Editing \u2717 IDE Integrated Developer's Environment \u2717 Atom IDE by GitHub \u2717 SublimeText commercial text editor vim old-hands text editor, still competes with IDEs ALE runs linters and formatters within vim \u2717 Visual Studio Code IDE by Microsoft \u2717 Webstorm commercial IDE ???+ abstract \"Linters and formatters * flake8 code linter for Python * black code formatter for Python Client side language javascript scripting language for the web eslint checks ES6 code against style requirements prettier code formatter for javascript Server side python data-oriented scripting language flask micro web framework wsgi bridge between the python language and webservers \u2717 socket push messages from server to all connected clients \u2717 python-socket python wrapper for socket.io Database mongodb NO-SQL database, JSON/Javascript based Other \u2717 shebanq web interface for Hebrew text and linguistic annotations","title":"References"}]}