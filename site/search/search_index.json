{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \u00b6 DARIAH contribution tool This is the documentation for the DARIAH contribution tool, an instrument to register and assess community contributions to the DARIAH . News Every now and then I resume what has happened during development. It is not regular and not comprehensive! More ... What does it do? \u00b6 Business logic The actual handling of contributions, assessments and reviews is the business logic of this app. More ... API The data of the tool is accessible through an API. In fact, this app itself uses that API, whenever the client needs data from the server. More ... Content This app inherits 800 contributions that have been entered in 2015-2017 into a FileMaker database. We have migrated those to a MongoDB model. More ... Techical \u00b6 Model The whole app is centered around data: contributions, assessments, reviews and more. We have to organize and specify that data. More ... Workflow At the highest level of abstraction a workflow engine implements the business logic. More ... Tables Several tables work together with the workflow engine. More ... Web The part of the app that guards the data sits at the server. From there it sends it to the web browsers (clients) of the users. More ... Authentication Users are authenticated at the server, and every bit of data that they subsequently receive, has passed a customs control. More ... Inventory We have listed most of the technology that we have made use of. More ... Lessons It has taken a lot of time to develop this app. Lots more than I expected from the start. More ... Maintenance \u00b6 Deploy Here are the bits and pieces you have to do in order to get a working system out of this. More ... Apidocs The technical documentation of the Python code is largely in so-called docstrings within the code. But you can see them nicely formatted here ... Code base A few remarks on our code and their languages. See also how we managed to keep the code in all those languages tidy. More ... Stats To get an impression of the kind of work behind this app, we reveal how many lines of code have been written in which languages. Here ... Testing Does the app work properly? Can we develop new functions without breaking one of the innumerable logical threads in it? We have a test suite that covers most of the code and checks most of the logic. The server is tested following the approach of the Flask documentation . The tests have been documented by docstrings. Here ... Do the test coverage all the code? Here you can see the source code with the lines that have not been executed during tests highlighted. Here ... The latest test results. Here ... Author \u00b6 Dirk Roorda DANS dirk.roorda@dans.knaw.nl 2019-11-28 (Extensive test suite and documentation) 2019-11-08 (Redesign from a clean slate) 2019-08-06 2019-07-29 2019-03-04 2017-12-14","title":"Home"},{"location":"#home","text":"DARIAH contribution tool This is the documentation for the DARIAH contribution tool, an instrument to register and assess community contributions to the DARIAH . News Every now and then I resume what has happened during development. It is not regular and not comprehensive! More ...","title":"Home"},{"location":"#what-does-it-do","text":"Business logic The actual handling of contributions, assessments and reviews is the business logic of this app. More ... API The data of the tool is accessible through an API. In fact, this app itself uses that API, whenever the client needs data from the server. More ... Content This app inherits 800 contributions that have been entered in 2015-2017 into a FileMaker database. We have migrated those to a MongoDB model. More ...","title":"What does it do?"},{"location":"#techical","text":"Model The whole app is centered around data: contributions, assessments, reviews and more. We have to organize and specify that data. More ... Workflow At the highest level of abstraction a workflow engine implements the business logic. More ... Tables Several tables work together with the workflow engine. More ... Web The part of the app that guards the data sits at the server. From there it sends it to the web browsers (clients) of the users. More ... Authentication Users are authenticated at the server, and every bit of data that they subsequently receive, has passed a customs control. More ... Inventory We have listed most of the technology that we have made use of. More ... Lessons It has taken a lot of time to develop this app. Lots more than I expected from the start. More ...","title":"Techical"},{"location":"#maintenance","text":"Deploy Here are the bits and pieces you have to do in order to get a working system out of this. More ... Apidocs The technical documentation of the Python code is largely in so-called docstrings within the code. But you can see them nicely formatted here ... Code base A few remarks on our code and their languages. See also how we managed to keep the code in all those languages tidy. More ... Stats To get an impression of the kind of work behind this app, we reveal how many lines of code have been written in which languages. Here ... Testing Does the app work properly? Can we develop new functions without breaking one of the innumerable logical threads in it? We have a test suite that covers most of the code and checks most of the logic. The server is tested following the approach of the Flask documentation . The tests have been documented by docstrings. Here ... Do the test coverage all the code? Here you can see the source code with the lines that have not been executed during tests highlighted. Here ... The latest test results. Here ...","title":"Maintenance"},{"location":"#author","text":"Dirk Roorda DANS dirk.roorda@dans.knaw.nl 2019-11-28 (Extensive test suite and documentation) 2019-11-08 (Redesign from a clean slate) 2019-08-06 2019-07-29 2019-03-04 2017-12-14","title":"Author"},{"location":"News/","text":"News \u00b6 2019-11-15 \u00b6 A complete redesign is nearing completion. We started a brand new repo, but the old work and its history is still at the old location .","title":"News"},{"location":"News/#news","text":"","title":"News"},{"location":"News/#2019-11-15","text":"A complete redesign is nearing completion. We started a brand new repo, but the old work and its history is still at the old location .","title":"2019-11-15"},{"location":"Tech/Authentication/","text":"Authentication \u00b6 Authentication on the production system is deferred to the DARIAH Identity Provider. The server performs shibboleth authentication, with credentials coming from the DARIAH Identity provider . /login /logout /slogout The login/logout actions take place at the server after visiting /login , /logout or /slogout . Logout in two stages Currently, /logout performs a logout from this app, but not from the DARIAH Identity Provider. To do the latter, one has to go to /slogout and close the browser. User records \u00b6 When users log in, the details from DARIAH identity provider will be stored in the user table. Auto update User details are stored upon every login. When the DARIAH identity provider has changed attributes of a user, these new attributes will be picked up and stored, replacing older values. So the user table updates itself automatically. These updates reach our user table only for those users that actually log in, at the moment that they do log in. Future users The system may contain records for users that have never logged in. This happens when future users of the system are assigned to field values by their email address. Whenever such a user logs in, the attributes obtained during the authentication will flow into the incomplete user record if it exists, otherwise a new user record will be made. The new user will find him/herself in all places where his/her email address had been entered. Attributes \u00b6 The systems maintains user-associated attributes from two sources: Attributes from the DARIAH identity provider user field in this app attribute provided by DARIAH comments eppn eppn a string by which the user is identified in the DARIAH context email mail the email address according to the DARIAH identity provider firstName givenName lastName sn name cn common name, probably just firstName lastName org o organization to which the user is affiliated membership isMemberOf a semicolon separated string of groups within the DARIAH organization to which the user belongs, e.g. lr_DARIAH-User , humanities-at-scale-contributors , dariah-eu-contributors rel affiliation the type of relation the user has with DARIAH, such as member@dariah.eu Ignored attributes We do not use unscoped-affiliation , which is the affiliation without the @dariah.eu part. Attributes generated by this app user field comments mayLogin whether the user is allowed to login. Default true , but the back office can use this field to prevent a user from logging in. When a user leaves, we advise to set mayLogin to true . It is not an option to delete a user, because (s)he can be the creator/modifier of records that are still in the system. authority the basis on which the identity of the user has been established. See the values below. group the permission level of this user. See the values below. dateLastLogin when the user has logged in most recently statusLastLogin whether the last login attempt was successful Like almost all records in the system, the provenance fields are added as well. Display of user attributes When a user is presented on the interface, we choose between the following representations, in order of highest preference first. name , coming from the DARIAH attribute cn (common name) firstName lastName email eppn-autority We append (org) if available. Groups \u00b6 When giving users permissions, the groups they are in play an important role. The attributes authority and group contain the necessary information. authority values authority comments absent the user has never been authenticated. Used for people that occur in the system, but have not yet logged in. DARIAH the user has been logged in by the DARIAH identity provider legacy the user has been imported from the FileMaker legacy data. This kind of user cannot log in. local the user has logged in on the development system. This kind of user should not be present in the production system! group values See the permission model . statusLastLogin values statusLastLogin comments Approved successful login attempt Rejected unsuccessful login attempt","title":"Authentication"},{"location":"Tech/Authentication/#authentication","text":"Authentication on the production system is deferred to the DARIAH Identity Provider. The server performs shibboleth authentication, with credentials coming from the DARIAH Identity provider . /login /logout /slogout The login/logout actions take place at the server after visiting /login , /logout or /slogout . Logout in two stages Currently, /logout performs a logout from this app, but not from the DARIAH Identity Provider. To do the latter, one has to go to /slogout and close the browser.","title":"Authentication"},{"location":"Tech/Authentication/#user-records","text":"When users log in, the details from DARIAH identity provider will be stored in the user table. Auto update User details are stored upon every login. When the DARIAH identity provider has changed attributes of a user, these new attributes will be picked up and stored, replacing older values. So the user table updates itself automatically. These updates reach our user table only for those users that actually log in, at the moment that they do log in. Future users The system may contain records for users that have never logged in. This happens when future users of the system are assigned to field values by their email address. Whenever such a user logs in, the attributes obtained during the authentication will flow into the incomplete user record if it exists, otherwise a new user record will be made. The new user will find him/herself in all places where his/her email address had been entered.","title":"User records"},{"location":"Tech/Authentication/#attributes","text":"The systems maintains user-associated attributes from two sources: Attributes from the DARIAH identity provider user field in this app attribute provided by DARIAH comments eppn eppn a string by which the user is identified in the DARIAH context email mail the email address according to the DARIAH identity provider firstName givenName lastName sn name cn common name, probably just firstName lastName org o organization to which the user is affiliated membership isMemberOf a semicolon separated string of groups within the DARIAH organization to which the user belongs, e.g. lr_DARIAH-User , humanities-at-scale-contributors , dariah-eu-contributors rel affiliation the type of relation the user has with DARIAH, such as member@dariah.eu Ignored attributes We do not use unscoped-affiliation , which is the affiliation without the @dariah.eu part. Attributes generated by this app user field comments mayLogin whether the user is allowed to login. Default true , but the back office can use this field to prevent a user from logging in. When a user leaves, we advise to set mayLogin to true . It is not an option to delete a user, because (s)he can be the creator/modifier of records that are still in the system. authority the basis on which the identity of the user has been established. See the values below. group the permission level of this user. See the values below. dateLastLogin when the user has logged in most recently statusLastLogin whether the last login attempt was successful Like almost all records in the system, the provenance fields are added as well. Display of user attributes When a user is presented on the interface, we choose between the following representations, in order of highest preference first. name , coming from the DARIAH attribute cn (common name) firstName lastName email eppn-autority We append (org) if available.","title":"Attributes"},{"location":"Tech/Authentication/#groups","text":"When giving users permissions, the groups they are in play an important role. The attributes authority and group contain the necessary information. authority values authority comments absent the user has never been authenticated. Used for people that occur in the system, but have not yet logged in. DARIAH the user has been logged in by the DARIAH identity provider legacy the user has been imported from the FileMaker legacy data. This kind of user cannot log in. local the user has logged in on the development system. This kind of user should not be present in the production system! group values See the permission model . statusLastLogin values statusLastLogin comments Approved successful login attempt Rejected unsuccessful login attempt","title":"Groups"},{"location":"Tech/Codebase/","text":"Codebase \u00b6 Statistics \u00b6 Legend The numbers stand for lines of code. 1000 lines is ~ 20 typed A4 pages of text. The statistics have been gathered by the cloc tool . Statistics Formalisms \u00b6 YAML \u00b6 See YAML . A simple plain-text way to convey structured data. What Markdown is to text, YAML is to XML-JSON. In this app we use YAML for configuration details. Usage in this app table definition files other configuration files . Markdown \u00b6 See Markdown . A simple plain-text way to write formatted text. See it as a shortcut to writing HTML. It is handy for writing documentation without being distracted by too many formatting options and issues. General usage Markdown is usually converted to HTML, but even when it is not converted, it is still very readable. If you use GitHub, one of the first things is to write a README file for your project. This must be a markdown file. If you use other documentation options on GitHub, such as Wiki or Pages, you will also write markdown. Markdown has a sister: YAML , which is used for structured data. Usage in this app all documentation here is written in Markdown the docstrings inside the Python code use Markdown all big editable text fields in this app support Markdown. JavaScript \u00b6 See JavaScript . The principal scripting language for web applications. It has evolved into a performant language with a beautiful syntax, capable of running on the server and in websites. Usage in this app This app uses JavaScript in the client only. A previous incarnation of this app has been written as a Single Page App which lead to an explosion of Javascript and techniques within Javascript, such as React and Redux. That became too complex for our purposes, and we are back to simple, but modern Javascript and JQuery . Python \u00b6 See Python . A general purpose scripting language with excellent data processing facilities. Usage in this app This app uses python (version 3.6.1+) for the web server. The web server itself is Flask , a light-weight framework for handling http(s) requests. We have added a set of controllers . CSS \u00b6 See CSS . Styling the app has nightmarish overtones, because the concerns of style often cut right across the concerns of the components. There are several ways to control the resulting mess, and one of the best is to use the modern features of CSS. General usage Cascading style sheets are the ultimate way to paint the final look and feel of the website. By using flex boxes instead of tables we can make the app respond gracefully to changes in the size of the display without resorting to the bureaucracy of overdefining style properties. Note that our app does not use the HTML <table> element any more for aligning pieces of content. Usage in this app We use a lot of the CSS-3 features, including variables , and calc() . This lessens our need for a style sheet preprocessor such as SASS to 0%. Note especially how colour management has become easy: all colour definitions are in variables all colour definitions are in HSLA , which allows a very consistent definition of families of colours. Quote from Mozilla : One advantage of HSLA over RGB is that it is more intuitive: you can guess 1 2 3 at the color you want, and tweak it from there. It is also easier to create a set of matching colors (e.g., by keeping the hue the same, while varying the lightness/darkness and saturation). This is exactly what we do. See vars.css . Shell \u00b6 See Shell . The shell is the interpreter of system level commands. Usage in this app Our app does not use it, but we use it to develop the app. All the development tasks, such pushing code to GitHub, transporting databases to the production server are done by intricate commands with many options which are easily forgotten. That's why we have a build script. You have to pass it just the name of a task, and the script executes that task with all the sophistication needed. HTML \u00b6 See HTML . The core language of the web. Usage in this app Surprisingly, our code contains very little HTML. Our server only sends a simple template page to the browser followed by one or more snippets to replace pieces of content by updated material. JSON \u00b6 See JSON . A format to serialize JavaScript objects. General usage In web applications, the program logic happens at two separate places (at least): the server and the client. It is important that data can flow seamlessly from one programming context to the other. JSON achieves that. Usage in this app to send data from client to server Keeping the code tidy \u00b6 There are three progressive levels of caring for your code. Level 1: code style Adopt a style guide and meticulously keep to it. It is hard, especially if you work in two syntactically and culturally diverse languages such as JavaScript and Python. Add CSS, Markdown and YAML to the mix, and you can feel the need for a next step. Yet this is the fundamental step, it cannot be skipped. Level 2: linters Linters are little programs that observe your code and check it for correctness and style, as far as that is possible without actually running the code. Usually, your editing environment runs them sneakily while you type or when you save, and give you unobtrusive but conspicuous feedback. It saves you a lot of round trips of compiling/building/running/staring at errors. Moreover, it gives you the feedback right where you are typing, so you do not have to lookup files and line numbers. Sometimes linters give you so much feedback that your heart sinks at the thought of going through all those cases and fix them all, even if you have a splendid IDE. That is where the next step comes in. Level 3: formatters Formatters have a lot in common with linters, but they fix the problems. Sometimes they parse your program with the parser of the language and then format the abstract syntax three they've got. That is the end of all style inconsistencies. Where to put the documentation \u00b6 Documentation aimed at programmers is best put as close to the code as possible. The Python code is now littered with docstrings which explain every quirk in the code. When writing docstrings I encountered a few stumbling blocks: Docstrings are meant to be read in the code, but they are more readable when properly formatted. Docstrings are more primitive than code: you have to repeat yourself quite a bit. Soon your docstrings litter your code and detract from the terse beauty of it. When you refactor, it is an extra burden to refactor the docstrings as well. For some of these issues are solutions: There are apidoc generators to that produce nice HTML formatted documentation out of them. They also apply inheritance of the code to the corresponding docstrings. You can link from one identifier to another, which lessens the repetitions. In your editor you can use folding to hide all docstrings with one keystroke, and then open them all again with another key stroke. Do not do docstrings early on, wait till the code is more or less mature. And then: good docstrings can help you to spot good refactorings. Tools \u00b6 Here is an overview of tools used in developing this app. Formatters, Linters, API generators Formatters are not perfect, sometimes they produce code at which the linter balks, especially yapf is doing that. Luckily, you can selectively suppress certain kinds of transformations. language linter formatter API-generator JavaScript eslint prettier ?? Python flake8 black pdoc3 Editor or IDE? For projects like these, you need a good editing environment. IDEs The good old ones like Eclipse are not really suited to the JavaScript and Python environments. There are interesting modern ones such as GitHub's Atom modernized ones such as Microsoft's Visual Studio Code and commercial ones such as Webstorm . Editors You can also choose to work with a text editor, such as the free Vim or the commercial Sublime Text . Vim My choice has been Vim, since I use it from its start in 1991. These are the key reasons for which Vim stands out: it has a compositional command set, like Unix itself. By this you get all your (massive) editing chores done without much remembering and thinking. it has a rich ecosystem of plugins. By this you can turn Vim into an IDE. It is rock solid and performant. You can edit many small files and then some big ones, at the same time. You do not loose data. My Vim setup Just for the record, here is a piece of my .vimrc file (the configuration file, which draws in plugins, and customises the interface). You can find out more about the plugins by clicking on them, they are all GitHub repos: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 call plug#begin () \"Python Plug 'vimjas/vim-python-pep8-indent' \" javascript Plug 'jelera/vim-javascript-syntax' Plug 'pangloss/vim-javascript' Plug 'othree/yajs.vim' Plug 'othree/javascript-libraries-syntax.vim' Plug 'mxw/vim-jsx' \" css Plug 'hail2u/vim-css3-syntax' \"utility Plug 'nathanaelkane/vim-indent-guides' Plug 'scrooloose/nerdtree' Plug 'w0rp/ale' \"color Plug 'morhetz/gruvbox' call plug# end () An honourable mention for the ALE plugin. This is an asynchronous plugin that invokes linters for your files while you edit. The beauty is, that if you have installed the linters first outside Vim, ALE is smart enough to detect them and run them for you, asynchronously, and with zero configuration. Then folding . First customize your Python syntax file very little: Make a new file ~/.vim/after/syntax/python.vim and fill it with 1 2 3 syn region pythonString matchgroup = pythonTripleQuotes \\ start =+ [uU]\\ = \\z ( '' '\\ | \"\"\"\\)+ end=\" \\z1\" keepend fold \\ contains = pythonEscape , pythonSpaceError , pythonDoctest , @Spell The only difference with the default is the word fold . Now in your ~/vimrc say 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 set foldmethod=syntax set foldnestmax=2 set foldenable set foldcolumn=1 set foldlevel=2 \" toggle fold under cursor noremap <F6> za inoremap <F6> za \" close all folds noremap <S-F6> zM inoremap <S-F6> zM \" open all folds noremap <C-F6> zR inoremap <C-F6> zR Then you have keystrokes (based on <F6> in my case) to do simple but easy to remember folding.","title":"Codebase"},{"location":"Tech/Codebase/#codebase","text":"","title":"Codebase"},{"location":"Tech/Codebase/#statistics","text":"Legend The numbers stand for lines of code. 1000 lines is ~ 20 typed A4 pages of text. The statistics have been gathered by the cloc tool . Statistics","title":"Statistics"},{"location":"Tech/Codebase/#formalisms","text":"","title":"Formalisms"},{"location":"Tech/Codebase/#yaml","text":"See YAML . A simple plain-text way to convey structured data. What Markdown is to text, YAML is to XML-JSON. In this app we use YAML for configuration details. Usage in this app table definition files other configuration files .","title":"YAML"},{"location":"Tech/Codebase/#markdown","text":"See Markdown . A simple plain-text way to write formatted text. See it as a shortcut to writing HTML. It is handy for writing documentation without being distracted by too many formatting options and issues. General usage Markdown is usually converted to HTML, but even when it is not converted, it is still very readable. If you use GitHub, one of the first things is to write a README file for your project. This must be a markdown file. If you use other documentation options on GitHub, such as Wiki or Pages, you will also write markdown. Markdown has a sister: YAML , which is used for structured data. Usage in this app all documentation here is written in Markdown the docstrings inside the Python code use Markdown all big editable text fields in this app support Markdown.","title":"Markdown"},{"location":"Tech/Codebase/#javascript","text":"See JavaScript . The principal scripting language for web applications. It has evolved into a performant language with a beautiful syntax, capable of running on the server and in websites. Usage in this app This app uses JavaScript in the client only. A previous incarnation of this app has been written as a Single Page App which lead to an explosion of Javascript and techniques within Javascript, such as React and Redux. That became too complex for our purposes, and we are back to simple, but modern Javascript and JQuery .","title":"JavaScript"},{"location":"Tech/Codebase/#python","text":"See Python . A general purpose scripting language with excellent data processing facilities. Usage in this app This app uses python (version 3.6.1+) for the web server. The web server itself is Flask , a light-weight framework for handling http(s) requests. We have added a set of controllers .","title":"Python"},{"location":"Tech/Codebase/#css","text":"See CSS . Styling the app has nightmarish overtones, because the concerns of style often cut right across the concerns of the components. There are several ways to control the resulting mess, and one of the best is to use the modern features of CSS. General usage Cascading style sheets are the ultimate way to paint the final look and feel of the website. By using flex boxes instead of tables we can make the app respond gracefully to changes in the size of the display without resorting to the bureaucracy of overdefining style properties. Note that our app does not use the HTML <table> element any more for aligning pieces of content. Usage in this app We use a lot of the CSS-3 features, including variables , and calc() . This lessens our need for a style sheet preprocessor such as SASS to 0%. Note especially how colour management has become easy: all colour definitions are in variables all colour definitions are in HSLA , which allows a very consistent definition of families of colours. Quote from Mozilla : One advantage of HSLA over RGB is that it is more intuitive: you can guess 1 2 3 at the color you want, and tweak it from there. It is also easier to create a set of matching colors (e.g., by keeping the hue the same, while varying the lightness/darkness and saturation). This is exactly what we do. See vars.css .","title":"CSS"},{"location":"Tech/Codebase/#shell","text":"See Shell . The shell is the interpreter of system level commands. Usage in this app Our app does not use it, but we use it to develop the app. All the development tasks, such pushing code to GitHub, transporting databases to the production server are done by intricate commands with many options which are easily forgotten. That's why we have a build script. You have to pass it just the name of a task, and the script executes that task with all the sophistication needed.","title":"Shell"},{"location":"Tech/Codebase/#html","text":"See HTML . The core language of the web. Usage in this app Surprisingly, our code contains very little HTML. Our server only sends a simple template page to the browser followed by one or more snippets to replace pieces of content by updated material.","title":"HTML"},{"location":"Tech/Codebase/#json","text":"See JSON . A format to serialize JavaScript objects. General usage In web applications, the program logic happens at two separate places (at least): the server and the client. It is important that data can flow seamlessly from one programming context to the other. JSON achieves that. Usage in this app to send data from client to server","title":"JSON"},{"location":"Tech/Codebase/#keeping-the-code-tidy","text":"There are three progressive levels of caring for your code. Level 1: code style Adopt a style guide and meticulously keep to it. It is hard, especially if you work in two syntactically and culturally diverse languages such as JavaScript and Python. Add CSS, Markdown and YAML to the mix, and you can feel the need for a next step. Yet this is the fundamental step, it cannot be skipped. Level 2: linters Linters are little programs that observe your code and check it for correctness and style, as far as that is possible without actually running the code. Usually, your editing environment runs them sneakily while you type or when you save, and give you unobtrusive but conspicuous feedback. It saves you a lot of round trips of compiling/building/running/staring at errors. Moreover, it gives you the feedback right where you are typing, so you do not have to lookup files and line numbers. Sometimes linters give you so much feedback that your heart sinks at the thought of going through all those cases and fix them all, even if you have a splendid IDE. That is where the next step comes in. Level 3: formatters Formatters have a lot in common with linters, but they fix the problems. Sometimes they parse your program with the parser of the language and then format the abstract syntax three they've got. That is the end of all style inconsistencies.","title":"Keeping the code tidy"},{"location":"Tech/Codebase/#where-to-put-the-documentation","text":"Documentation aimed at programmers is best put as close to the code as possible. The Python code is now littered with docstrings which explain every quirk in the code. When writing docstrings I encountered a few stumbling blocks: Docstrings are meant to be read in the code, but they are more readable when properly formatted. Docstrings are more primitive than code: you have to repeat yourself quite a bit. Soon your docstrings litter your code and detract from the terse beauty of it. When you refactor, it is an extra burden to refactor the docstrings as well. For some of these issues are solutions: There are apidoc generators to that produce nice HTML formatted documentation out of them. They also apply inheritance of the code to the corresponding docstrings. You can link from one identifier to another, which lessens the repetitions. In your editor you can use folding to hide all docstrings with one keystroke, and then open them all again with another key stroke. Do not do docstrings early on, wait till the code is more or less mature. And then: good docstrings can help you to spot good refactorings.","title":"Where to put the  documentation"},{"location":"Tech/Codebase/#tools","text":"Here is an overview of tools used in developing this app. Formatters, Linters, API generators Formatters are not perfect, sometimes they produce code at which the linter balks, especially yapf is doing that. Luckily, you can selectively suppress certain kinds of transformations. language linter formatter API-generator JavaScript eslint prettier ?? Python flake8 black pdoc3 Editor or IDE? For projects like these, you need a good editing environment. IDEs The good old ones like Eclipse are not really suited to the JavaScript and Python environments. There are interesting modern ones such as GitHub's Atom modernized ones such as Microsoft's Visual Studio Code and commercial ones such as Webstorm . Editors You can also choose to work with a text editor, such as the free Vim or the commercial Sublime Text . Vim My choice has been Vim, since I use it from its start in 1991. These are the key reasons for which Vim stands out: it has a compositional command set, like Unix itself. By this you get all your (massive) editing chores done without much remembering and thinking. it has a rich ecosystem of plugins. By this you can turn Vim into an IDE. It is rock solid and performant. You can edit many small files and then some big ones, at the same time. You do not loose data. My Vim setup Just for the record, here is a piece of my .vimrc file (the configuration file, which draws in plugins, and customises the interface). You can find out more about the plugins by clicking on them, they are all GitHub repos: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 call plug#begin () \"Python Plug 'vimjas/vim-python-pep8-indent' \" javascript Plug 'jelera/vim-javascript-syntax' Plug 'pangloss/vim-javascript' Plug 'othree/yajs.vim' Plug 'othree/javascript-libraries-syntax.vim' Plug 'mxw/vim-jsx' \" css Plug 'hail2u/vim-css3-syntax' \"utility Plug 'nathanaelkane/vim-indent-guides' Plug 'scrooloose/nerdtree' Plug 'w0rp/ale' \"color Plug 'morhetz/gruvbox' call plug# end () An honourable mention for the ALE plugin. This is an asynchronous plugin that invokes linters for your files while you edit. The beauty is, that if you have installed the linters first outside Vim, ALE is smart enough to detect them and run them for you, asynchronously, and with zero configuration. Then folding . First customize your Python syntax file very little: Make a new file ~/.vim/after/syntax/python.vim and fill it with 1 2 3 syn region pythonString matchgroup = pythonTripleQuotes \\ start =+ [uU]\\ = \\z ( '' '\\ | \"\"\"\\)+ end=\" \\z1\" keepend fold \\ contains = pythonEscape , pythonSpaceError , pythonDoctest , @Spell The only difference with the default is the word fold . Now in your ~/vimrc say 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 set foldmethod=syntax set foldnestmax=2 set foldenable set foldcolumn=1 set foldlevel=2 \" toggle fold under cursor noremap <F6> za inoremap <F6> za \" close all folds noremap <S-F6> zM inoremap <S-F6> zM \" open all folds noremap <C-F6> zR inoremap <C-F6> zR Then you have keystrokes (based on <F6> in my case) to do simple but easy to remember folding.","title":"Tools"},{"location":"Tech/Deploy/","text":"Deployment \u00b6 Basic information \u00b6 what where source code GitHub repository https://github.com/Dans-labs/dariah-contrib docs at GitHub Pages https://dans-labs.github.io/dariah-contrib doc source https://github.com/Dans-labs/dariah-contrib/blob/master/docs app live https://dariah-beta.dans.knaw.nl Build script \u00b6 There is a shell script, build.sh that can perform all tasks needed for developing, testing, deploying and administering the app. Some functions work on your development machine, some work on the remote staging and production machines. It works handiest if you copy the function dab (acronym for dariah build )in addbash.rc into your .bashrc file. Then the command dab without arguments puts you in the server directory (on all machines), and dab --help gives you an overview of what you can do. Here are the kinds of things you can do task development staging, production start/stop mongo yes yes backup data yes yes restore data yes yes transfer backup to local machine yes no reset test database yes yes reset dev database yes no (not present on remote machines) reset workflow table yes yes make user root yes yes generate docs yes no dev server yes no gunicorn server yes yes run tests yes yes ship everything yes no update production no yes Python \u00b6 This app needs Python , version at least 3.6.3. development Install it from https://www.python.org/downloads . The list of Python dependencies to be pip -installed is in requirements.txt . Install them like so: 1 pip3 install pymongo flask production Python can be installed by means of the package manager. 1 2 3 4 5 6 yum install rh-python36 rh-python36-python-pymongo rh-python36-mod_wsgi scl enable rh-python36 bash cp /opt/rh/httpd24/root/usr/lib64/httpd/modules/mod_rh-python36-wsgi.so modules cd /etc/httpd cp /opt/rh/httpd24/root/etc/httpd/conf.modules.d/10-rh-python36-wsgi.conf conf.modules.d/ pip install flask More info about running Python3 in the web server mod_wsgi guide . The website runs with SELinux enforced, and also the updating process works in that mode. Mongo DB \u00b6 This app works with database Mongo DB version 4.0.3 or higher. On the mac Installing 1 brew install MongoDB Upgrading 1 2 3 4 5 brew update brew upgrade MongoDB brew link --overwrite MongoDB brew services stop MongoDB brew services start MongoDB Using Daemon 1 mongod -f /usr/local/etc/mongod.conf Stop it with Ctrl + c Console 1 mongo If the DARIAH data has been loaded, say on the mongo prompt: 1 use dariah and continue with query statements inside the daria collection. In programs Via pymongo (no connection information needed). 1 pip3 install pymongo 1 2 3 4 5 6 from pymongo import MongoClient clientm = MongoClient () MONGO = clientm . dariah contributions = list ( MONGO [ 'contrib' ] . find () Web framework \u00b6 For the server application code we use Flask , a Python3 micro framework to route URLs to functions that perform requests and return responses. It contains a development web server. What the server code does The code for the server is at its heart a mapping between routes (URL patterns) and functions (request => response transformers). The app source code for the server resides in serve.py and other .py files in controllers imported by it. See also the API docs of the controllers . The module index.py defines routes and associates functions to be executed for those routes. These functions take a request, and turn it into a response. Sessions and a secret key The server needs a secret key, we store it in a fixed place. Here is the command to generate and store the key. server 1 2 cd /opt/web-apps date +%s | sha256sum | base64 | head -c 32 > dariah_jwt.secret mac 1 2 cd /opt/web-apps date +%s | shasum -a 256 | base64 | head -c 32 > dariah_jwt.secret Web server \u00b6 production The production web server is gunicorn . Flask is a wsgi app can can be called straight away by gunicorn . In development, you can just call gunicorn from the command line with the right arguments (see build.sh ). In production we install a service that runs Flask under gunicorn. 1 ./build.sh install does this, when run on the production server. See the same script under the commands guni and gunistop to see how the service is started and stopped. development In development, flask runs its own little web server. You can run the development server by saying, in the top level directory of the repo clone: 1 ./build.sh serve which starts a small web server that listens to localhost on port 8001. In this case the app is served locally. Whenever you save a modified python source file, the server reloads itself. User authentication \u00b6 We make use of the DARIAH infrastructure for user authentication AAI (see in particular Integrating Shibboleth Authentication into your Application . Documentation \u00b6 The docs are generated as static GitHub pages by mkdocs with a DANS theme which has been customized from mkdocs-material . To get the DANS theme, follow the instructions in mkdocs-dans . The API docs are generated from docstrings in the Python source code by means of pdoc3 which can be pip-installed. File structure \u00b6 By GitHub clone we mean a clone of Dans-labs/dariah-contrib . The absolute location is not important. Production server For the production server we assume everything resides in /opt , on the development machine the location does not matter. On production we need in that location: shibboleth Config for the DARIAH identity provider. webapps dariah-contrib Root of the GitHub clone. dariah_jwt.secret Secret used for encrypting sessions, can be generated with gen_jwt_secret.sh Development machine On the development machine we need just the GitHub clone and dariah-contrib Root of the GitHub clone. /opt/web-apps/dariah_jwt.secret","title":"Deploy"},{"location":"Tech/Deploy/#deployment","text":"","title":"Deployment"},{"location":"Tech/Deploy/#basic-information","text":"what where source code GitHub repository https://github.com/Dans-labs/dariah-contrib docs at GitHub Pages https://dans-labs.github.io/dariah-contrib doc source https://github.com/Dans-labs/dariah-contrib/blob/master/docs app live https://dariah-beta.dans.knaw.nl","title":"Basic information"},{"location":"Tech/Deploy/#build-script","text":"There is a shell script, build.sh that can perform all tasks needed for developing, testing, deploying and administering the app. Some functions work on your development machine, some work on the remote staging and production machines. It works handiest if you copy the function dab (acronym for dariah build )in addbash.rc into your .bashrc file. Then the command dab without arguments puts you in the server directory (on all machines), and dab --help gives you an overview of what you can do. Here are the kinds of things you can do task development staging, production start/stop mongo yes yes backup data yes yes restore data yes yes transfer backup to local machine yes no reset test database yes yes reset dev database yes no (not present on remote machines) reset workflow table yes yes make user root yes yes generate docs yes no dev server yes no gunicorn server yes yes run tests yes yes ship everything yes no update production no yes","title":"Build script"},{"location":"Tech/Deploy/#python","text":"This app needs Python , version at least 3.6.3. development Install it from https://www.python.org/downloads . The list of Python dependencies to be pip -installed is in requirements.txt . Install them like so: 1 pip3 install pymongo flask production Python can be installed by means of the package manager. 1 2 3 4 5 6 yum install rh-python36 rh-python36-python-pymongo rh-python36-mod_wsgi scl enable rh-python36 bash cp /opt/rh/httpd24/root/usr/lib64/httpd/modules/mod_rh-python36-wsgi.so modules cd /etc/httpd cp /opt/rh/httpd24/root/etc/httpd/conf.modules.d/10-rh-python36-wsgi.conf conf.modules.d/ pip install flask More info about running Python3 in the web server mod_wsgi guide . The website runs with SELinux enforced, and also the updating process works in that mode.","title":"Python"},{"location":"Tech/Deploy/#mongo-db","text":"This app works with database Mongo DB version 4.0.3 or higher. On the mac Installing 1 brew install MongoDB Upgrading 1 2 3 4 5 brew update brew upgrade MongoDB brew link --overwrite MongoDB brew services stop MongoDB brew services start MongoDB Using Daemon 1 mongod -f /usr/local/etc/mongod.conf Stop it with Ctrl + c Console 1 mongo If the DARIAH data has been loaded, say on the mongo prompt: 1 use dariah and continue with query statements inside the daria collection. In programs Via pymongo (no connection information needed). 1 pip3 install pymongo 1 2 3 4 5 6 from pymongo import MongoClient clientm = MongoClient () MONGO = clientm . dariah contributions = list ( MONGO [ 'contrib' ] . find ()","title":"Mongo DB"},{"location":"Tech/Deploy/#web-framework","text":"For the server application code we use Flask , a Python3 micro framework to route URLs to functions that perform requests and return responses. It contains a development web server. What the server code does The code for the server is at its heart a mapping between routes (URL patterns) and functions (request => response transformers). The app source code for the server resides in serve.py and other .py files in controllers imported by it. See also the API docs of the controllers . The module index.py defines routes and associates functions to be executed for those routes. These functions take a request, and turn it into a response. Sessions and a secret key The server needs a secret key, we store it in a fixed place. Here is the command to generate and store the key. server 1 2 cd /opt/web-apps date +%s | sha256sum | base64 | head -c 32 > dariah_jwt.secret mac 1 2 cd /opt/web-apps date +%s | shasum -a 256 | base64 | head -c 32 > dariah_jwt.secret","title":"Web framework"},{"location":"Tech/Deploy/#web-server","text":"production The production web server is gunicorn . Flask is a wsgi app can can be called straight away by gunicorn . In development, you can just call gunicorn from the command line with the right arguments (see build.sh ). In production we install a service that runs Flask under gunicorn. 1 ./build.sh install does this, when run on the production server. See the same script under the commands guni and gunistop to see how the service is started and stopped. development In development, flask runs its own little web server. You can run the development server by saying, in the top level directory of the repo clone: 1 ./build.sh serve which starts a small web server that listens to localhost on port 8001. In this case the app is served locally. Whenever you save a modified python source file, the server reloads itself.","title":"Web server"},{"location":"Tech/Deploy/#user-authentication","text":"We make use of the DARIAH infrastructure for user authentication AAI (see in particular Integrating Shibboleth Authentication into your Application .","title":"User authentication"},{"location":"Tech/Deploy/#documentation","text":"The docs are generated as static GitHub pages by mkdocs with a DANS theme which has been customized from mkdocs-material . To get the DANS theme, follow the instructions in mkdocs-dans . The API docs are generated from docstrings in the Python source code by means of pdoc3 which can be pip-installed.","title":"Documentation"},{"location":"Tech/Deploy/#file-structure","text":"By GitHub clone we mean a clone of Dans-labs/dariah-contrib . The absolute location is not important. Production server For the production server we assume everything resides in /opt , on the development machine the location does not matter. On production we need in that location: shibboleth Config for the DARIAH identity provider. webapps dariah-contrib Root of the GitHub clone. dariah_jwt.secret Secret used for encrypting sessions, can be generated with gen_jwt_secret.sh Development machine On the development machine we need just the GitHub clone and dariah-contrib Root of the GitHub clone. /opt/web-apps/dariah_jwt.secret","title":"File structure"},{"location":"Tech/Inventory/","text":"Technical references \u00b6 This is an alphabetical list of tech references. Sometimes we refer to a technology without making use of it in the app, we have marked those entries with an \u2717. References \u00b6 Web design webApi interacting with the loaded document in a browser Styling css cascading stylesheets: flexbox laying out boxes in flexible ways hsl color space html markup language for the web Shorthands markdown rich text from plain text yaml configuration language, as simple as markdown. Client side language javascript scripting language for the web eslint checks ES6 code against style requirements prettier code formatter for javascript Server side python data-oriented scripting language flask micro web framework gunicorn task runner, acting as webserver behind a proxy wsgi bridge between the python language and webservers Database mongodb NO-SQL database, JSON/Javascript based Generic bash shell scripting iso8601 date and time format Testing pytest test framework for python, well-suited to Flask coverage making sure that the tests cover the whole code base Documentation mkdocs Static page generator in Python pdoc3 API-doc generator from Python docstrings cloc counting lines of code Editing \u2717 IDE Integrated Developer's Environment \u2717 Atom IDE by GitHub \u2717 SublimeText commercial text editor vim old-hands text editor, still competes with IDEs ALE runs linters and formatters within vim \u2717 Visual Studio Code IDE by Microsoft \u2717 Webstorm commercial IDE ???+ abstract \"Linters and formatters * flake8 code linter for Python * black code formatter for Python","title":"Inventory"},{"location":"Tech/Inventory/#technical-references","text":"This is an alphabetical list of tech references. Sometimes we refer to a technology without making use of it in the app, we have marked those entries with an \u2717.","title":"Technical references"},{"location":"Tech/Inventory/#references","text":"Web design webApi interacting with the loaded document in a browser Styling css cascading stylesheets: flexbox laying out boxes in flexible ways hsl color space html markup language for the web Shorthands markdown rich text from plain text yaml configuration language, as simple as markdown. Client side language javascript scripting language for the web eslint checks ES6 code against style requirements prettier code formatter for javascript Server side python data-oriented scripting language flask micro web framework gunicorn task runner, acting as webserver behind a proxy wsgi bridge between the python language and webservers Database mongodb NO-SQL database, JSON/Javascript based Generic bash shell scripting iso8601 date and time format Testing pytest test framework for python, well-suited to Flask coverage making sure that the tests cover the whole code base Documentation mkdocs Static page generator in Python pdoc3 API-doc generator from Python docstrings cloc counting lines of code Editing \u2717 IDE Integrated Developer's Environment \u2717 Atom IDE by GitHub \u2717 SublimeText commercial text editor vim old-hands text editor, still competes with IDEs ALE runs linters and formatters within vim \u2717 Visual Studio Code IDE by Microsoft \u2717 Webstorm commercial IDE ???+ abstract \"Linters and formatters * flake8 code linter for Python * black code formatter for Python","title":"References"},{"location":"Tech/Lessons/","text":"Lessons \u00b6 It took long to develop this tool, and it has grown big, and has shrunk a lot after that. Here is some reflection on the choices I've made, and the things I've learned. \"Best\" practices \u00b6 The previous incarnation of the tool has been built using modern, top-notch, popular frameworks and tools, such as React, Redux, Webpack, MongoDb, Python, modern Javascript (ES6), and its documentation is in Markdown on Github. But it is a complex beast, and it will be hard for other developers to dive in. It has been a steep learning curve to all the frameworks. The node_modules directory of Javascript auxiliary libraries counted 20,000 files. In a clean-slate approach after that I have retraced my steps. The app is no longer a Single Page App at the client side, but an old school server-driven model-view controller app. This approach - alternative approaches \u00b6 I have asked myself the question: why do we need so much programming for such a mundane task? Especially before the clean-slate, with 3 times as much hand-written code plus 20,000 library files, that was a worrying question. Generic app/framework We could have used an app like Trello or Basecamp, or even GitHub itself, or a content management system that has not been designed to support a specific workflow like this. We would have had several disadvantages: an extra dependency on a Silicon-Valley service the struggle to customize the service fighting to let the service do something for which it has not been designed the need to instruct the users to use the system according to the intended workflow. Clean-slate approach: from the ground up Remove the focus on the client: all business logic to the server. The gains of an SPA are not crucial for this app, which will never have a mass audience. It is also not needed to give the users an experience that is close to a native app. Reduction The overhead of front-end development is gigantic in terms of frameworks needed: React, Redux, Webpack, Lodash, all together some 20,000 files in the local node-modules directory. Plus ,5000 lines of custom written Javascript code and another 5,000 lines in JSX (React Javascript). That has all been ditched now, while at the server there is very little increase in code. Why that overhead? One of the reasons for that is that no matter how intelligent the app at the client side is, the server still needs to check the complete business logic. With a thick client, the logistics of data-synchronization between server and client becomes very intricate. The limit of generic programming I have been tempted to push a lot of declarative logic into yaml files. I have gone to extremes to build a generic workflow engine that could be tweaked in an extremely flexible way by just editing yaml files. It did not work. The generic system became intractable and started baffling me more and more. The tweaking of yaml files bcame very dangerous because of all kinds of hidden constraints on the configuration values. Back to good old OO-programming It turned out that object oriented programming has the right patterns to deal with tasks like this: one can implement fairly generic systems in base classes, and then write derived classes for special cases, that inherit the common parts of the logic from the base classes. That worked really well. What we have now, is something that has been built from the ground up again . So, the price has been high, much higher than I expected (and promised), but I think we've got something to build on.","title":"Lessons"},{"location":"Tech/Lessons/#lessons","text":"It took long to develop this tool, and it has grown big, and has shrunk a lot after that. Here is some reflection on the choices I've made, and the things I've learned.","title":"Lessons"},{"location":"Tech/Lessons/#best-practices","text":"The previous incarnation of the tool has been built using modern, top-notch, popular frameworks and tools, such as React, Redux, Webpack, MongoDb, Python, modern Javascript (ES6), and its documentation is in Markdown on Github. But it is a complex beast, and it will be hard for other developers to dive in. It has been a steep learning curve to all the frameworks. The node_modules directory of Javascript auxiliary libraries counted 20,000 files. In a clean-slate approach after that I have retraced my steps. The app is no longer a Single Page App at the client side, but an old school server-driven model-view controller app.","title":"\"Best\" practices"},{"location":"Tech/Lessons/#this-approach-alternative-approaches","text":"I have asked myself the question: why do we need so much programming for such a mundane task? Especially before the clean-slate, with 3 times as much hand-written code plus 20,000 library files, that was a worrying question. Generic app/framework We could have used an app like Trello or Basecamp, or even GitHub itself, or a content management system that has not been designed to support a specific workflow like this. We would have had several disadvantages: an extra dependency on a Silicon-Valley service the struggle to customize the service fighting to let the service do something for which it has not been designed the need to instruct the users to use the system according to the intended workflow. Clean-slate approach: from the ground up Remove the focus on the client: all business logic to the server. The gains of an SPA are not crucial for this app, which will never have a mass audience. It is also not needed to give the users an experience that is close to a native app. Reduction The overhead of front-end development is gigantic in terms of frameworks needed: React, Redux, Webpack, Lodash, all together some 20,000 files in the local node-modules directory. Plus ,5000 lines of custom written Javascript code and another 5,000 lines in JSX (React Javascript). That has all been ditched now, while at the server there is very little increase in code. Why that overhead? One of the reasons for that is that no matter how intelligent the app at the client side is, the server still needs to check the complete business logic. With a thick client, the logistics of data-synchronization between server and client becomes very intricate. The limit of generic programming I have been tempted to push a lot of declarative logic into yaml files. I have gone to extremes to build a generic workflow engine that could be tweaked in an extremely flexible way by just editing yaml files. It did not work. The generic system became intractable and started baffling me more and more. The tweaking of yaml files bcame very dangerous because of all kinds of hidden constraints on the configuration values. Back to good old OO-programming It turned out that object oriented programming has the right patterns to deal with tasks like this: one can implement fairly generic systems in base classes, and then write derived classes for special cases, that inherit the common parts of the logic from the base classes. That worked really well. What we have now, is something that has been built from the ground up again . So, the price has been high, much higher than I expected (and promised), but I think we've got something to build on.","title":"This approach - alternative approaches"},{"location":"Tech/Model/","text":"Model \u00b6 MongoDB \u00b6 We store the data in a MongoDB . Data as documents A MongoDB does not work with a fixed schema. A MongoDB collection consists of documents , which are essentially JSON-like structures, arbitrarily large and arbitrarily nested. That makes it easy to add new kinds of data to documents and collections when the need arises to do so. This will not break the existing code. MongoDB is optimized to read quickly, at the cost of more expensive data manipulation operations. Its documentation favours storing related data inside the main document. This increases the redundancy of the data and may lead to consistency problems, unless the application tries to enforce consistency somehow. In this app, with a limited amount of data, we use MongoDB primarily for its flexibility. We still adhere largely to SQL-like practices when we deal with related tables. So instead of storing the information of related documents directly inside the main document, we only store references to related documents inside the main documents. Terminology Because our treatment of data is still very relational, we prefer wording derived from SQL databases, at least in the present documentation: MongoDB SQL collection table document record Data model \u00b6 This application uses configuration files in tables.yaml and those in tables to model tables and fields, with their permissions. It has base classes ( table , record , field , details ) to deal with most situations, but special tables may use their own derived classes. classification of tables user tables The main tables that receive user content: contrib , assessment , review . user entry tables Tables that receive user content too, namely the entries users make in assessments and reviews: criteriaEntry and reviewEntry . valueTables Tables that define the values for fields in other tables, such as discipline , keyword , tadirahObject . The user table is also a value table. systemTables A subset of the value tables: decision and permissionGroup . Essential for the integrity of the business logic. details Details are records, usually in another table, having a field that points to their master record by means of an _id value. Which tables act as details for which masters is specified in tables.yaml . master-detail A record may have detail records associated with it. We call such a record a master record with respect to its details. Convention Whenever possible, the field in a detail table that points to a master is named after the master table. cascade When a master record is deleted, its details have a dangling reference to a non-existing master record. In some cases it is desirable to delete the detail records as well. criteriaEntry criteriaEntry records are deleted with their master: an assessment record. criteria criteria records are not deleted with their master record. Deletion prohibited In all cases where a record has dependencies, deletion of such a record is prohibited, unless all of its dependencies are marked for cascade-deletion. In order to remove a contribution with assessments and reviews, you first have to delete all its assessments and reviews. provenance Fields for recording the edit history of a record. provenanceSpecs Field specifications for the provenance fields. We have these fields: editors List of ids of non-owner users that may edit this record, even if their group permissions do not allow it. creator Id of the user that created the record. dateCreated Datetime when the record was created. modified Trail of modification events, each event has the name of the user who caused the change and the datetime when it happened. The trail will be weeded out over time. The field \"editors\" may be changed by the owner of a record, and by people with higher powers such as the backoffice, not by the editors themselves (unless they also have higher power). All other fields cannot be modified by users, not even by users with higher powers. Only the system itself will write and update these fields. Backdoors A person with access to the underlying Mongo DB can do with the data what (s)he wants. This requires a direct interaction with the machine on which the database resides. Webaccess is not sufficient. The individual table models consist of the specifications of the fields in that table. For each field there is a key under which some specs are written. fieldSpecs label A user-friendly display name of the field. type The data type of the field. It can be a plain data type, or the name of a value table that contains the possible values of this field. Possible plain types are: text A string of characters, usually just a one-liner. See text . url A syntactically valid URL: i.e. a string of text that can be interpreted as a URL. A validation routine will check this. See url . email A syntactically valid email address. A validation routine will check this. See email . markdown A string of characters, which may extend to several pages, formatted as Markdown text. See markdown . bool2 true or false . See bool2 . bool3 true , null , or false . See bool3 . int An integer number. See int . decimal An decimal number. See decimal . money An decimal number with an implicit monetary unit: \u20ac. See money . datetime A date time, mostly represented in its ISO 8601 format. See datetime . Related values When a field refers to other records, there is much more to specify. In this case type is the name of a value table. See related . multiple Whether there is only one value allowed for this field, or a list of values. perm Who has read and edit access to this field? Conventions The specification is greatly simplified by conventions. Only what deviates from the following conventions needs to be specified: label : same as table name, first letter capitalized type : text multiple : false perm.read : public perm.edit : edit , i.e. the creator and the editors of the record. See below for more about permissions. Permission model \u00b6 The authorization system is built on the basis of groups and permission levels. Users are assigned to groups, and things require permission levels. When a user wants to act upon a thing, his/her group will be compared to the permission level of the thing, and based on the outcome of the comparison, the action will be allowed or forbidden. The configuration of the permissions system as a whole is in perm . The the table-specific permissions are under the perm keys of the table config files mentioned above. Groups Under the key roles the groups and pseudo groups are given. Here is a short description. group is pseudo description nobody no deliberately empty: no user is member of this group public no user, not logged in auth no authenticated user edit yes authenticated user and editor of records in question own yes authenticated user and creator of records in question coord no national coordinator office no back office user system no system administrator root no full control Explanation Groups are attributes of users, as an indication of the power they have. Informally, we need to distinguish between: Nobody nobody is a group without users, and if there were users, they could not do anything. Useful in cases where you want to state that something is not permitted to anybody. The public public is a group for unidentified an unauthenticated users. They can only list/read public information and have no right to edit anything and can do no actions that change anything in the database. Authenticated users auth is the group of DARIAH users authenticated by the DARIAH Identity provider. This is the default group for logged-in users. They can see DARIAH internal information (within limits) an can add items and then modify/delete them, within limits. National coordinators coord is the group of National Coordinators. They are DARIAH users that coordinate the DARIAH outputs for an entire member country. They can (de)select contributions and see their cost fields but only for contributions in the countries they coordinate. Backoffice employees office is the group of users that work for the DARIAH ERIC office. They can modify records created by others (within limits), but cannot perform technical actions that affect the system. System managers sysadmin is the group of users that control the system, not only through the interface of the app, but also with low-level access to the database and the machine that serves the app. Can modify system-technical records, within limits. root root is the one user that can bootstrap the system. Complete rights. Still there are untouchable things, that would compromise the integrity of the system. Even root cannot modify those from within the system. Root is the owner of the system, and can assign people to the roles of system managers and backoffice employees. From there on, these latter groups can do everything that is needed for the day-to-day operation of the functions that the system is designed to fulfill. Pseudo groups In some cases, the identity of the user is relevant, namely when users have a special relationship to the records they want to modify, such as ownership , editorship , etc. When those relationships apply, users are put in a pseudo group such own or edit . Conventions For deletion of records we have a convention without exceptions: records in userEntry tables cannot be deleted directly, only as a result of deleting their master record. Users that have created a record or are mentioned in its editors field may also delete that record if no workflow conditions forbid it. Super users may delete records in user tables. Nobody may delete records in value tables. Assigning users to groups Once users are in a group, their permissions are known. But there are also permissions to regulate who may assign groups to users. Not yet implemented These rules are not yet in force in the redesigned system These permissions derive from the groups as well, with a few additional rules: nobody can assign anybody to the group nobody ; a person can only add people to groups that have at most his/her own power; a person can only assign groups to people that have less power than him/herself. Notes Example If you are office , you cannot promote yourself or anyone else to system or root . If you are office , you cannot demote another member of office to the group auth . You cannot demote/promote your peers, or the ones above you. You can demote yourself, but not promote yourself. You can demote people below you. You can promote people below you, but only up to your own level. nobody Note that users in group nobody have no rights. There should be no users in that group, but if by misconfiguration there is a user in that group, (s)he can do nothing. root A consequence of the promotion/demotion rules is that if there is no user in the group root , nobody can be made root from within the system. Likewise, if a user is root, nobody can take away his/her root status, except him/herself. When importing data into the system by means of load.sh you can specify to make a specific user root . Which user that is, is specified in config.yaml , see rootUser . The command is 1 ./load.sh -r to be executed in the home directory on the server. Alternatively, issuing 1 ./load.sh -R will also convert all other root users on the system to office users. Once the root user is in place, (s)he can assign system admins and back office people. Once those are in place, the daily governance of the system can take place. Name handling \u00b6 The problem There are a lot of names in these yaml files. The most obvious way to use them in our programs (Python on the server, JavaScript on the client) is by just mentioning them as strings, e.g.: 1 title = DM [ 'tables' ][ 'permissionGroup' ][ 'title' ] and 1 title = DM . tables . permissionGroup . title or 1 const { tables : { permissionGroup : { title } } } = DM But then the question arises: how can we use these names in our programs in such a way that we are protected agains typos? Partial solution We tackle this problem in the server code, but not in the client code. Python Well, we convert the .yaml model files to Python modules that expose the same model, but now as Python data structure. This is done by means of the config.py script, just before starting the server. That enables us to collect the names and generate some code. Every part of the .yaml files that may act as a name, is collected. We then define a class Names that contains a member name = ' name ' for each name . So whenever we want to refer to a name in one of the models, we have a Python variable in our name space that is equal to that name prepended with N. . By consequently using N. name instead of a plain string, we guard ourselves against typos, because the Python parser will complain about undefined variables.","title":"Model"},{"location":"Tech/Model/#model","text":"","title":"Model"},{"location":"Tech/Model/#mongodb","text":"We store the data in a MongoDB . Data as documents A MongoDB does not work with a fixed schema. A MongoDB collection consists of documents , which are essentially JSON-like structures, arbitrarily large and arbitrarily nested. That makes it easy to add new kinds of data to documents and collections when the need arises to do so. This will not break the existing code. MongoDB is optimized to read quickly, at the cost of more expensive data manipulation operations. Its documentation favours storing related data inside the main document. This increases the redundancy of the data and may lead to consistency problems, unless the application tries to enforce consistency somehow. In this app, with a limited amount of data, we use MongoDB primarily for its flexibility. We still adhere largely to SQL-like practices when we deal with related tables. So instead of storing the information of related documents directly inside the main document, we only store references to related documents inside the main documents. Terminology Because our treatment of data is still very relational, we prefer wording derived from SQL databases, at least in the present documentation: MongoDB SQL collection table document record","title":"MongoDB"},{"location":"Tech/Model/#data-model","text":"This application uses configuration files in tables.yaml and those in tables to model tables and fields, with their permissions. It has base classes ( table , record , field , details ) to deal with most situations, but special tables may use their own derived classes. classification of tables user tables The main tables that receive user content: contrib , assessment , review . user entry tables Tables that receive user content too, namely the entries users make in assessments and reviews: criteriaEntry and reviewEntry . valueTables Tables that define the values for fields in other tables, such as discipline , keyword , tadirahObject . The user table is also a value table. systemTables A subset of the value tables: decision and permissionGroup . Essential for the integrity of the business logic. details Details are records, usually in another table, having a field that points to their master record by means of an _id value. Which tables act as details for which masters is specified in tables.yaml . master-detail A record may have detail records associated with it. We call such a record a master record with respect to its details. Convention Whenever possible, the field in a detail table that points to a master is named after the master table. cascade When a master record is deleted, its details have a dangling reference to a non-existing master record. In some cases it is desirable to delete the detail records as well. criteriaEntry criteriaEntry records are deleted with their master: an assessment record. criteria criteria records are not deleted with their master record. Deletion prohibited In all cases where a record has dependencies, deletion of such a record is prohibited, unless all of its dependencies are marked for cascade-deletion. In order to remove a contribution with assessments and reviews, you first have to delete all its assessments and reviews. provenance Fields for recording the edit history of a record. provenanceSpecs Field specifications for the provenance fields. We have these fields: editors List of ids of non-owner users that may edit this record, even if their group permissions do not allow it. creator Id of the user that created the record. dateCreated Datetime when the record was created. modified Trail of modification events, each event has the name of the user who caused the change and the datetime when it happened. The trail will be weeded out over time. The field \"editors\" may be changed by the owner of a record, and by people with higher powers such as the backoffice, not by the editors themselves (unless they also have higher power). All other fields cannot be modified by users, not even by users with higher powers. Only the system itself will write and update these fields. Backdoors A person with access to the underlying Mongo DB can do with the data what (s)he wants. This requires a direct interaction with the machine on which the database resides. Webaccess is not sufficient. The individual table models consist of the specifications of the fields in that table. For each field there is a key under which some specs are written. fieldSpecs label A user-friendly display name of the field. type The data type of the field. It can be a plain data type, or the name of a value table that contains the possible values of this field. Possible plain types are: text A string of characters, usually just a one-liner. See text . url A syntactically valid URL: i.e. a string of text that can be interpreted as a URL. A validation routine will check this. See url . email A syntactically valid email address. A validation routine will check this. See email . markdown A string of characters, which may extend to several pages, formatted as Markdown text. See markdown . bool2 true or false . See bool2 . bool3 true , null , or false . See bool3 . int An integer number. See int . decimal An decimal number. See decimal . money An decimal number with an implicit monetary unit: \u20ac. See money . datetime A date time, mostly represented in its ISO 8601 format. See datetime . Related values When a field refers to other records, there is much more to specify. In this case type is the name of a value table. See related . multiple Whether there is only one value allowed for this field, or a list of values. perm Who has read and edit access to this field? Conventions The specification is greatly simplified by conventions. Only what deviates from the following conventions needs to be specified: label : same as table name, first letter capitalized type : text multiple : false perm.read : public perm.edit : edit , i.e. the creator and the editors of the record. See below for more about permissions.","title":"Data model"},{"location":"Tech/Model/#permission-model","text":"The authorization system is built on the basis of groups and permission levels. Users are assigned to groups, and things require permission levels. When a user wants to act upon a thing, his/her group will be compared to the permission level of the thing, and based on the outcome of the comparison, the action will be allowed or forbidden. The configuration of the permissions system as a whole is in perm . The the table-specific permissions are under the perm keys of the table config files mentioned above. Groups Under the key roles the groups and pseudo groups are given. Here is a short description. group is pseudo description nobody no deliberately empty: no user is member of this group public no user, not logged in auth no authenticated user edit yes authenticated user and editor of records in question own yes authenticated user and creator of records in question coord no national coordinator office no back office user system no system administrator root no full control Explanation Groups are attributes of users, as an indication of the power they have. Informally, we need to distinguish between: Nobody nobody is a group without users, and if there were users, they could not do anything. Useful in cases where you want to state that something is not permitted to anybody. The public public is a group for unidentified an unauthenticated users. They can only list/read public information and have no right to edit anything and can do no actions that change anything in the database. Authenticated users auth is the group of DARIAH users authenticated by the DARIAH Identity provider. This is the default group for logged-in users. They can see DARIAH internal information (within limits) an can add items and then modify/delete them, within limits. National coordinators coord is the group of National Coordinators. They are DARIAH users that coordinate the DARIAH outputs for an entire member country. They can (de)select contributions and see their cost fields but only for contributions in the countries they coordinate. Backoffice employees office is the group of users that work for the DARIAH ERIC office. They can modify records created by others (within limits), but cannot perform technical actions that affect the system. System managers sysadmin is the group of users that control the system, not only through the interface of the app, but also with low-level access to the database and the machine that serves the app. Can modify system-technical records, within limits. root root is the one user that can bootstrap the system. Complete rights. Still there are untouchable things, that would compromise the integrity of the system. Even root cannot modify those from within the system. Root is the owner of the system, and can assign people to the roles of system managers and backoffice employees. From there on, these latter groups can do everything that is needed for the day-to-day operation of the functions that the system is designed to fulfill. Pseudo groups In some cases, the identity of the user is relevant, namely when users have a special relationship to the records they want to modify, such as ownership , editorship , etc. When those relationships apply, users are put in a pseudo group such own or edit . Conventions For deletion of records we have a convention without exceptions: records in userEntry tables cannot be deleted directly, only as a result of deleting their master record. Users that have created a record or are mentioned in its editors field may also delete that record if no workflow conditions forbid it. Super users may delete records in user tables. Nobody may delete records in value tables. Assigning users to groups Once users are in a group, their permissions are known. But there are also permissions to regulate who may assign groups to users. Not yet implemented These rules are not yet in force in the redesigned system These permissions derive from the groups as well, with a few additional rules: nobody can assign anybody to the group nobody ; a person can only add people to groups that have at most his/her own power; a person can only assign groups to people that have less power than him/herself. Notes Example If you are office , you cannot promote yourself or anyone else to system or root . If you are office , you cannot demote another member of office to the group auth . You cannot demote/promote your peers, or the ones above you. You can demote yourself, but not promote yourself. You can demote people below you. You can promote people below you, but only up to your own level. nobody Note that users in group nobody have no rights. There should be no users in that group, but if by misconfiguration there is a user in that group, (s)he can do nothing. root A consequence of the promotion/demotion rules is that if there is no user in the group root , nobody can be made root from within the system. Likewise, if a user is root, nobody can take away his/her root status, except him/herself. When importing data into the system by means of load.sh you can specify to make a specific user root . Which user that is, is specified in config.yaml , see rootUser . The command is 1 ./load.sh -r to be executed in the home directory on the server. Alternatively, issuing 1 ./load.sh -R will also convert all other root users on the system to office users. Once the root user is in place, (s)he can assign system admins and back office people. Once those are in place, the daily governance of the system can take place.","title":"Permission model"},{"location":"Tech/Model/#name-handling","text":"The problem There are a lot of names in these yaml files. The most obvious way to use them in our programs (Python on the server, JavaScript on the client) is by just mentioning them as strings, e.g.: 1 title = DM [ 'tables' ][ 'permissionGroup' ][ 'title' ] and 1 title = DM . tables . permissionGroup . title or 1 const { tables : { permissionGroup : { title } } } = DM But then the question arises: how can we use these names in our programs in such a way that we are protected agains typos? Partial solution We tackle this problem in the server code, but not in the client code. Python Well, we convert the .yaml model files to Python modules that expose the same model, but now as Python data structure. This is done by means of the config.py script, just before starting the server. That enables us to collect the names and generate some code. Every part of the .yaml files that may act as a name, is collected. We then define a class Names that contains a member name = ' name ' for each name . So whenever we want to refer to a name in one of the models, we have a Python variable in our name space that is equal to that name prepended with N. . By consequently using N. name instead of a plain string, we guard ourselves against typos, because the Python parser will complain about undefined variables.","title":"Name handling"},{"location":"Tech/Stats/","text":"cloc github.com/AlDanial/cloc v 1.82 T=1.74 s (66.1 files/s, 12591.5 lines/s) Language files blank comment code Python 59 2649 3815 6825 YAML 24 150 7 3609 Markdown 17 570 0 1655 CSS 7 66 9 1185 JavaScript 2 41 6 607 Bourne Shell 3 85 39 511 HTML 1 0 0 61 Windows Resource File 1 2 0 9 INI 1 0 0 4 -------- -------- -------- -------- -------- SUM: 115 3563 3876 14466 TESTS cloc github.com/AlDanial/cloc v 1.82 T=0.10 s (201.9 files/s, 47051.9 lines/s) Language files blank comment code Python 21 997 1291 2607 -------- -------- -------- -------- -------- SUM: 21 997 1291 2607","title":"Stats"},{"location":"Tech/Tables/","text":"Tables with custom logic \u00b6 Here are the particulars of our tables. Contrib \u00b6 Selection \u00b6 Contributions can be selected by National Coordinators. Or they can be rejected. After a selection decision, the contribution and itts associated assessments and reviews become frozen . Nobody can modify them anymore. Stray material There might be stray assessment and reviews associated with a contribution, in the sense that they do not belong to the workflow. These stray records are not frozen, and may be deleted if the owner wishes to do so. Assessment \u00b6 In particular the current score of the assessment is presented here. The score is computed workflow function computeScore and presented by presentScore . Not only the score is presented, but also its derivation. Submission \u00b6 It is presented whether the assessment currently counts as submitted for review, and if yes, also the date-time of the last submission. In this case there is also a button to withdraw the assessment from review. If the assessment does not count as submitted, a submit button is presented. Permissions This is not the whole truth, the presence of these action buttons is dependent on additional constraints, such as whether the current user has rights to submit, and whether the assessment is completen and whether the contribution is not frozen. It can also be the case that the assessment has been reviewed with outcome revise . In that case, the submit button changes into an Enter revisions button, and later to Submit for review (again) . Stalled If the contribution has received an other type since the creation of this assessment, this assessment will count as stalled , and cannot be used for review. In this case, the criteria of the assessment are not the criteria by which the contribution should be assessed. So the system stalls this assessment. It is doomed, it can never be submitted. Unless you decide to change back the type of the contribution. If that is not an option, the best thing to do is to copy the worthwhile material from this assessment into a fresh assessment. CriteriaEntry \u00b6 These records are meant to be shown as detail records of an assessment. As such, they are part of a big form. Each record is a row in that form in which the user can enter a score and state evidence for that score. The display of the rows is such that completed entries are clearly differentiated from incomplete ones. Review \u00b6 The biggest task for review templates is to show the reviews of both reviewers side by side, and to make the review editable for the corresponding reviewer. In doing so, the app needs to know the exact stage the review process is in, to be able to temporarily lock reviews when they are considered by the final reviewer. This is responsible to present the reviewers with controls to make their decisions, and present to other users the effects of those decisions. ReviewEntry \u00b6 These records are meant to be shown as detail records of a review. As such, they are part of a big form. Each record is a row in that form in which the user can enter review comments.","title":"Tables"},{"location":"Tech/Tables/#tables-with-custom-logic","text":"Here are the particulars of our tables.","title":"Tables with custom logic"},{"location":"Tech/Tables/#contrib","text":"","title":"Contrib"},{"location":"Tech/Tables/#selection","text":"Contributions can be selected by National Coordinators. Or they can be rejected. After a selection decision, the contribution and itts associated assessments and reviews become frozen . Nobody can modify them anymore. Stray material There might be stray assessment and reviews associated with a contribution, in the sense that they do not belong to the workflow. These stray records are not frozen, and may be deleted if the owner wishes to do so.","title":"Selection"},{"location":"Tech/Tables/#assessment","text":"In particular the current score of the assessment is presented here. The score is computed workflow function computeScore and presented by presentScore . Not only the score is presented, but also its derivation.","title":"Assessment"},{"location":"Tech/Tables/#submission","text":"It is presented whether the assessment currently counts as submitted for review, and if yes, also the date-time of the last submission. In this case there is also a button to withdraw the assessment from review. If the assessment does not count as submitted, a submit button is presented. Permissions This is not the whole truth, the presence of these action buttons is dependent on additional constraints, such as whether the current user has rights to submit, and whether the assessment is completen and whether the contribution is not frozen. It can also be the case that the assessment has been reviewed with outcome revise . In that case, the submit button changes into an Enter revisions button, and later to Submit for review (again) . Stalled If the contribution has received an other type since the creation of this assessment, this assessment will count as stalled , and cannot be used for review. In this case, the criteria of the assessment are not the criteria by which the contribution should be assessed. So the system stalls this assessment. It is doomed, it can never be submitted. Unless you decide to change back the type of the contribution. If that is not an option, the best thing to do is to copy the worthwhile material from this assessment into a fresh assessment.","title":"Submission"},{"location":"Tech/Tables/#criteriaentry","text":"These records are meant to be shown as detail records of an assessment. As such, they are part of a big form. Each record is a row in that form in which the user can enter a score and state evidence for that score. The display of the rows is such that completed entries are clearly differentiated from incomplete ones.","title":"CriteriaEntry"},{"location":"Tech/Tables/#review","text":"The biggest task for review templates is to show the reviews of both reviewers side by side, and to make the review editable for the corresponding reviewer. In doing so, the app needs to know the exact stage the review process is in, to be able to temporarily lock reviews when they are considered by the final reviewer. This is responsible to present the reviewers with controls to make their decisions, and present to other users the effects of those decisions.","title":"Review"},{"location":"Tech/Tables/#reviewentry","text":"These records are meant to be shown as detail records of a review. As such, they are part of a big form. Each record is a row in that form in which the user can enter review comments.","title":"ReviewEntry"},{"location":"Tech/Web/","text":"Web \u00b6 Server \u00b6 Most logic is handled by server side controllers, informed by yaml configuration files which are read when the web server starts. To see how all the controllers are specified, consult the docstrings Client \u00b6 We use ES6, also known as JavaScript for the client side of the app. The Javascript written for this app is quite lean, only a few functions to add interaction to edit widgets and open/collapse buttons. And a few data fetchers from the server. All code is hand-written, except for the JQuery library. We do not employ any form of transpiling, bundling, uglifying. The code you see is run in the browser as is.","title":"Web"},{"location":"Tech/Web/#web","text":"","title":"Web"},{"location":"Tech/Web/#server","text":"Most logic is handled by server side controllers, informed by yaml configuration files which are read when the web server starts. To see how all the controllers are specified, consult the docstrings","title":"Server"},{"location":"Tech/Web/#client","text":"We use ES6, also known as JavaScript for the client side of the app. The Javascript written for this app is quite lean, only a few functions to add interaction to edit widgets and open/collapse buttons. And a few data fetchers from the server. All code is hand-written, except for the JQuery library. We do not employ any form of transpiling, bundling, uglifying. The code you see is run in the browser as is.","title":"Client"},{"location":"Tech/Workflow/","text":"Workflow Engine \u00b6 Description \u00b6 The workflow engine of this app is a system to handle business logic. Whereas the database consists of neutral things (fields, records, lists), the workflow engine weaves additional attributes around it, that indicate additional constraints. These additional workflow attributes are computed by the server on the fly, and then stored in a separate table in the database: workflow . From then on the following happens with the workflow attributes: the server uses the workflow info to enforce the business logic; the server updates the workflow attributes after any insert/update/delete action. See more in the docstrings .","title":"Workflow"},{"location":"Tech/Workflow/#workflow-engine","text":"","title":"Workflow Engine"},{"location":"Tech/Workflow/#description","text":"The workflow engine of this app is a system to handle business logic. Whereas the database consists of neutral things (fields, records, lists), the workflow engine weaves additional attributes around it, that indicate additional constraints. These additional workflow attributes are computed by the server on the fly, and then stored in a separate table in the database: workflow . From then on the following happens with the workflow attributes: the server uses the workflow info to enforce the business logic; the server updates the workflow attributes after any insert/update/delete action. See more in the docstrings .","title":"Description"},{"location":"Workings/API/","text":"API \u00b6 Outdated These API specs are currently outdated. All API calls are structured like this: https://dariah-beta.dans.knaw.nl /api/db/ verb ? parameters Below there is a partial specification of the verbs and their parameters. Permissions Data access is controlled. You only get the data you have rights to access. If you fetch records, it depends on your access level which records and which fields are being returned. The contribution tool itself uses this API to feed itself with data. Source code In those cases where this documentation fails to give the information you need you might want to look into the source code: index.py list \u00b6 list?table= table name &complete= false or true task Get the records of the table with name table name . Details If complete=false , fetch only the titles of each record. Otherwise, fetch all fields that you are entitled to read. The result is a json object, containing sub objects for the specification of the data model of this table. The actual records are under entities , keyed by their MongoDB _id . Per entity, the fields can be found under the key values . view a table https://dariah-beta.dans.knaw.nl/api/db/list?table=contrib&complete=true view \u00b6 view?table= table name &id= mongoId task Get an individual item from the table with name table name , and identifier mongoId , having all fields you are entitled to read. view an item https://dariah-beta.dans.knaw.nl/api/db/view?table=contrib&id=5bab57edb5dbf5258908b315","title":"API"},{"location":"Workings/API/#api","text":"Outdated These API specs are currently outdated. All API calls are structured like this: https://dariah-beta.dans.knaw.nl /api/db/ verb ? parameters Below there is a partial specification of the verbs and their parameters. Permissions Data access is controlled. You only get the data you have rights to access. If you fetch records, it depends on your access level which records and which fields are being returned. The contribution tool itself uses this API to feed itself with data. Source code In those cases where this documentation fails to give the information you need you might want to look into the source code: index.py","title":"API"},{"location":"Workings/API/#list","text":"list?table= table name &complete= false or true task Get the records of the table with name table name . Details If complete=false , fetch only the titles of each record. Otherwise, fetch all fields that you are entitled to read. The result is a json object, containing sub objects for the specification of the data model of this table. The actual records are under entities , keyed by their MongoDB _id . Per entity, the fields can be found under the key values . view a table https://dariah-beta.dans.knaw.nl/api/db/list?table=contrib&complete=true","title":"list"},{"location":"Workings/API/#view","text":"view?table= table name &id= mongoId task Get an individual item from the table with name table name , and identifier mongoId , having all fields you are entitled to read. view an item https://dariah-beta.dans.knaw.nl/api/db/view?table=contrib&id=5bab57edb5dbf5258908b315","title":"view"},{"location":"Workings/Business/","text":"Business Logic \u00b6 Here we document the functionality of the app from the perspective of the users and stakeholders. We focus on the scenarios that are supported. Status of implementation The DARIAH contribution tool is a big experiment in accounting for research releated output of institutions that cooperate in a European Research Infrastructure with limited funding. The development of this tool so far has been a significant amount of work, in a landscape that has been changing in several respects: the underlying goals and expectations the business logic that is needed the technology on which all is based It is likely that further developments will lead to simpler goals, easier business logic, and a simpler implementation. That is why we do not implement all of the initial specs. Some of the not-implemented items we mark with: (\u2717) a single \u2717: but expected to be implemented at some point in the future. (\u2717\u2717) a double \u2717: unsure if it will ever be be impemented. Business content \u00b6 All information regarding the assessment and review of contributions, is in so-called back-office tables: packages , criteria , types . Source of business rules The business tables have been compiled under guidance of the HaS project by Lisa de Leeuw. Dirk Roorda has entered them into a big back office configuration file which will be read by an import script and transported into the MongoDB database. Contributions \u00b6 A contribution is a piece of work in Digital Humanities, delivered by a person or institute, and potentially relevant to the European DARIAH research infrastructure. Selection by National Coordinators The National Coordinators of DARIAH may add such a contribution to their agreed budget of in-kind contributions to DARIAH as a whole. This makes it necessary to assess contributions against a set of well-defined criteria. Assessment scenario \u00b6 Contributions may represent diverse efforts such as consultancy, workshops, software development, and hosting services. Diversification and time dependency This asks for a diversification of contribution types and associated criteria. The assessor of a contribution (from now on called applicant ) needs to state how that contribution scores for each relevant criterion, and for each score, evidence must be given. Moreover, types and criteria may change over time, but during an assessment and review cycle they should be fixed. Packages, types, criteria Contribution types and their associated assessment criteria are represented by a package record. What is a package? A package is a fixed constellation of types and criteria; it defines a set of contribution types, and a set of criteria, and a mapping between criteria and types. Every criterion is linked to a number of contribution types, meaning that the criterion is relevant to contributions of those types and no others. Every criterion is associated with exactly one package, hence the package ultimately determines the mapping between types and criteria. Active packages At any point in time there are one or more active packages, usually just one. Validity interval A package has a validity interval, i.e. a start date and an end date. A package is active at a point in time, if that point in time is inside the validity interval. The types of an active package are the active types, and its criteria are the active criteria. Technically, more than one package can be valid at the same time. In that case, the sets of active types and criteria are the union of the sets of types and criteria for each active package. But the intention is that there is always exactly one active package. Workflow looks at active packages Other components may call workflow functions in order to determine what the active packages, types and criteria are, so they can render inactive and active ones in different ways. Moreover, workflow will prevent certain actions for inactive items. Inactive contribution type Contributions with an inactive type cannot be assessed. If there are already assessments of such a contribution in the system, they will remain in the system, but workflow will mark them as stalled , and they can no longer be edited. In order to assess such a contribution, you have to change its type to an active contribution type. Rationale Time dependent packages of types and criteria allow evolution of insights. If the current classification of contributions into types appears to have shortcomings, it is possible to remedy the types. Also, criteria can be tweaked and rewritten. Evolution of packages If the current package has trivial mistakes, e.g. in wording or spelling, you can modify its criteria and type records. However, the best way to change a package for significant changes is by creating a new package, and associate new types and criteria to it, leaving the current package unchanged. Then set the validity interval to a suitable value. You can let the old and new package overlap for testing purposes. During that interval, the old and new types and criteria are valid. After that, you can terminate the old package by adjusting its validity interval. Assessments \u00b6 Applicants with write-access to a contribution can add a self-assessment to a contribution. A self assessment is a record in the assessment table, and consists of a few metadata fields. Criteria and criteria entry records When an assessment record is created, additional detail records will be created as well. These are criteriaEntry records. For each assessment, there is a fixed set of criteriaEntry records. This set is determined by the currently active set of criteria: one criteriaEntry record will be created per active criterion. A criteriaEntry record has a field for choosing a score and a text field for entering the evidence. Scores are defined in yet another type of record. Assessment scoring \u00b6 Score records The scores for a criterion are entered in with the help of score records, which are detail records of criteria. Scores have a number, typically 0 , 2 , 4 , and a short description, typically None , Partial , Full , but the number and nature of scores may vary freely between criteria. The score of an assessment as a whole is the sum of the individual scores expressed as percentage of the total amount of points that can be assigned. A temporary overall score is obtained by treating unfilled scores as having value 0 . Non applicable scores Some criteria may allow scores with a value -1 (non-applicable). If an assessment assigns that score to a criterion, 0 points are added, but points missed from this criterion will be subtracted from the total score, so that this criterion will not be counted in the average. Example Suppose there are four criteria, A, B, C, D. A, B, and C have scores 0 , 2 , and 4 . D has scores -1 , 0 , 2 , 4 . Now there are two contributions U and V, with scores as follows: Criterion contrib U contrib V A 4 4 B 4 4 C 4 4 D -1 0 sum 12 12 total 12 16 score 100% 75% See how U does better than V although they have an equal number of points. But for U criterion D does not count, while for V it counts, but the score is 0. Note Not all criteria will allow -1 values! Review scenario \u00b6 After a contributor has filled out an assessment, (s)he can submit it for review. The office will select two reviewers, and they will get access to the self-assessment. Upon asking for review, the assessment and the contribution will be locked. Reviewer roles The two reviewers have distinct roles: reviewer 1 (expert) inspects the assessment closely and advises a decision; reviewer 2(final say) makes the decision. (\u2717\u2717) Both reviewers can enter comments in a comment stream, which are detail records of the assessment. The advice/decision that can be made by the reviewers is approve End of review process with positive outcome. The assessment will remain locked. The assessment score will be made public. reject End of review process with negative outcome. The assessment will remain locked. No assessment score will be made public. (\u2717\u2717) The applicant may enter an objection. In that case the back office will ask a second opinion and take appropriate action, which might lead to a change of decision, e.g. towards revise , or to a new review by other reviewers. revise The assessment and contribution will be unlocked, and the applicant can modify both of them in response to comments by the reviewers. When (s)he is finished, the applicant can resubmit the modified version. Selection scenario \u00b6 The National Coordinator of a country can select contributions from his/her country as in-kind contribution of his country to DARIAH for a specific year. Selection may overrule Ideally, only contributions that have been well-reviewed will be selected. But the app also supports the selection of contributions in whatever stage of the assessment/review process. Selection states The national coordinator can select or deselect a contribution. Deselect means: explicitly reject . (S)he can also refrain from making a decision. As a consequence, there are three possible selection states for a contribution: selected deselected undecided Selection interface The contribution record has a button for selecting it. Only NCs and backoffice people can see/use it. There is also an overview page for contributions which show the selected state of them. NCs can use this overview to (de)select the contributions of their country. Revoking selection decisions Once a NC makes a selection decision, (s)he cannot revoke it. As a last resort, a backoffice member can undo a decision, after which the NC gets a new chance to decide. Selection workflow There are no preconditions for selecting a contribution other than that a contribution is not already selected or deselected. After (de)selection, a contribution gets the workflow attribute frozen , which prevents all modifications of that contribution, except changing its selected field (only by backoffice personnel). Also, all its assessments and reviews, including their criteria entry records and review entry records get frozen . Moreover, the contribution will be consolidated, displayable on the interface, (\u2717) and a pdf report can be generated from the consolidated record on demand. Management information \u00b6 The app compiles management information of a statistical nature, both to the public and authenticated users. Access rights The quantity of information given is dependent on user rights. The public can see contributions, but not assessments and reviews, except the ones that are finalized with outcome \"accept\". In those cases, the assessment score is also visible. National coordinators NCs can (se)select contributions from this overview, but only the ones that belong to the country for which they are national coordinator. Left-overs \u00b6 (\u2717\u2717) Email notification It might be handy to send emails to users involved in assessing and reviewing to notify them that a key event has occurred, such as the submission of an assessment, the appointment of reviewers, the decisions by reviewers. Currently, the app does not send mail. password mail Users are able to request a password reset, and will get a mail with a password link. These emails are not sent by the app, but by the DARIAH Authentication Infrastructure. (\u2717\u2717) Concurrent access When multiple users work on the same item, or one user works on the same item in multiple browsers/browser windows/browser tabs, save conflicts may occur. These save conflicts are not handled graciously. The last saver wins. This problem is hard to solve, but it can be mitigated. One way of mitigation is already in the app: whenever a user leaves a field (s)he has been editing, it will be saved to the database.","title":"Business"},{"location":"Workings/Business/#business-logic","text":"Here we document the functionality of the app from the perspective of the users and stakeholders. We focus on the scenarios that are supported. Status of implementation The DARIAH contribution tool is a big experiment in accounting for research releated output of institutions that cooperate in a European Research Infrastructure with limited funding. The development of this tool so far has been a significant amount of work, in a landscape that has been changing in several respects: the underlying goals and expectations the business logic that is needed the technology on which all is based It is likely that further developments will lead to simpler goals, easier business logic, and a simpler implementation. That is why we do not implement all of the initial specs. Some of the not-implemented items we mark with: (\u2717) a single \u2717: but expected to be implemented at some point in the future. (\u2717\u2717) a double \u2717: unsure if it will ever be be impemented.","title":"Business Logic"},{"location":"Workings/Business/#business-content","text":"All information regarding the assessment and review of contributions, is in so-called back-office tables: packages , criteria , types . Source of business rules The business tables have been compiled under guidance of the HaS project by Lisa de Leeuw. Dirk Roorda has entered them into a big back office configuration file which will be read by an import script and transported into the MongoDB database.","title":"Business content"},{"location":"Workings/Business/#contributions","text":"A contribution is a piece of work in Digital Humanities, delivered by a person or institute, and potentially relevant to the European DARIAH research infrastructure. Selection by National Coordinators The National Coordinators of DARIAH may add such a contribution to their agreed budget of in-kind contributions to DARIAH as a whole. This makes it necessary to assess contributions against a set of well-defined criteria.","title":"Contributions"},{"location":"Workings/Business/#assessment-scenario","text":"Contributions may represent diverse efforts such as consultancy, workshops, software development, and hosting services. Diversification and time dependency This asks for a diversification of contribution types and associated criteria. The assessor of a contribution (from now on called applicant ) needs to state how that contribution scores for each relevant criterion, and for each score, evidence must be given. Moreover, types and criteria may change over time, but during an assessment and review cycle they should be fixed. Packages, types, criteria Contribution types and their associated assessment criteria are represented by a package record. What is a package? A package is a fixed constellation of types and criteria; it defines a set of contribution types, and a set of criteria, and a mapping between criteria and types. Every criterion is linked to a number of contribution types, meaning that the criterion is relevant to contributions of those types and no others. Every criterion is associated with exactly one package, hence the package ultimately determines the mapping between types and criteria. Active packages At any point in time there are one or more active packages, usually just one. Validity interval A package has a validity interval, i.e. a start date and an end date. A package is active at a point in time, if that point in time is inside the validity interval. The types of an active package are the active types, and its criteria are the active criteria. Technically, more than one package can be valid at the same time. In that case, the sets of active types and criteria are the union of the sets of types and criteria for each active package. But the intention is that there is always exactly one active package. Workflow looks at active packages Other components may call workflow functions in order to determine what the active packages, types and criteria are, so they can render inactive and active ones in different ways. Moreover, workflow will prevent certain actions for inactive items. Inactive contribution type Contributions with an inactive type cannot be assessed. If there are already assessments of such a contribution in the system, they will remain in the system, but workflow will mark them as stalled , and they can no longer be edited. In order to assess such a contribution, you have to change its type to an active contribution type. Rationale Time dependent packages of types and criteria allow evolution of insights. If the current classification of contributions into types appears to have shortcomings, it is possible to remedy the types. Also, criteria can be tweaked and rewritten. Evolution of packages If the current package has trivial mistakes, e.g. in wording or spelling, you can modify its criteria and type records. However, the best way to change a package for significant changes is by creating a new package, and associate new types and criteria to it, leaving the current package unchanged. Then set the validity interval to a suitable value. You can let the old and new package overlap for testing purposes. During that interval, the old and new types and criteria are valid. After that, you can terminate the old package by adjusting its validity interval.","title":"Assessment scenario"},{"location":"Workings/Business/#assessments","text":"Applicants with write-access to a contribution can add a self-assessment to a contribution. A self assessment is a record in the assessment table, and consists of a few metadata fields. Criteria and criteria entry records When an assessment record is created, additional detail records will be created as well. These are criteriaEntry records. For each assessment, there is a fixed set of criteriaEntry records. This set is determined by the currently active set of criteria: one criteriaEntry record will be created per active criterion. A criteriaEntry record has a field for choosing a score and a text field for entering the evidence. Scores are defined in yet another type of record.","title":"Assessments"},{"location":"Workings/Business/#assessment-scoring","text":"Score records The scores for a criterion are entered in with the help of score records, which are detail records of criteria. Scores have a number, typically 0 , 2 , 4 , and a short description, typically None , Partial , Full , but the number and nature of scores may vary freely between criteria. The score of an assessment as a whole is the sum of the individual scores expressed as percentage of the total amount of points that can be assigned. A temporary overall score is obtained by treating unfilled scores as having value 0 . Non applicable scores Some criteria may allow scores with a value -1 (non-applicable). If an assessment assigns that score to a criterion, 0 points are added, but points missed from this criterion will be subtracted from the total score, so that this criterion will not be counted in the average. Example Suppose there are four criteria, A, B, C, D. A, B, and C have scores 0 , 2 , and 4 . D has scores -1 , 0 , 2 , 4 . Now there are two contributions U and V, with scores as follows: Criterion contrib U contrib V A 4 4 B 4 4 C 4 4 D -1 0 sum 12 12 total 12 16 score 100% 75% See how U does better than V although they have an equal number of points. But for U criterion D does not count, while for V it counts, but the score is 0. Note Not all criteria will allow -1 values!","title":"Assessment scoring"},{"location":"Workings/Business/#review-scenario","text":"After a contributor has filled out an assessment, (s)he can submit it for review. The office will select two reviewers, and they will get access to the self-assessment. Upon asking for review, the assessment and the contribution will be locked. Reviewer roles The two reviewers have distinct roles: reviewer 1 (expert) inspects the assessment closely and advises a decision; reviewer 2(final say) makes the decision. (\u2717\u2717) Both reviewers can enter comments in a comment stream, which are detail records of the assessment. The advice/decision that can be made by the reviewers is approve End of review process with positive outcome. The assessment will remain locked. The assessment score will be made public. reject End of review process with negative outcome. The assessment will remain locked. No assessment score will be made public. (\u2717\u2717) The applicant may enter an objection. In that case the back office will ask a second opinion and take appropriate action, which might lead to a change of decision, e.g. towards revise , or to a new review by other reviewers. revise The assessment and contribution will be unlocked, and the applicant can modify both of them in response to comments by the reviewers. When (s)he is finished, the applicant can resubmit the modified version.","title":"Review scenario"},{"location":"Workings/Business/#selection-scenario","text":"The National Coordinator of a country can select contributions from his/her country as in-kind contribution of his country to DARIAH for a specific year. Selection may overrule Ideally, only contributions that have been well-reviewed will be selected. But the app also supports the selection of contributions in whatever stage of the assessment/review process. Selection states The national coordinator can select or deselect a contribution. Deselect means: explicitly reject . (S)he can also refrain from making a decision. As a consequence, there are three possible selection states for a contribution: selected deselected undecided Selection interface The contribution record has a button for selecting it. Only NCs and backoffice people can see/use it. There is also an overview page for contributions which show the selected state of them. NCs can use this overview to (de)select the contributions of their country. Revoking selection decisions Once a NC makes a selection decision, (s)he cannot revoke it. As a last resort, a backoffice member can undo a decision, after which the NC gets a new chance to decide. Selection workflow There are no preconditions for selecting a contribution other than that a contribution is not already selected or deselected. After (de)selection, a contribution gets the workflow attribute frozen , which prevents all modifications of that contribution, except changing its selected field (only by backoffice personnel). Also, all its assessments and reviews, including their criteria entry records and review entry records get frozen . Moreover, the contribution will be consolidated, displayable on the interface, (\u2717) and a pdf report can be generated from the consolidated record on demand.","title":"Selection scenario"},{"location":"Workings/Business/#management-information","text":"The app compiles management information of a statistical nature, both to the public and authenticated users. Access rights The quantity of information given is dependent on user rights. The public can see contributions, but not assessments and reviews, except the ones that are finalized with outcome \"accept\". In those cases, the assessment score is also visible. National coordinators NCs can (se)select contributions from this overview, but only the ones that belong to the country for which they are national coordinator.","title":"Management information"},{"location":"Workings/Business/#left-overs","text":"(\u2717\u2717) Email notification It might be handy to send emails to users involved in assessing and reviewing to notify them that a key event has occurred, such as the submission of an assessment, the appointment of reviewers, the decisions by reviewers. Currently, the app does not send mail. password mail Users are able to request a password reset, and will get a mail with a password link. These emails are not sent by the app, but by the DARIAH Authentication Infrastructure. (\u2717\u2717) Concurrent access When multiple users work on the same item, or one user works on the same item in multiple browsers/browser windows/browser tabs, save conflicts may occur. These save conflicts are not handled graciously. The last saver wins. This problem is hard to solve, but it can be mitigated. One way of mitigation is already in the app: whenever a user leaves a field (s)he has been editing, it will be saved to the database.","title":"Left-overs"},{"location":"Workings/Content/","text":"Initial Content \u00b6 There are already 800 contributions in the system. They have been collected in a FileMaker database in the past. Import legacy content We convert this content and use it for an initial filling of the contribution tool. The legacy import is automated and repeatable, even into a database that has been used in production for a while. Legacy contributions \u00b6 The legacy content for this application consists of a FileMaker database. What is in the database? In it there is a web of tables and value lists. The essential content is a contribution table containing 800 contributions. There is also a bit of assessment information. How do we get that data out? From FileMaker to XML We have exported tables and value lists as XML. This is a manual and clumsy process. XML to Mongo DB The machinery for this step is programmed in a Python script, and the configuration details are spelled out in a config . It reads the XML, extracts the field definitions from it, and reads the data from the fields in the rows. We then do the following: adapt the table and field organization; adjust the field types and the values, especially for datetime and currency; generate value tables and cross-tables; add extra information for countries, so that they can be visualized on a map; link values to existing tables; import a moderately denormalized version of the data into MongoDB. Importing and reimporting The source data model is complex, the target data model is complex, and the app as a whole must support a complex workflow. It is impossible to design everything up-front, so we need to be able to retrace our steps and redo the import. As long as the system is not in production, we can just regenerate the database whenever needed, thereby loosing all manual modifications. But there comes a time, and it has arrived now, that people want to experiment with the data. But the app is not finished yet, and maybe there are more design jumps to make. So we need an import script that can reimport the initial data without disturbing the new data. We have written mongoFromFm.py that does exactly this. From transfer to import We started out running the import script in the development situation, populating a MongoDB instance there, dumping its data, and bulk-importing that into the production instance. The problem with that is that the production system will have a different set of users than the development system. Now contributions get tied to users, so if we move over contributions without users, their creator fields will dangle. It turns out to be much better to use the import script also in the production situation. So we ship the FileMaker input for the script to the production server, and run the import there, with slightly different settings. An additional advantage is, that we replace a coarse bulk import by a much more intelligent and sensitive approach: we add the records programmatically, and we have a chance to look before we act. Requirements The task for the import script boils down to these requirements: records that have been manually modified in the target system MAY NOT be overwritten; existing relationships between records MUST be preserved. See later, under Discussion how this is achieved. Usage Production 1 python3 mongoFromFm.py production Development 1 python3 mongoFromFm.py development Extras in development mode In development mode, the following things happen: excel spreadsheets with the original FileMaker data and the resulting MongoDB data are generated; a bunch of test users is added; the ownership of some contributions is changed to the developer, for ease of testing. Discussion \u00b6 The main idea is that all records that come out of the conversion progress, are marked as pristine . Later, when a record is changed under the influence of the tool, this mark is removed. Preventing data loss All records generated by this program will have a field isPristine , set to true . The DARIAH contribution tool will remove this field from a record after it has modified it. This import tool does not delete databases, nor collections, only individual documents. Before import, an inspection is made: the ids of the existing records are retrieved. The ids will be classified: pristine , non-pristine , troublesome . Troublesome means: not pristine, and occurring in the records to be imported. The following will happen: Existing pristine records will be deleted. The import records will be filtered: the ones with troublesome ids will be left out. The filtered import records will be inserted. This guarantees that there is no data loss: no records that have been touched by the application are deleted nor overwritten. Maintaining existing relationships The mongoId creation happens deterministically, with fixed identifiers, generated on the basis of the table name and the record number ONLY. The records are generated in a deterministic order. If the import script has not changed, the results will be identical. If identical records are imported, the results will be identical. If identical records are imported repeatedly, there will be no change after the first time. If the script changes, but the number and order of records that are generated remains the same the generated ids are still the same. Relationships may still break This does not guarantee that no relationships will break. But the only case where things might go wrong are the non-pristine records. If they refer to a value table, and the value table has been reorganized, data may become corrupt. If this happens, ad-hoc remedies are needed. The script will output a clear overview with the number of non-pristine records per table. The user table All production users in the system are not pristine. So they will be untouched. No initial data refers to production users. So the legacy users are disjoint from the production users. The same holds for the test users: they live only on the test system. Nothing in the production system has any link to a test user. The import script creates some group assignments for production users. These links between group and user happen per eppn , not per id. If the receiving database has different assignments in place, they will be non-pristine, and hence will not be overwritten. The import script has stabilized over time, in the sense that it does not change the existing organization of tables, but only adds new data.","title":"Content"},{"location":"Workings/Content/#initial-content","text":"There are already 800 contributions in the system. They have been collected in a FileMaker database in the past. Import legacy content We convert this content and use it for an initial filling of the contribution tool. The legacy import is automated and repeatable, even into a database that has been used in production for a while.","title":"Initial Content"},{"location":"Workings/Content/#legacy-contributions","text":"The legacy content for this application consists of a FileMaker database. What is in the database? In it there is a web of tables and value lists. The essential content is a contribution table containing 800 contributions. There is also a bit of assessment information. How do we get that data out? From FileMaker to XML We have exported tables and value lists as XML. This is a manual and clumsy process. XML to Mongo DB The machinery for this step is programmed in a Python script, and the configuration details are spelled out in a config . It reads the XML, extracts the field definitions from it, and reads the data from the fields in the rows. We then do the following: adapt the table and field organization; adjust the field types and the values, especially for datetime and currency; generate value tables and cross-tables; add extra information for countries, so that they can be visualized on a map; link values to existing tables; import a moderately denormalized version of the data into MongoDB. Importing and reimporting The source data model is complex, the target data model is complex, and the app as a whole must support a complex workflow. It is impossible to design everything up-front, so we need to be able to retrace our steps and redo the import. As long as the system is not in production, we can just regenerate the database whenever needed, thereby loosing all manual modifications. But there comes a time, and it has arrived now, that people want to experiment with the data. But the app is not finished yet, and maybe there are more design jumps to make. So we need an import script that can reimport the initial data without disturbing the new data. We have written mongoFromFm.py that does exactly this. From transfer to import We started out running the import script in the development situation, populating a MongoDB instance there, dumping its data, and bulk-importing that into the production instance. The problem with that is that the production system will have a different set of users than the development system. Now contributions get tied to users, so if we move over contributions without users, their creator fields will dangle. It turns out to be much better to use the import script also in the production situation. So we ship the FileMaker input for the script to the production server, and run the import there, with slightly different settings. An additional advantage is, that we replace a coarse bulk import by a much more intelligent and sensitive approach: we add the records programmatically, and we have a chance to look before we act. Requirements The task for the import script boils down to these requirements: records that have been manually modified in the target system MAY NOT be overwritten; existing relationships between records MUST be preserved. See later, under Discussion how this is achieved. Usage Production 1 python3 mongoFromFm.py production Development 1 python3 mongoFromFm.py development Extras in development mode In development mode, the following things happen: excel spreadsheets with the original FileMaker data and the resulting MongoDB data are generated; a bunch of test users is added; the ownership of some contributions is changed to the developer, for ease of testing.","title":"Legacy contributions"},{"location":"Workings/Content/#discussion","text":"The main idea is that all records that come out of the conversion progress, are marked as pristine . Later, when a record is changed under the influence of the tool, this mark is removed. Preventing data loss All records generated by this program will have a field isPristine , set to true . The DARIAH contribution tool will remove this field from a record after it has modified it. This import tool does not delete databases, nor collections, only individual documents. Before import, an inspection is made: the ids of the existing records are retrieved. The ids will be classified: pristine , non-pristine , troublesome . Troublesome means: not pristine, and occurring in the records to be imported. The following will happen: Existing pristine records will be deleted. The import records will be filtered: the ones with troublesome ids will be left out. The filtered import records will be inserted. This guarantees that there is no data loss: no records that have been touched by the application are deleted nor overwritten. Maintaining existing relationships The mongoId creation happens deterministically, with fixed identifiers, generated on the basis of the table name and the record number ONLY. The records are generated in a deterministic order. If the import script has not changed, the results will be identical. If identical records are imported, the results will be identical. If identical records are imported repeatedly, there will be no change after the first time. If the script changes, but the number and order of records that are generated remains the same the generated ids are still the same. Relationships may still break This does not guarantee that no relationships will break. But the only case where things might go wrong are the non-pristine records. If they refer to a value table, and the value table has been reorganized, data may become corrupt. If this happens, ad-hoc remedies are needed. The script will output a clear overview with the number of non-pristine records per table. The user table All production users in the system are not pristine. So they will be untouched. No initial data refers to production users. So the legacy users are disjoint from the production users. The same holds for the test users: they live only on the test system. Nothing in the production system has any link to a test user. The import script creates some group assignments for production users. These links between group and user happen per eppn , not per id. If the receiving database has different assignments in place, they will be non-pristine, and hence will not be overwritten. The import script has stabilized over time, in the sense that it does not change the existing organization of tables, but only adds new data.","title":"Discussion"}]}